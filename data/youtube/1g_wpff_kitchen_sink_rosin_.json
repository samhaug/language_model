{"AORyULfIRe0": "yo yo yo what's good youtube welcome to another live stream i'm going to raise you up just a bit sweet so gopro updated and um my new wi-fi router is in we're showing signal strength here for the stream decent it didn't show that before so now that's pretty good to see anyways let me go ahead and share the link out to instagram and we're going to go ahead and get started uh all right cool well glad we got that up and running let me go ahead and uh get this stream up on my computer oh you're right let's see i'm sure you guys want to see some of this stuff that i have here before we even get started right let me show you what we got going on first of all we weighed everything out earlier we uh pulled it out of the freeze dryer fresh this morning fat fat nineteen point four percent yield killed it one thousand one hundred and thirty grams of nug went in 220 grams of full spectrum hash came out as well as 19 grams of 25 and 160 micron so this is our test press that we use for edibles and then we'll go ahead and press out the rest of these full spectrum pucks i loaded them by hand into the bags and then after putting them in the bags i pressed them down in a rosin evolution pre-press to make sure that they stayed nice and tight and they are evenly distributed went up from uh the small little four ounce jars and uh upgraded to the six ounce jars i got tired of filling those little small ones to the brim and i said it's time to upgrade baby so we got the jar all weighed out we'll be able to weigh this when we're done tonight and see what our final yield was on this run now i'm a big fan of doing my presses a little bit warmer than what most people would i'm using my stool for you guys today so i'm gonna do this but um yeah you guys that watch me often know i will press anywhere from 200 to 225 depending on the cultivar depending on the hash um not all hash comes out the same um for example we talked about trichome density before right we talked about just because it looks like a lot of hash it may not be a lot of hash because um the oil inside may not be full you know what i'm saying the trichome may just be partially full in that case it's fluffy hash where it looks like a lot but it doesn't weigh a lot that's why this 25 micron and this 160 micron it looks like a large puck but it only weighs 19 grams compared to this puck which is smaller in comparison by about a centimeter right and it's not as fat but it weighs more probably about five to six grams more and that's because this is the full spectrum 45 through the 159 which contains higher density trichomes that are full of milk oil compared to the 25 and 160. so just a little throw that in there for you interior genetics what's going on asteroid farms if you guys aren't following asteroid farms on instagram go follow so old homie of mine platinum grove colorado who used to have a really pimp-ass girl room the nasa lab right um one of the inspirations for me initially stepping up my garden well now it's his turn to come back and he just got a brand new house with a nice ass basement and guess what it's time for a brand new build out so we're gonna help each other out take some ideas off of each other and i'm basically gonna have him record the whole thing and um show you guys how he does his build out you know and he'll give you guys pointers along the way and uh you know show you how to build a grow room so by the time you know he's done with that uh build out i'll be done uploading my video on how when i built out my grow room and uh his will be pretty much part two of a series that i'm going to be doing where it's going to be either build-outs brand new or remodels or upgrades and you guys will be able to pay for each episode um individually and absorb all the information that went into that whole build or remodel so it was actually his idea and we're gonna do his build out as episode two so it's gonna be awesome it's gonna have amazing information in there and it's gonna be great for you guys because instead of just seeing my room all the time and you're like oh that's great but not everybody has your room like yours or not everybody has a layout like yours you'll get to see different layouts different places different um climates right you're gonna get to see it all and you're gonna get to see the different thought process that goes into building these so it's gonna be really cool but anyways go check out his instagram because he's already started posting updates on it and it's not often that you get to jump on board and watch it build out brand new from the start most of the time you hop in and it's like uh the person's halfway done they've already done a bunch of [ __ ] you kind of like feel like the new guy you know what i'm saying in a crowd of old guys that been around for a long time now's your chance to get in on the bottom floor be one of those old guys he's been there forever by the time he gets done finishing you could be like i was there when you were still drawing the blueprints yo whippersnapper some of this blue icy and then i'm going to glove up the press is set to 175. we're going to do a low temp press on our test bag it's not going to be a clear indication of how the full spectrum is going to respond but it's going to give me a good enough idea to know whether or not 175 is going to provide the flow i'm looking for or if i'm going to need to go up to my 204 temperature 202 maybe i will put a link up right now i'm not sure that you guys can do it that's what's up jinder mah what's up dude i haven't seen you in forever holy moly so yeah check out asteroid farms what's going on viper what's up slug upstate what's going on we're just giving everybody a minute to trickle on in taking dabs we're enjoying life i haven't even taken a dab with you guys yet have i ah let's take a dab i'm gonna go in on some of this blue icy first oh let's get in there who wants to get in those guts with me we've got 31 left on this battery oh let's get in there the gopro doesn't focus well close up so you guys are gonna take what you get and be happy all right first time catching a live stream viper you caught a very good live stream for your first live stream this whole live stream i've already prepped all my parchment i prepped all my bags normally i'm not this prepped normally i still have full parchment and [ __ ] i wiped down the plates i'm ready to go baby it's waiting on you guys i mean what do you think we're about to press a half pound of hash a half pound cheers at 413 on the email okay ah that blue icy i swear on the exhale it tastes just like a blue icy it is so crazy every time it never fails what's up easy labs green ranger nashville how you doing freshly pressed what's up annie oh gee what's up hidden creek we got the whole [ __ ] family here i think i'm gonna do a live stream prank and start a live stream pressing hash with the hair straightener so we can get a million people asking oh no what happened did the press break fantastic so huh all righty all righty all righty let's swap out the battery you already know we got it we gotta swap out the battery we don't want it to die it'll be another short live stream i had to find it i couldn't forget i couldn't remember where the [ __ ] i put it down now i can't remember which batteries are charged now let's see lucky number nothing they'd be lying sometimes you got to pull them out and pop them back in they're not lying tonight [ __ ] [ __ ] well one of you's got to be higher than 25 [ __ ] percent let's find out i i don't feel comfortable knowing i've got 25 percent left on the battery i just took a fat ass dab and i'm about to start pressing i'm gonna get excited forget and this is gonna die by the way if the stream does go down unexpectedly please stay tuned because i will bring it right back up this is a non-stop party tonight that battery was at 100 you [ __ ] lying ass charger all right all right so for those of you that are new here i do a [ __ ] ton in a very small space so this is where i'm going to be collecting at when i'm all done here okay we're gonna move some [ __ ] out of the way shout out to roz and inc i want you guys to look at something real close here custom product for who that is who has bags like that who has bags like that do you want to know why he carries these two and a half by four and a half bags hey cuz of me baby who wants some bags i know some of you press and some of you need some bags all right anyways we've got parchment laid out we've got our bags ready to go okay and here you guys are going to go you're going to go right here look at that just for you how's that view yeah you could just chill set you down now is a good time if you got to use the restroom you got to go grab a drink you know what i'm saying whatever you got to do go handle that let me figure out this lighting i don't want it to be too bright for you guys oh [ __ ] that's too bright all right i feel good about this you guys are ordering from rosin inc make sure you guys use my coupon codes they're on my site solventlessbnb.com let's get ready get up there get up there just a little love from my homie right there you know all right remember you guys we're gonna be doing the 160 micron and the 25 micron mixed together as a test press before we move on to the full spectrum presses here we have the uh [ __ ] puck come on turn on screen doesn't recognize my gloves oh well anyways i hope that you guys were able to see that now this uh this is just some uh edibles mix of some uh the last strain that i pressed out which is the grape cosby little dark but it is [ __ ] chirpy and delicious and it will do amazing for edibles because the flavor that it will add is not too over the top but it's definitely like you notice it notice it so we're going to drip this into that edibles jar this is probably not going to flow crazy this is a test press a low temp test press at 175 all right i honestly feel like i should go up a little so so so so oh that looks like it's gonna be good well i think i found the temperature i'm gonna try for the first full spectrum maybe it'll flow a little bit more i'll take it for edibles siezer um don't worry that was just a plate on top of the press jumping around my cold plate up there good extraction well the flow was okay wasn't quite what i want it's kind of a weird flow so all right well not quite what i'm looking for believe it or not 185 that's gonna give me that awkward drip i'm gonna hold pull my audience to see what you guys think the temperature is that i'm gonna go with i already know what i'm going with i'm gonna go with uh my usual all right well we're gonna go ahead and do the first full uh spec press now all the same size that one was at 175. small bag we'll do this one first oh jesus 224 no too high 202 202 202 that's that's where i'm feeling 202 today i can feel it in my huevos rancheros pressing some cool spectrum once cash baby all right the room temperature is 74 degrees i do not believe in running a cold room during the pressing stage now bear with me you guys this is going to be a tad bit awkward as i have to deal with not only the um press this new jar which is larger and doesn't sit on the edge so i have to hold it like this in fact i don't even know what your viewpoint is now does that make sense right there where it's at for you guys i mean all right here we go we're at 191.1 191.2 0.3 0.4 0.5 six that's how slow my press heats up i love it i don't care what anybody says it doesn't overshoot temperatures during pressing because it sleeps it heats up slowly it doesn't like jump up all crazy because it has some overpowered 150 to 200 watt heaters it just has two 100 watt heaters nicely spaced out let's take a dab again next time tomorrow it sounds delicious this [ __ ] is like caramel taffy uh oh uh oh ah excuse me i felt like my nose was gonna run all over camera i didn't want that oh yeah what's up mommy my [ __ ] that'll do good that'll do oh that gets the job done i'll tell you what how you guys feeling about this prize we're at 199.7 199.7.8.9 200. 201 i'm not there and dad just [ __ ] knock me on my ass about let's get ready to remove look at that still plenty of paper towel left to clean bangers with throw that away save that puffing my man it's a good night when my boy puffin comes to visit you're pressing yourself right now huh hell yeah get it dog we are warmed up to 202 degrees let's go this is gonna be a second test press slightly lower than my normal but for some reason i'm feeling this temperature why i saw gonza say 204 and i'm like yeah yeah that's my normal i feel it but then all of a sudden my balls rumbled and spoke to me from deep within and they said no 202 now they're all right [ __ ] it the balls haven't lied yet oh what would you look at that there already me gusta come here come to daddy a little [ __ ] go what i say i knew it i felt it i felt it in my huevos rancheros oh yeah let's get messy somebody's got to make a meme out of the next big bag that squirts all over this jar so i'm feeling good about that that's a fish grab that and we're gonna move this over here i'll turn you right now 202 my [ __ ] i want to know did i get it all did i oh man that was a beautiful extraction 202 good job balls sometimes it's in your brains sometimes it's in your balls tonight it was in my balls oh check your insta i just saw you let me see let's let's see what am i checking your story i don't know maybe i jumped in at the wrong part of that conversation oh elevated uh nah there wasn't a lot on the parchment there was barely anything in here really maybe like two to three grams that was a small piece of uh that was small and that was 19 grams in there and uh [ __ ] dude not a lot left in there it's all over the [ __ ] sides of this jar is where it's at you guys want to see a big squirt huh all right let's get it gotta get off this cyber dream stuff though um all right let's get it big old fatty this one feels like a 27-30-ish somewhere in there all right let's go i don't want to listen to trap sorry i gotta get i gotta get the right music on first this is bothering me put on some good old tj hotto all right let's go i'm gonna fold this one down the center i just pinch it crease it down the center diagonal the other diagonals and that gives you kind of like a good flow right there already and then you just pinch the sides together and it's like let me check my battery here 85 percent go dude i'm all smoking one squeeze okay shout out to my man mysticalgo my official artist on sbnb records back down oh yep bomb extraction on that one let's go cross mountain ranges split cells of tomorrow it flowed so well there's like not even that much on the paper let us go yes uh let's keep this party [ __ ] going good over here peace i think i put some information in the description maybe i didn't add that i guess but 25 micron double bag from rosin inc for the uh for the bags my bad y'all i tried to put some information in there and i left out that one yo if you guys can't catch the live streams sign up for my patreon i upload them there so come to daddy um is i'm the master of my own fate i determine my own yield i grew the flowers this is the source of the single i am my own god excuse you uh is that's why i keep the ends of these parchment pieces wrap them over the parchment paper i wish i could slow this down listen listen did you [ __ ] catch it though my man [ __ ] threw it in putting him on game yo understand he let y'all know i'm putting y'all on game for free i'm gonna i'm gonna take it back one time is it too loud uh um let's go ladies and gentlemen it's time to flow i'm gonna bring you guys back some i feel like i'm not bad hold on you guys don't really need that flashlight i don't think anyways let me take it off and passed wrench squad hold it down here's a pro tip on really fat pucks make sure you leave a little extra space behind the puck so that you don't get a blowout but i can mute it that's fine didn't need that light but i'm not liking how that's lining up on the [ __ ] plates i'm gonna move it over just a tad real quick like uh is i go go go go go real big extraction what is it [ __ ] this one's really done i'm calling it now it's gonna be a dumper is nobody [ __ ] with you you know what as a kid i actually did like origami and [ __ ] you know what was better than origami paper [ __ ] planes god damn it paper planes with a [ __ ] i could fold all types of paper planes you guys when i was a kid not now you know why i can't do it now because i lost the book i didn't look that [ __ ] up read that [ __ ] in the book you know what i mean books are cool as [ __ ] you can learn how to make paper airplanes and [ __ ] who would have thought mm-hmm starting to get good at this [ __ ] so um okay well brown is foreign i until you quit this next one's gonna squirt like a [ __ ] so um okay i have that camera ready i'm gonna record this one on my phone i'm not sure how i'ma do it watch me money money all the time hey money please no worries it's only 200 degrees touch it with your [ __ ] hands that sense sorry i'm trying to align this a little bit differently here safety reasons there it is my press has a ram that can adjust positions so um it's a little bit different than most the ram isn't stationary which means that it can come out of alignment sometimes if i move it or whatever but it's all we're helping to make good taking care of your summer projects is a breeze bam summer savings on all your favorite brands make things even easier so freshen up the deck and fire up the grill summer's here and it's as close as your home yard get more done let's get it i've used this prosper [ __ ] ever you're biking like five years now i just let go of the [ __ ] bar my bad you guys the camera is on a very unsteady amount kinda i don't have the tripod leg spread out very well all right is to the place whoa the puck slit in the plates when i was adjusting it apparently almost popped out of the front no blow out though but i gotta pull it out real quick and readjust it i'm so glad that didn't blow out but somehow that slipped out that's a trip i'm so happy that i didn't blow out that's crazy what the [ __ ] i've never had that happened that's a trip that's a trip son experience saved for real he wouldn't would he he [ __ ] would let's go back on the plates you go you son of a [ __ ] that was a trippy trip trippy trip trip this thing is about i was like why is it not going like it should be leaking way more and then all of a sudden the handle slipped and i was like what the [ __ ] [ __ ] [ __ ] what happened put that back on the plate for a second up a let that so hmm who's your daddy who's your daddy what a safe uh is before oh yeah good extraction i'll take it oh yeah let's power through it two more to go parchment gets added to the jar everything i want everything together mixed together equal even steven the stuff that sticks to the parchment might be different terpenes might be different different [ __ ] you know you don't want to leave it out shop from the comfort of your home at sprint.com that's not a lot of work bro i know i'm saying it's already time to upgrade the jar again i just got this damn jar i'm upset it's not fun i thought for sure it was going to be good enough we knew get a bigger drawer uh bertha it's time to go squirtle what the [ __ ] it did it again something's going on here is it sliding something's happening here there's no blowout but it's sliding off pressure's not being applied evenly to the plates somehow or they're too tight or i pre-packed them at an angle never had this happen okay i know what i'm about to do fix this i'm going to have to increase the pressure even slower every problem there is a solution hmm leaned on him to apply pressure for the pre-press by doing it evenly i had to have been time to upgrade and put my bigger plates on i guess for these bigger presses right inside there i hope you guys are watching this if you guys aren't [ __ ] paying attention i don't know what you're doing right now when i mess up that's when you guys need to pay attention because i don't mess up very often not to sound cocky whatsoever but i don't so if i make a mistake that's resulting in some type of mess up it would benefit everybody around me to pay attention and realize what i'm doing wrong so i got my light that's fine before i make sure i got the jar underneath there i didn't think it was going to squirt like that still this parchment is heavy from the [ __ ] first part of the press but it's still flowing good that's all that matters and no blowout fan dumb strong on the top setting don't you get a damn hum my curl is my rocker maybe i choose you and i don't even have i don't even have um i have a couple theories for what it is lots of adjusts right now it's increasing pressure too fast and also i have the press at a a pretty good angle right now leaning towards the front and i think that the hash is melting and making the puck inside uneven and then it blows out the front like that so i think i'm just going to go ahead oops just hit the table i'm going to go ahead and flatten out the press a little bit more risky with this puck anymore how would i do on the save [ __ ] crazy giving her incredible so okay my battery at 34 okay um check the alignment of the plates one more me come on gloves don't stupid i know it's not that cold in here but then mm-hmm little 74 degrees in here oh missiles contemplating life changes foreign foreign let me grab some parchment hold on i got an idea not so much as an idea that i just want to do something for this press just insurance actually all okay searching for savings on descriptions is so complex it's like getting a documentary unfortunately you just savings period not all patients eligible savings so um me yes i haven't even hit one ton of pressure you guys think i'm kidding look so just hit two and a half oh figured it out right pressure was increasing too fast and the angle probably didn't know so so well aren't you guys glad i got the internet connections really good attraction well wow please so um batteries are charged let's get a battery swap in before we do anything else we'll go from there sound good oh think so okay all right so good job bertha good job baby we're gonna take a dab and then after this dab i'm going to um go ahead and collect what's on the parchment and get it added to the jar while it's still warm man that's heavy that's heavy i mean yield wise i'm thinking it's got to be it's got to be upwards of 80 it has to be you know there's it is no [ __ ] way it's not it's a little harsh didn't it that's okay i'll turn it down i knew there was a little [ __ ] nap thingy in here a little [ __ ] flew right on me else we know there's never a good time to run out of fresh luckily our delivery and free pickup excuse me so whether it's a few extra guns from the back all right i mean you can't help it i got lights on in here when i come in through the garage little [ __ ] just came in with me i guess oh boy oh what are we taking a dab off i mean this is the 160 and 25. this [ __ ] is looking better than some people's like 90s come on man this is my throwaway edibles what's your calling air force reserve af reserve dot com all righty i got the drugs already got a little bit of runs on here i'm gonna grab some more though on the outside of one of these parchments but i smeared i know there's like a bunch on the outside i could take so i don't have to dip into the jar or anything i'm i'm weird right now you're like call that fresh press man i wish i'll give you a melt so like wood that's runs wow wow that flavor is intense now that i haven't dabbed it in a while in a while holy [ __ ] it is intense um hitter hitter hitter hitter hitter goodness q-tip foreign all right fire all right here we go we're gonna collect the rest uh i don't have a tool i can even stick that far down into the jar to reach the bottom that's being 100 you guys want to get the girls so all right all right all right all right hash dust all over there packing the bags so all right you guys i'm going to turn up the music and i'm probably going to space out a little bit i have the chat right behind the camera though so i can see what you guys are saying and i will talk to you guys so this is transparency that your other hashers don't show you they don't show you that collection pressing the washing a to z the whole entire process raw unedited unfiltered unscripted i'm just saying so splash so watching the hashtag so so what makes a business a business circumstance we're offering websites marketing tools and guidance all for free learn more at godaddy.com small piece of hash a little solid piece or something oh that's all that matters like foreign uh 4300 there's little chunks of thc on this one like thca i mean you can see it which is crazy this is the one i had to press twice because this parchment doesn't have silicone coating on it get some cheap i am going to try it it's like clearly on top of the parchment paper it's like not sticky either is that's a trip whoa almost dropped the camera into the [ __ ] departure i forgot i wrapped the email coil back there well i gotta stand up for this because the you know coil is gonna drag across the table let's try this [ __ ] real quick oh yeah that melts that's pure thca holy [ __ ] [ __ ] how the [ __ ] i know i pressed it twice but what like it is so i'm taking a picture of that foreign doesn't have silicone that [ __ ] if you're celebrating at home with family delicious has you covered with appliances up to 40 wow like a samsung laundry pair with steam wash and steam sanitized plus drive for 628 each sure your dryer download the app or shop now on rose.com that you can take on anything what's your warrior find out gohani.com it's like what have you done is is is so [ __ ] crazy wow that [ __ ] blows my [ __ ] mind i have never seen it come off like the press like that off of what i did a simple repressed so so so wow so so so me uh um oh that's [ __ ] big is so oh is i was just time the song is silly but so um hey wow hey cbs pharmacy delivers and right now to meet the needs of the current health situation we're offering free one to two day delivery of prescriptions and other store essentials visit cbs.com or call your local cbs pharmacy to get started restrictions apply damn i have to know how much this weighs after i collect the uh [ __ ] i dropped all my q-tips they're in one spot let me pick them up real quick i have like five more boxes of them but i don't like this [ __ ] around yeah now otherwise i'm gonna get high as [ __ ] and not do it but we're about to take a fat dab so if you guys are prepped bad uh all right now this is the 160 and the 25. so so so and i'll give you an eighth edibles jar let's play with this one a little bit for animals oh yeah good color on that edibles is is so sorry i grabbed the tripod leg on it accidentally a little bit there paper towel snagged it out all right if you put in multiple guesses this is your only chance to go back and delete your other guesses and leave only your one answer i don't want there to be any confusion when people go back to look to see who won so everybody go ahead put in your final answer delete all your other answers that you may have put in when i'm done taking this dab we're gonna see who won", "LOElLD68v8s": "get a good life [Music] [Music] we're back we're doing another press with some xxx og we're at stage one move it overflow this is the quarter pound sum triple XL g flour rosin just straight flour we're only at stage two stead of two more stages to go we're at 2,300 pounds of pressure we're running this at 109 degrees or under 90 degrees we just breached up to stage three we're still pressing we're gonna scoop this to collect or again where if you guys just joined us one of bras endeavor so we're at stage three and counting each stage is about 53 seconds the liquify time was 30 seconds 160 second total stages total cycle 45 46 thousand 46,000 pounds on the on the game we just went up to stage four so we're at 7,000 pounds of force which equivalent to about a hundred and forty thousand pounds 75 tons of pressure under a hundred and thirty one hundred under 130 square inches of surface area it looks like this is definitely going to reach what the test was the test tested at twenty nine percent return and I'm pretty sure that this quarter pound is going to throw about 25 28 grams let's see we'll see it's almost off the parchment paper under there when we are at Stage four we're fitting it fish finishing up this full cycle you guys just joined us is more we're here at a stiched area here shooting a function video this is what the will be you're going on YouTube the self collecting we're at ninety three percent ninety three percent runoff so you got eight percent maximum amount of oil being stuck within your your your parchment paper so there's no need for scraping it's almost all self collected yeah we just finished the cycle we're back at home so let's pull this out it's for what have we got in the oven here what do we got okay that's quite a lot of oil I mean that's a quarter pound JK there was a twenty eight grams per bag for bags let me lift this up and show you guys what it looks like [Music] it's just these pouches every single time they come out like this", "A4liVeDhO6g": "[Music] shoot [Music] [Music] [Applause] [Music] one [Music] so [Music] so [Music] [Applause] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] so [Music] hey guys make sure you check us out at nugsmasher.com to check out all of our awesome products we also have tons of great information on our site including calculator tools and terp files be sure to check us out there at nugsmasher.com you", "l0Umw8g7cQo": "[Music] what's going on guys today we're at the ranch this is our property we have them South Colorado Springs we live in our trailers kick it great views great place to grow pot smoke down all right guys this is the press we are going to be working with today it is a 12-ton rosin press [Music] you can go air compressed or there's a manual option we're gonna go with the manual route today just because we don't want to mess with the air compressor we haven't ran it in a while so we're gonna go with the manual group so we're gonna power it up here it's a little bit difficult to see but we're gonna go ahead and set our temperature [Music] we're gonna be running it at 222 degrees Fahrenheit for this press and we're gonna go ahead and set both our plates that temperature as you can see then we're gonna wait for this bad boy to heat up and we're gonna go press some weed these presses usually run about $1,500 but we actually got quite a good deal and got it for a trade for a pound of weed so usually around 1500 got a fair economy all right everybody here is the weed that we're gonna be pressing in to rosin this is some of our homegrown wheat it is actually four different strains we got some granddaddy purp elephant [ __ ] and EOG tangerine Kush we're gonna try to get about 10 grams of our bud into this puck press and this is what we're gonna be using to press down our weed we're gonna grind up our weed and fill this up and once this is all the way fill you [ __ ] give it a twist and that will tighten it and you put this lid on you screw the lid on and I'll compact all the weed into one nice little puck for the press [Music] alright guys so now that you've seen how that process works we're just gonna fill up and grind up a bunch more weed and we'll see you when that puck pressure's filled up we forgot our big grinders so [Music] alright guys so we're just gonna screw on the lid here and now we're gonna press all the way down into a nice compact puck [Music] dance seven point six minutes all right guys we want to get to ten grams for this press so we're gonna go ahead and put the puck back in and repress the weed to try to get to ten grams [Music] alright everyone here is our new puck [Music] alright guys so the total weight of our puck is 13.8 grams so we're about three point eight over but we wanted but that's alright half an ounce you can see right up here is where we're going our we'd it's a pretty nice setup we'll show you inside in a different video all right we're looking to upgrade that right now so we'll show you the full loop setup when it's all finished we're about there on both of them it's hard to see but both are about two twenty two so we're gonna get the puck in she's ready to go all right guys we're broke as [ __ ] we're using the nonstick just great value parchment paper it works great for this that's all you need we're gonna go ahead and get our two pieces ready [Music] though here is our half of an ounce puck and we're gonna go ahead and put it the press there it is pressing it 222 degrees for roughly in banana half paper good make sure it's centered so we just manually press it together and should get a decent yield from this all right guys we're gonna release it we're gonna take this inside and scrape it oh yeah we got some little dirt here that's right oh yeah [Music] your [ __ ] dabs we're gonna go ahead and take a few so I looks like a good size tab a today's video thanks for watching those are [ __ ] rosin fresh we're gonna take these dad's right now [Music] yeah", "a6BMDjoAf_0": "[Music] Peggy's going on YouTube it's your boy sub eg backhanded with another banger video for the eyes man for today's video guys as you guys can see by that little intro man I got a bunch of goodies right here man and I thought I was just gonna smoke this by myself this weekend but why not make a video for you guys and let you guys know I think it's gonna be a review slash smoke sesh that's just vibing out man so right here guys I got some 710 labs live rosin some bootylicious number one it's tier two I just picked this up as you guys can see right here mint fresh super super super fresh might have already been taking add a bit too out of it but that doesn't matter because we got something else right here guys 710 lives of their 8th right here this is called oh gee KB and man I have heard nothing but good things from like their products man they the fact that 710 labs takes so much pride into the products that they all they do is just put in hard work and the results speak for themselves man they got people talking all over Instagram YouTube Twitter like I'm talking about like everywhere that I go everyone asked me as I have you tried seven ten have you tried seven ten and I've tried their hash rasen but I have not tried their what their Weed Man so I'm very very excited to get into this but for before we get into this man I have to give a big shout-out to today's sponsor man gothic comm where they sell bracelets pendants and rings super sick ass rings man well you can go buy some for your friends girlfriend whatever the case is man and you could use my own personal code savvy G to get a discount percentage of well I'll put everything down in the description below man go show them some support because a support of today's video guys alright honestly we're like I said I was ready to be smoking on both of these guys they both cost me around this one right here was $80 this one right here with $70 so a total of 150 it is a pretty hefty price to paint but you know if you want to smoke it you got to pay the price so we're gonna save this for the second part of the video we're gonna roll up a nice joint with that but first since I do have my rig right here MJ Arsenal and my torch all the goodies right here I think the only thing we can do is take it out all right so I'm already have been dabbing it so I'm bushed shut you guys in there it might be missing some so this right here guys is what I have currently right now and it's honestly some of the turkeys like 710 miles has to be some of the turkeys but I mean wax that I have ever smelled man like the the Terps in this thing is just so amazing and I kid you not man stabbing on Psalm endless like wax is just so freakin good man thinking about doing is heating this up for 30 about 30 to 35 seconds and then waiting about 45 seconds to 50 seconds I feel like that gives it the best the taste and it's not very harsh smoke yet you get very very milky clouds so let's go ahead and torch this thing up boys cheers i heated it up to 35 seconds I'm gonna go ahead and let it go till 45 just because I had you have the window open man and I'm gonna go ahead and meanwhile though let's go ahead and choose a nice little chunky one just to start things off boys because honestly I have I've only dabbed it once today and I feel like man this one's gonna catch me because I'm telling you the hatch frozen is so smooth that you only really feel it when it starts building up in your chest man so we're about during your second day and still got another ten and yeah let me know in the comments below if you've ever tried 7-10 labs man if you have make sure to hit that like button on this video and if you haven't we'll hit that like button anyways cheers boys oh dude that tastes so freakin good I already look at the depths what's already coming in that was the perfect dab I kid you not watch this I love waiting all right the reason I love waiting that much time it's because you just get the smoothest hits the milk is hits and then the Terps now I'm tick I'm talking about I can taste everything in this I can taste everything man it's phenomenal and after you're done your your nail should look like this man you should still see oil in there and man there's seven ten laps definitely definitely worth that's the $80 that I paid man I would I want to buy the Percy that one right there is $100 I believe or $90 so if you guys want to see a video on the percy live rosin let me know in the comments below like this video and subscribe so auntie dad dad got me super high and I just remember that we also have to smoke another joint so I'm probably gonna step aside from the dabbing and we're gonna go ahead and crack this open man this oh gee Kb your see you guys see how fresh it is man I have not opened it brand new look well you can see the seal right there man this is gonna be the first time guys that I have ever seen this flower in person and I'm kind of excited already it's like a little glimpse right there so let me go ahead and pop this sucker out oh dude oh now just follow my computer oh dude it's as beautiful as the oh oh my goodness dude yo I know my camera is not gonna you know pick up every single watch I'm gonna have to use my phone for this guy's but this but right here you guys know the phone camera picks up every single trichome everything that's why we use the phone and look look at this guy's just look at this you guys are gonna be in all all right so this is the og KB right here look at this dude the beautiful orange hairs I'm talking about look at the trichomes look at the purple little stems think you know oh my gosh the curation on this dude is just so phenomenal I haven't even smelt it what does it smell like I have never saw anything like this smells like me but it's just a different type of legal officer dude this is like literally one of the most aya peeling pieces of bud that I have ever seen and now it's just a fact so what I'm gonna do now is I'm gonna roll this up I don't know how much help I roll it up but dude this thing is just so phenomenal looking I don't even know if I want to break it apart but I just want to just put this in a casing and just put it in a corner of my room do you never touch it so this thing looks so phenomenal I'm gonna like I said I'm a roll up a joint and I'll see you guys in a little bit right here we got the joint right here man it's honestly I rolled there I've been getting so much better at rolling these freaking joints so all we got to do now man is pretty much just burn off the paperboys and basically here you go here you have it your perfect little or joint now I want to say there's like 1.5 in here man but does it really matter though because I just want to know how good this is man I want to know if it's worth the money because this this 8 was more expensive than a cookie so he hasn't gonna be the first one to find out boys choose only through the age of 18 or 21 whatever it is in your legal statement choose wood hmm after that personation I did get a little bit of some a little bit of paper tape so [Music] whoo it's actually pretty freakin smooth man so far so far I do i I did roll these in some raw hemp papers organic hemp so there might be another reason why but honestly so far it's burning a really really smooth I am getting a lot of earthiness from this plant itself and it's giving off a little bit of a sweet tone but honestly it tastes very very very good like it's something like I know we did we'd look I know we do sweet guys but you can tell the difference from something that is properly cultivated to some [ __ ] that you know that that is make that is being made in mass amounts and things like that like these guys they take so much pride because it's small batch all the time they never they never overdo the batches man because they want to make sure that every single planet that they grow is growing to a hundred percent perfection on their own terms you know but so far man I haven't I'll any clips of this this it's burning very very smooth like I'm telling you let me give you guys a little close-up of pilots burning right I rolled it really really good it's burning even all the way through you got the little tip right there if you ask them to see that and it's an enjoyable smoke man it's currently 10 o'clock in the morning and usually whenever this is the first time I'm smoking also unusually whenever I smoke first my eye like I started coffee or [ __ ] to him whenever it's the first work of a bit but this is smooth like I was saying and it tastes very piney earthy but it's enjoyable as heck I don't taste any really fruitiness flavors of it no I taste a little bit of sweetness but it's not overpowering this thing is chiefing though it's obviously getting me super super baby I ended up waiting a little bit after taking the dabs and then just I mean I rolled out the joint while I was high a [ __ ] and then I roll I waited a little bit more that way I could get the full effects because I didn't want to go from being a super stone because that was a super fat that that I just took I don't want to go from being super stoned to you know getting even more stunning I even being able to differentiate the type of high that I'm getting in the flavor because my mouth was dry as heck I kid you not that dad got me so tight but I was brick and faded for it for a minute I'm not gonna lie I have not been that faded in such a long time and it's such a good feeling now and then they finished chugging milk for sure all boys that looks like we're at had a lighter fluid on our centers [Music] this thing is McEnroe I hope you guys like the new location I mean I'm just in my garage and I hope you guys not like the current location man I was gonna do it in my car but I was like man for a special day like this my mind let's just do it somewhere where you know you guys can see all the smoke around me not the same setting give you guys some a little something different I just in used to Macanese well I'm about to finish this whole joint to the face but not on camera because that would literally turn my videos so freakin long all right guys so my review for this ogk be it's about its max I'm high right now like super faded and it's such a different height such a soothing high but usually with wheat it's either I don't get high or I get like you know too high to the point where I don't even enjoy the high because my head's hurting but this right here yeah but this one right here is honestly just hitting like every single aspect bro like I'm sure I know I know for a fact after I'm done with this I'm probably I'm probably gonna have a better good morning you know I'm already have me good morning gonna go better and EMA like like I would say though I would definitely would not mind dropping another 150 hundred 60 on some 17 laps again like because there I'm telling you they're rosin smack nice it was just the perfect gap the turf through there the taste is there that I was there everything about it was so good and I feel like usually I don't get that feeling with other concentrates you know with other flower so yeah man I would definitely purchase this again time though I do want to try the Persie rosin you know want to try I'm not even sure what kind of strain this was it was like an indica or sativa guessing it's either hinder guard hybrid so next time I want to try just a straight-up indica and see how much that smacks me because I feel like if this give this is smack at me right now I can only imagine what a straight up in meter with you so yeah guys I'm gonna go ahead and hand out the video here because I'm high as a hack right now I just want to enjoy the rest of this man hopefully you guys enjoyed today's video and you know and if you did go ahead and leave a like comment and most importantly hit your boat with that subscribe button and reach so let's try to get 400 K subs I mean 40 K subs boys that's the next call right and you guys are gonna help me do that guys so I hope you guys have a blessed day and we're out", "e9i7qFYpXzI": "okay should be back now we should be back now all right hopefully the stream stays up i just kept you guys on the battery [Music] cheers we're gonna get into this straight away after a dab that was a lot of work you know and we're going to blast the room with some chili chills real quick it's gotten a little bit warm i turned off the ac i don't know what the view is going to be like today but ah [ __ ] good day [Music] cheers oh we're still warming up we're still at 200. we're gonna drop it in there on top of the bang you're not even hot enough to melt it off yet [Music] 275 [Music] oh my god alien oh jizzle raping my face off oh god oh no wonder i died the e-deal shot up to five 500. [ __ ] oh my god it tastes like burns pigeon ass oh my [ __ ] oh my god [ __ ] me oh no oh no oh no no no no no what the [ __ ] man i hate when it does that [ __ ] [ __ ] me good night folks that's a no for me dog you people are dabbing over 500. oh [ __ ] i don't know how oh [ __ ] [ __ ] [ __ ] [Music] shake that [ __ ] off i was a scorcher man perfect growers dope i'm liking it i can't compare it until after the finished run though you know [Music] 165. let's go that's giggity giggity giggity go slide over you know slide in the electric sorry talking dirty do ya oh i'm not sure exactly if you guys are gonna fit right there might have to slide you guys back on over sorry you're just kind of in [ __ ] away maybe i could do that how's that all right let's go no more [ __ ] around so hmm check your side clearance check your front clearance based off of your front clearance you should know your back clearance if you need to adjust the bag don't be afraid to adjust it once you've started the press it's okay it's better to stop the press adjust and then get it going again then to try and just continue and have a [ __ ] up you know it's not what you want so um look at that [ __ ] casper jizz give it to me baby yeah and i know i can't sing it's okay my rosin speaks look at that [ __ ] saying look at that [ __ ] sing at 165 give me the juice at 165 give me the juice we're not even hitting a ton of pressure yet [Music] we're going to smack this the whole time with this low temp all the way down low temp low pressure not even i'm still not even at a ton of pressure yet we're juicing it baby we're juicing it i feel it never another 90 yielder here the 165 [ __ ] sweet spot on the window all right now we just barely get one ton of pressure i'm gonna push up to two and a half pretty quick here still going this is hambone window baby ham bone mindo nug run we're just now passing five tons take it up to seven i'm going to nine and now i'm going to hit 10 and hold right nine and a half i feel like i need to stop there a little bit more a little bit more stop hold or right under 10. and there goes the right drip again so it looks like we've got some more coming out at this 10 ton mark pulling it wait till i get this over there on the table it's gonna look [ __ ] amazing oh my god it's [ __ ] sugary saucy [ __ ] oh my goodness i can't wait to put you in my finger real quick since we've got the battery life here's the puck it's pretty good pretty dry oh man you wait till i get in there and [ __ ] work on that jewelry buttered up pieces up here get you on here for the next one cover this one over here grab the next one and keep going all right um don't know oh [Music] oz are you hungry yet did you get your did you get your donuts i haven't looked at the chat since i started pressing by the way i'm just [ __ ] with you guys right now let's see what's going on in here low 10. [Music] um so as those other cannabinoids get pressed out at the higher pressures you'll see the color change slightly this ever so slightly towards the end of the press just passing five tons uh he's on mystic thanks for joining the [ __ ] today um and that's going to be all she wrote out of that one folks i'm gonna get you my spell right now and you guys keep slipping i'm gonna have to move you a little bit there so so next one so adrian i appreciate the donation hey man i like my sticker bro if i could i'd put it on the fridge right now i'm gonna have to start pranking out super chat stickers something silly so sup my hobies appreciate dude and from here we just enjoy the drip so i'm going to take you off the tripod and show you around excuse me oh yeah the smooth sound jesus look at that drip drip drip the moon i'm gonna have to get you the other sta uh other stickers matt gotta get you i gotta print out some mystic the goat ones baby so wet i'm leaking how did why is sb and these rosin so clean it's not rosin it's sb and the godges milking the [ __ ] fire out of whatever i'm handed hambone making it so easy ah hambone you make my life so easy bro look at this look at this i'm just i'm sitting back bro i'm sitting back we're not even really in pressure yet look at that sitting on the needle still chilling kicking back take it take a move from candy paint get in there look at that it's about to move on the right it's about to move you see it i have two more presses to go and then collection so probably about 25 minutes 30 minutes okay i'm gonna go ahead and wake up okay yes that sounds now but seriously you guys this is all credit to the farmer man you guys know how it is i have my finesse as a processor but at the end of the day i can only extract what's there to begin with i can't create something from nothing so this is all about me just finessing that hambone drip see where there's a single drip you can see the color on it wait until i get my hands on this the equipment that's going in there so i can give it to my electrician he's gonna come out and run me um you know properly underground through conduit to the shed and then uh all i'll tamp down the uh the foundation the gravel and then um i'm gonna have a plumber come out too try and get something going with this plumbing but i have february to get it sorted so i'm not really in a rush rush they're not really big jobs either neither one of them you just hit eight got a good extraction there those mass i am going to make some progress on that soon i've been really busy but luckily things have been lining up timing wise you know you guys i had to get the whole kind of room reset done and deal with the botched clone issue and then now we're good to go so it's just off to the races you know off to the [ __ ] races i got the controller project coming in in about a week um to two weeks out probably from getting those parts in so i've been trying to get the room ready to go and get that sorted and wash all of hound bones material i had to get caught up on because i was focusing on harvesting and all of that you know i had quite a bit of material backlogged that i needed to wash for for hambone and for myself like the next wash that i'm going to do is probably going to be still not even my own stuff it's going to be probably here he grows endless summer number three nug run [Music] did oh [ __ ] who's gonna win oh [ __ ] who's gonna win i am not washing tonight nope tonight's family night i'm kicking it with my wizard what's up baby [Music] what are you crazy until mystic starts using his own tracks that are not copyrighted then i uh i can't play mystic i keep getting copyrighted for mystic so i can't leave my [ __ ] uh my [ __ ] music on you know what i'm saying i can't leave the stream up so i've been having to take down all the streams so that's why i'm not playing music right now he's working on it he's going to get it taken care of all right i see these terps now uh get over here real good extraction we got one more moving through this [ __ ] today son you know why though cause it's [ __ ] family day and i'm not trying to take too much time away from the squad the numero uno's big bambinos the bosses in my life all the women that i live with oh my goodness kidding me i can't upset all three of them today be gone too long come on give me that flow [Music] um [Music] beneath you touch me until i get my satisfaction satisfaction satisfaction satisfaction push and they just touch me until i get my satisfaction man the gems back in my high school days were so good so good so [ __ ] good we need a producer for mystic or worthy you're gonna have to keep listening to this free stream i'm like [ __ ] uh what's his name from [ __ ] emperor's new groove i will hum my own theme song bro i'm on a mission somebody's gotta make music if i don't have it i will make it myself crunk that's what it was crunk cronkite nesma he's my crunk i've tried all the low five stations they don't work they don't work because they all use samples from songs and they remix them and the sample gets flagged y'all don't understand this lo-fi is not doing nothing new they're just they're just remixing [ __ ] that shit's got any copyright strikes [ __ ] oh that's all she wrote boys that's all she wanted so we're gonna get out of here it's been fun it's been real [Music] and over and over and over and over and over and over and over again yeah that's a battery yep 21 [ __ ] it's not gonna last gotta plug you guys in again and it's probably gonna [ __ ] die when i do it let's see get over here it's almost like i know what i'm doing huh [Music] fish oh yeah that's good that's good i like it that's good to you that'll do pig oh no ow what i wasn't even touching it no no how it better not die no how is it [ __ ] up why what's going on there [Music] so [Music] [Music] [Music] [Music] [Music] [Music] so [Music] [Music] [Music] oh [Music] so so [Music] like no last week [Music] [Music] [Music] so [Music] so [Music] um so [Music] so so [Music] so [Music] [Music] [Music] so [Music] [Music] oh no we got a typical white christian in here no i was playing man what are you guys talking about i look down and that's what i'm seeing pop it twist it [Music] golden unicorn dick oh my god what the [ __ ] so [Music] [Music] [Music] [Music] so [Music] so he makes really good videos i wish i had his production skill and time but i have kids and a wife and a job and yeah it's hard to do it all not there yet i've shown how to make okay rosin from [ __ ] plenty of times [Music] not everything i do comes out looking like this [Music] [Music] and i'm not here to teach you how to make [ __ ] in the gold i'm not here to tell people polish turds i'm not here to help people make fire from the get-go you don't polish turds on this channel [Music] [Music] i had my weightlifting day as i moved past that i got more important [ __ ] to worry about the lifting weights my whole life it's awesome to stay in shape and be healthy but humans don't stay stationary and lift weights we do functional movements it's good to be functional and get out of there you know it's not hard to always work with excellent material if you supply yourself which you should be doing if you're a processor no processor should go into business relying on other people to keep this business afloat you should go into business being able to sustain yourself 100 if you can't you're in the wrong industry you shouldn't be hashing you shouldn't be processing okay my two cents on the mata fire fire isn't it settled so nice no loss when this settles i'll have a nice little muffin let me grab a jar [Music] ah all right so real quick i'm gonna weigh this and then i gotta run because my wife is about to kill our kids so i'm gonna tear that out teared 67.4 alrighty 67.4 divided by 91.5 74 yield all right you guys i'm gonna get out of here thanks for hanging out and uh i'll catch you guys on the next one", "Nqi2iU7kbD0": "each year Microsoft Research hosts hundreds of influential speakers from around the world including leading scientists renowned experts in technology book authors and leading academics and makes videos of these lectures freely available okay I'm happy to introduce le rahimi from Intel Labs Berkeley hi I'm going to talk about random kitchen sinks but before I get into it I want to just make sure everybody can pace themselves through the talk I'm gonna start with really lightweight stuff and then and then we're going to ramp up slowly and then I'll do experiments and you can turn your minds off and then I'll hit you again with some math and then then this is new work and it's not published so we'll try to breeze through it but it's still pretty mafi I'm going to start here so this is to give you a little bit of context about where all this work comes from there's a new trend in AI back in the day when we wanted to build smart things we would start with a really complicated statistical model like Bay's nets where where inference was NP hard and and learning was even NP harder and and we gave it some data and you know a few thousand examples and and we trained some intelligent thing and something happened in the late 90s where instead of models like this these really structured statistical models we started using more generic models non parametric models like our BFS and in to compensate for the lack of statistical structure in these models we started feeding these models lots and lots of data domain knowledge started coming from here instead of from here the nice thing about these generic models is that the optimization problems tend to be convex at least they're in p but because we have so much data the optimization problem in practice ends up taking a lot of time so this talk is really about tricks for making these types of optimization problems on this types of data sets go faster to support the new kind of artificial intelligence that that we're doing these days so I'll give you a few examples of this trend from here to here here's an example from Alyosha afros this is a problem where you want to you given an image you say oh I really don't like these houses being here so you block them out and then you want to fill in this blotted out part with something pretty and relevant so I leave these days takes a very data-heavy approach you just crawls through millions of images through in Flickr and finds a little patches and substitutes the patches in here it's a very heavy weight heavy heavy data-driven approach contrast this with something he did 10 years ago where he actually had a pretty sophisticated hmm based model not very data-driven but the model is a lot more complicated this works a lot better here's another here's another example this is work from Antonio to our album here's each of these little cells is uh is a tiny image there's 10 million images in here and he uses this data set to do object recognition okay extremely data-heavy the the operation that he goes through is just a nearest-neighbor search and in these 10 million images compare that with why did a while back where they actually build these this pretty complicated discriminative model then that the tikkun to account the spatial relationships between objects and wasn't trained on that much data this works really well here's another example from greg shop norwich here's a pose detector I here microsoft recently solved this problem on the Xbox so your goal is to recover the 3d pose of the human body and Greg's approach here is he takes a graphic simulator generates 150,000 examples of a fake person under random different poses and just matches this image and against this image using using a very simple distance metric contrast this with something the same group did 10 years ago that involved actually reasoning about the 3d geometry of the shape in real time and trying to match it against the 2d image and recover the 3d shape of the body this works amazingly well in its fast my own motivation for this stuff is is building object recognition systems that you can train on that can recognize millions of objects in your real world so you know this is this is the system trained on about 30 objects and you know it runs in real time but as you scale the number of objects it goes more and more slowly and the accuracy drops so it would be neat to take systems like this and be able to scale them up just the same way those previous examples i showed you work part of the reason this trend is happening now is that we have access to a lot more data than we did before i think i'm speculating here we have really high fidelity simulators like the graphic simulator that greg was using to generate these body poses we have the web that has lots of images and annotations on it and ever since the MacArthur Foundation started giving grants to people for building games there's a lot of Mechanical Turk stew there's a lot of hand annotated stuff that you get off of the off of the web so this talk is about supporting this trend and i'm going to show you two tricks that I've been playing with I'll start with the random features trick this is a way to speed up colonel machines so a little bit of background on colonel machines here's a classification task so this is a trick for speeding up your classifier and I'm going to tell you about the classification problem a little bit I'll tell you about the colonel machines and I'll tell you about how we speed them up so in this classification problem you got a space a bunch of points the points are labeled among the two classes and you're trying to find a decision surface between them linear decision surfaces don't always separate your to class as well so one would like to consider nonlinear decision surfaces and in kernel machine the decision surface the form of the decision surface that we use is is a weighted sum of kernels placed on your on your training examples okay so so there are n parameters here there's a kernel that we define you know maybe it's a Gaussian or something like that and we place the kernel on each one of these points and then we come up with a good waiting and that that will describe a family of a mountain of curves in this space so this is turns out I mean this is well-known a function of this form when this kernel is positive definite is equivalent to a linear function in a feature eyes space of the input and these features are such that they satisfy this relationship so the colonel effectively maps the features Maps your inputs into some feature space and then takes the inner product in those spaces ok so this is a review of of colonel machines and this is a really neat trick because whereas you would normally be trying to fit a decision surface in an infinite dimensional space right so this is feature for example in general could be an infinite dimensional feature mapping where as you would normally have to find an Omega in some infinite dimensional space this kernel trick lets you search for n parameters only so you can start implementing these things inside computers which is great and it even has this nice interpretation like I said in terms of instead of searching for curves like this you map your data into a potentially infinite dimensional space implicitly and then learn a linear decision surface in that space so this works really well the problem with these colonel machines is that they force you to deal these enormous matrices if you have 10 million training examples one way or another you're going to have to represent a 10 million by 10 million matrix whose entries consist of the kernel evaluated on pairs of your training data points I made this really big to take up the whole screen to emphasize how big these matrices can be so you can do infinite dimensional things in finite dimensional computers with the kernel trick but these things are still huge and so some researchers have come up with very popular tricks for dealing with matrices like this here's another trick the trick is well we talked about how these kernels actually compute inner products between feature eyes din puts so what we're going to do is instead of dealing with the kernel or with this infinite dimensional feature mapping we're going to find a finite dimensional feature mapping in fact a low dimensional feature mapping such that instead of having to take this inner product in this infinite dimensional space you can just take the dot product of the feature eyes inputs ok so you know just like just like with with Colonel machines your decision surface took this form with this machine because we're now using finite dimensional features your kernel machine just taste this far ok and to go back to this diagram the idea again is we're going to to find these these nonlinear decision boundaries we're going to map our data into some finite dimensional low dimensional space and then train a linear decision boundary there and this mapping is going to be such that this relationship problems ok so instead of training your curl machine with kernels and dealing with these enormous matrices we're actually going to randomly feature eyes your inputs and then train a linear classifier and this relatively low dimensional space and we're going to guarantee that that the resulting classifier is going to be close to the classifier we would have had we would have gone and I'm going to tell you about 22 different types of random features one of them are free random features and they're based on on the Fourier transform of the colonel and another one is a discrete random feature that's based on gridding up this space i'll go through both of them the proof for why this works is really simple it's four lines and I I think it's kind of neat to look at so I'm going to just pop out pop up some math and I'm going to walk walk through it because works this is just really neat okay so the trick is we would like we're given a colonel I forgot to mention this only works with shift and bearing it colonel so so you have to be able to represent the colonel like this and at the bottom we're going to get we're going to derive these these random features such that this relationship holds we want the inner product between the random random feature eyes inputs to almost be equal to the value of the colonel now I'll walk you through here okay so step one take the Fourier transform of you colonel okay so this is just the standard Fourier transform that you learn about an elementary school step 2 so this is an interval we're going to replace the integral so p is the Fourier transform right of the colonel we're going to replace the integral x by approximating this this integral with with a with a with a with an average okay so treat this fourier transform as a probability distribution draw samples from it yes is no patience for texts were vectors you're now in a scalar space no X's are still vectors multi-dimensional yeah yeah so this is so omega prime is actually inner product between Omega and a vector X minus y you are ways here or in the zone no because I'm drawing from from from POV so so what I didn't what what I slipped under the the rug here is that because this Colonel is positive definite this the free transfer so here's a theorem the Fourier transform of the positive definite kernel is positive definite this is Buckner's theorem you don't learn that in elementary school for some reason but no I'm not sorry in systems and signals and systems in allen will skis book that has all these free identities this identity is not there unfortunately it's a really powerful one so so the point is that we can treat this fourier transform as a probability distribution it's positive you can sample from it so let's approximate this integral using using a sample averages and now i'm just going to rewrite the summation i'm going to split up each of these turn into the product and then I'm going to write this in vector form this vector depends only on X this vector depends only on why and we have our random features ok so the Fourier random feature really what it's doing is saying if you want to compute k of x and y take x project that down into a random direction w w is drawn from the fourier transform of the colonel so you take X you project it down onto the hyper plane and then you you compute a phaser from that okay so you just project it down and then you just wrap it around the complex circle and this complex number not becomes your random feature okay and and and there's a squiggle mark here certainly this relationship holds an expectation all right so certainly this is true it seems like something I'll give you better than others for for this so the sampling scheme is given to you the sampling scheme is draw from the Fourier transform of the colonel regular samples is an industry transform or you could just randomly sample things went better than the other you could draw a non random sample yeah yeah so so certainly this says that well so so this says there exists a random sampling such that these two guys are close to each other sorry a random sampling will probably produce something that's close to each other and that implies that there exists a deterministic sampling such as these two guys are close to each other the problem is I don't know how to come up with one I know how to come up with one by just sampling but I don't know how to how to construct one okay okay so the point of this was to show you that at least an expectation feature izing your x and y and computing their inner product gives you something gives you gives you the colonel value okay i've also showing you how to compute the z it's just draw a bunch of samples from the fourier transform of the colonel and compute compute these phasers what I really want to want though is not these results and expectations we want to show that this actually holds throughout the space so let me let me go through that right now let me let me tell you what we what we know how to do so we know that that the inner product for a given x and y is going to be close to k of k of x and y and expectation but we can also show that the tails of this are quite light okay so this is just by hosting so these guys for given x and y aren't going to deviate very much with very high probability using the Union bound on this you can you can show the same thing on a discrete data set of endpoints but even more so using a covering number argument you can show that this holds throughout the whole space okay so so if you draw enough if the dimensionality of your random feature is high enough then then this inner product will approximate this kernel for all the points in your space with very high probability okay so it's not just the result in expectation this is actually result that holds with very high probability throughout the whole space in fact let me reinterpret that theorem for you so it says that with probability at least 1 minus P P some probability that you given the inner product of the whole space approximates the colonel over the whole space with with probability at least one minus P as long as you have enough the dimension of your random feature is high enough as long as you sample from the Fourier transform enough times ok so this depends on linearly on the dimension of the space this is a standard epsilon squared dependence on the error over there and the dependence on there's a dependence on the curvature of the kernel as well as you might expect to see where it right so you go one big deal that's right that's in fact we'll see i'll show you an experiments in the second part of the talk that i'll compare the the cost of this d versus the cost of choosing these features optimally yeah values of dr and yours in your concrete things yeah why don't I why not just when we get to the experiments all so here's another random feature so that was the furry random features as a totally different class of random features that that that one can also construct so you give me a colonel and and my job again to remind you is to build a function possibly randomized one such that the inner product between the feature eyes inputs is equal to the colonel okay so so this random feature works like this grid up your input space your space of exes so just lay down a random grid i'll tell you how to pick the the pitch of the grid in each bin of the grid a sign a sign a sign a binary spit strength pioneer bit string is just the representation of the number of the grid and unary okay so a grid one gets a one over there good to good to you and then the random feature representation of a point is just it's grid ID written in unary okay that way when you compute the inner product you're basically just one if you're in the same bin or zero if you're not on the same thing and now all that's left is for me to tell you how to compute the these random grid grid pitches and in the same way that we picked the Omegas from the fourier transform of the colonel here we define a hat transform of the colonel instead of sinusoids it's in terms of these these hat basis functions okay so so you randomly sample your your grid pitches from from the hat transform of the colonel that you're trying to approximately and again you get the same same theorems and same results as with the free for you a colonel so um let me let me show you how this looks like in code this is it's very simple right um if you want to train a data set with a with an l2 lost you want to train a classifier using an l2 loss with free random features you generate a random a bunch of random w's so these are the you just sample from the Fourier transform of say the Gaussian kernel Fourier transform of the Gaussian kernel is again a Gaussian so you just draw a bunch of Gaussian w's and then you pass your data through to the random feature so this is the complex sinusoid and then and then you just now fit a linear linear solver you just fit fit a hyperplane to in this feature I space and that's just the least squares problem isn't backslash over here okay and boom you have your you have your hyper plane that's that that's training in three lines of matlab code and then for testing you you map your input vector through the through the random map and you you evaluate the inner product with respect to the Alpha that you just learned so let's get to the issue of dimensions that you brought up so here's a bunch of data sets that we run this the Sun so typical dimensions anywhere from twenty one dimensions to 127 dimensions on these data sets I'll show you higher dimensional data sets data set sizes range from a few thousand to a few million and generally training is really fast with these random Fourier features and here are the typical dimensions that I picked these are much smaller than what the theorem would predict so the theorem is quite loose the theorem would predict well so it depends what epsilon you want there's a 1 over epsilon squared so if you wanted accuracy of 1% it be 0 point 0 1 squared right that's 1 over point 1 squared so it's 10,000 okay so so we're getting into so the theorem is loose because of this guy right here okay so in practice you know we get typically better than state-of-the-art performance on on various heavily tuned svm libraries the free random features work on most data sets on some of them like the forest data set it didn't work so well and so this data set has this characteristic that if you actually were to train an SVM with our BF kernels on it most of the points become support vectors I'm sorry thats Sarah yes sorry there's two different flavors there's the two class version and there's a setting class version I suspect this is the two-class version given the rate yeah I am I took the version of the forest cover from these people here from the core vector machine I just grabbed it from there and ran everything else on it anyway so so the point of this was the point of this line is to tell you that that there are that that there are the two free if we the two random features are complementary in some sense these are really good for learning really smooth decision surfaces these are really good for doing nearest neighbors types of search surfaces yeah are you turning so the 500 see if it works and then if it works really well then you set it to 100 if it doesn't work really well you said it's about maybe play with good lighting good job I I have I have yes so you just you can just sack up the random features for the two guys and things work quite well it isn't the best of both worlds yeah you tend to that I mean so in the sense that that if you run it on all these guys you get basically the same performance why is it better than back well so there's an approximation going on the approximations in terms of the colonel and on in terms of the decision surface or not not even in terms of the ideal decision surface we really are learning a different service it just tells you that that the RBF representation is it the best representation and it could be the hinge love you're also teasing regularized least squared classification yes so you you get basically this I've run all these stuff with all these things with the hinge losses one basically get the same results and it stops it stops the matter what lost you you use when you have very large data sets okay so on let me let me just tell you a few of the properties of these things so as you would expect this is on the forest it is set if you bigger the data set right so big data sets help but you knew that and also as you would expect as you increase the dimension of the random feature your error also drops so this is air dropping quite fast in training and testing time not increasing very fast so so in practice these things tend to tend to have desirable properties let me um okay so this is this is random features this was about training colonel machines faster let me generalize the problem this is this is where the random kitchen sinks come from so we were learning these feature mappings based on a kernel that you give me but why have bought why start with a colonel in the first place so back in the day this is this is a picture out of a out of a paper by block from 1962 we had these neural networks and and there was this idea just from day one but there should be some randomization that happens at the first liar so so this idea of having some randomization in in your training algorithm is is classical now we don't draw our neural networks like this anymore here's here's maybe a more modern way of doing things here's your input it goes through some nonlinearities but on the areas have some parameters and then you wait the output of the nonlinearities and what you're what you're learning during training is these weights and the parameters of the nonlinearities and actually this is also outmoded we just write this now our neural network is is a weighted sum of nonlinearities and their parameters and we just learn the weights and the parameters so let me focus on one popular way of doing it of training these parameters so so when you do a to boosting you build dysfunction stage wise you train the alphas you train the Omegas and then you do that for the next step and so we have T of these stages in random features we were also training a decision surface of a similar form our omegas were random and we were just training for the office in Colonel machines we're also doing something similar except instead of a finite sum it's just an integral and we're learning this function alpha okay so so a lot of these so basically the world of machine of shallow network machine learning just all is focused on learning decision boundaries of this form and I'm going to focus on one particular way of training these and that's the greedy method which goes back to well it goes back about fifty years so the idea and I'm going to focus on a function fitting framework ok forget the data set for now somebody gives you a function f star and says please approximate it with a function of this form you get to choose the Omegas in the alphas but I want the resulting some to be to be close to this function in sum and sum norm in a function space ok so you're given a function a target function to approximate and you asked to come up with a bunch of weights and a bunch of parameters such that they're such that the way that some is close to the to the target function so the greedy approach you know which looks like a de boost looks like matching pursuit looks like a lot of these other things goes like this start with the function 0 and then we're going to find one term we're going to add one term to the function and that's going to be the term that gets the one term addition that gets us closest to F star and now we have a new function and then you you iterate again for the next for the next edition for the next term in the function you again come up with the one that that minimizes the difference between the residual and the target function and you do this T times okay so this this this has the flavor of a blue spring and we know a lot about the performance of a function fitting algorithms like this in fact this result goes back 20 years 25 years 15 years 15 years so suppose you want to approximate a function of this form this is our target function it consists of an of infinitely many terms and we want to approximate it with a T term function that we built as in the previous slide so what's known is that the distance between the approximation that we built as in the previous slide and the target function decays as 1 over square root of the number of terms and there is a constant here that measures in some sense the norm of the target function so so but the l1 norm of the alphas is is a norm on on functions at this one okay all right so this is proved by by induction over the over t let me let me write this down graphically for you it says it says that if you for any function in this in this space with an infinite expansion there exists a function with atty term expansion if you're a lot to pick Alpha and Omega that's not too far from that function within 1 over square root of T so so it's a statement about for all functions in the blue there exists a function in the purple that's not too far ok thats that's about as good of a rate as you can get this this this rate is is tight all right so so here's what here's another idea this is the random kitchen sinks idea for forfeiting functions you're again given the target function and you're again asked to come up with alphas and omegas such that this this T term approximation is close to F star but now we do something much simpler just pick the Omegas from some distribution just randomly instead of instead of going through this greedy algorithm there's nothing greedy about this now just pick them randomly and then to pick the alphas just just solve this this convex problem yeah so this looks like a least squares problem for example all the ones it's a batch optimization over T alpha ok so now and then just return the Alpha and the Omega so you compute so so let's see how well this does and you would expect that that the performance guarantees would somehow depend on the distribution that you use to sample the Omegas right and so and so here's here's a result if you remember the the reason the theoretical results for for the greedy algorithm dependent dependent on the l1 norm of of the coefficients of the function we were trying to approximately the see over here okay so let's define an analogous norm for the target function we're trying to approximate we're going to call it the p norm and it's going to depend on the probability fusion they use the sample your your your parameters okay so you could think of this as a as an important sampling ratio between the alphas and the distribution that you're using simple easy okay so so the theorem says if somebody gives me enough start to approximate using the the the the algorithm I just showed you on the previous slide then then the T term approximation with probability at least one minus Delta for for any Delta that you pick also drops as 1 over square root of T so we have the same rate in terms of the number of terms that we need in the expansion as we do with the greedy algorithm but here we just managed to sample the parameters randomly and then there's this dependence on the on the importance ratio between olympians yeah stars face it right yup do your future before I thought you see Vanessa leave it from your bro so here's so here's here's here's what this theorem says okay so you fix I have star and then and then we're saying so you fix it up star and F star is drawn from this big set that looks like that it's it's infinite expansions of of your weighted features and we say that if you consider this random set so this is a random set consisting of all alphas all weights but then these features are drawn randomly it says that with very high probability the difference the distance between this fixed f star and this set is drops as 1 over square root of T okay so whereas before we were making a claim about for all points in the set there is a point close to here in this case we're just saying for a fixed point in this set and that's all you need to talk about to talk about function fitting after all somebody gives you a function to fit and then you draw stuff you don't need you don't need to approximate the whole space ever you just need to approximate the function at hand that you need to approximate and that's why we managed to get this one over square root of T this constant could be substantial yeah you could pick a sampling distribution for the weak learners for the for the features that's terrible for the given function yeah it's easy to construct it's just if you use a direct Delta for example for your omegas you just learn a really crappy classified but at least the theorem is correct and that that crappiness is reflected in this rear anymore find the daleks right now your zune you just pick all rather than once we leave out some fraction replace them or something we can do a little bit more yeah what works really well is if you start with this random thing and then just do a few iterations of gradient descent on the Omegas in the office yeah so that works incredibly well that sounds like a rollover of course it's all in neural network interested just can't say that yeah so okay so what what what what is neural what is neat about it is that it's a neural network that you initialize with random weights and you have guarantees about how far you end up from some the thing you're trying to approximate well so it depends where you start with the training from scratch I'm not very good at it even though I've tried very hard i often get stuck in local minima generally you end up doing quite well if you if you do this and then gradient descent and the spare minutes I'll report I don't do the refinement gradient descent refinement I've informally just tried training starting from zero or various parameters that I thought might be good and it works ok but nowhere near as well is this I mean it's much slower because because you need to run greeting the set for a lot longer to the random sampling rate because I've tried stuff like we're the fees were pca of the dataset and that didn't work well either yeah so this is random sampling is completely independent of the data and that but you need to choose the in Vegas from the same probability distribution that you expect to be good that's right that's right that's right so it is a design issue so the reason this stuff ends up working well in my intuition about all of this stuff and what these theorems mean is that is that really it doesn't matter what the nonlinearities are but just put a lot of effort into figuring out what their weights should be that's where the magic is not in here necessarily that's that's that's how I view this this this result instead of taking the greedy approach you go and you take the stupid and then you go back you try to do these squares faceted yeah end up doing much worse if you get there's a generalization yep since it just even here not great it's not hurting you because of the way we pick the weak learners right right because they're random right so you're talking about the Dale Sherman's result of of YUM yeah so yeah yeah you can't you keep right so if you pick your your double your own so Dale Sherman's result is if you pick the Omegas from from boosting you don't want to go back and refine your alphas only you got to go back in okay alright so this theorem is in terms of some norm defined and I mean we can come up with a much stronger form of this theorem in terms of the LM fini Norma between the functions but but these but these features have to be sigmoid like this again this is this is if you're if you're going to nitpick about about my choice of of function norm here this gives you a result in terms of the elevator so you buy that it's enough to to just fit a fixed function in the space that you don't need a universal claim about the whole space that I that I convince you that this is a good enough thing great I'm gonna skip the proof the proof idea it basically boils down to coming up with tail bounds for for a zero-mean random variable in a Banach space it's a bounded zero-mean random variable in a Banach space just replace this imp with this random variable and and then you apply standard results from there I'm so everything I told you about so far about the random kitchen sinks was about fitting functions but really we're going to be fitting data and using a standard decomposition of the empirical risk sorry of the defect between the empirical risk and the true risk you can show this bound so if if F hat is the T term expansion that you derived by looking at n data points and minimizing the empirical risk then the true risk of F hat compared to the true risk of of the of the best risk minimizer decay is as follows ok 1 over square root of t plus 1 over square root of the number of data points that you looked at so so the one over square root of T comes from the previous theorem this one over square root of n is a standard result from from learning theory this is no it's not it's not a uniform convergence result this is this is a convert result about this optimizer ok it uses the uniform convergence for this part of the decomposition this part as that was argue as I was talking with over about is only needs only needs a point wise ok so let's go over some some experiments here's the adult data set it's a relatively small data set that but he uses it this is the number of terms that we add in the expansion this is ADA boosts testing air so after adding a few terms about 40-50 terms is enough for a to boost it plateaus out to about fifteen percent error for us we need to draw a lot more random features to get the similar error rates ok so so ada boost got there faster with many many it was got there with many fewer terms we have to use a lot more terms to get to the same accuracy but we got there much faster our optimization problems much much faster it boosts does this pretty heavyweight iterative thing where it has to touch more or less the entire data set at every iteration we just touch the data set once in our in our least squares solution so this is the runtime as the number of feature increases adaboost takes a lot of time this is on a log scale we take very little time and in fact let me combine these two graphs together to this graph this is the amount of training time versus the amount of error that you get okay so even though we ended up using a lot more a lot more terms in our expansion we're still much much faster because our optimization procedure is much faster for a given area here's another data set this is data coming from an accelerometer from hip worn thing that detects your physical activity stop date abou staffed ur about a hundred iterations because it just was taking too long too long whereas this thing just the random kitchen sinks kept kept on kept on ticking and again you have a couple of orders of magnitude less time that you spent for the same error rate another standardized data set similar thing again a few orders of magnitude for for similar areas and that's consistent across the board here's uh here's a face detection experiment we compared ADA boost with haar wavelets against the free a random features what's neat about this comparison is is that you can't train for your random features with eddie boost very well it's a hard weak learner to fit it's hard to fit sinusoids to to data but we're picking them randomly in our case so that's easy so so part of the benefit of this random kitchen sink trick is that you can start using features that you wouldn't be able to use with ada boost because you no longer need to fit them to data that's convenient so we get slightly better performance than a TI boost on on on our test accuracy training is much faster seconds instead of minutes but we do use about a factor of 10 more in in weak learners and again here's your point John about about how at runtime d is what's important in these types of experiments we were hoping to have a fast trainer and there are lots of tricks that we started using in a phase detector that we built that can that can avoid you having to slide the window but the detection window over the whole image there's a there's an optimization that happens where you can prune a lot of the search space for for the sliding window space so that's how we get around that kind of slowed down yeah under random features I hopped on my school waves so you're doing scary so what do you see these varsity these because it seems like you know the way over come to be define Sparsit about fuzz and then get rid of those random guys you never you yeah yeah so so that's that sort of experiment I don't report on them here but one idea was instead of least squares just use Pegasus and hope they gets Parsons out of that I can't find a good setting of the parameters of Pegasus to get as good accuracy as i get with with least squares so little girl is in Swedish yeah but then those problems become huge and I don't have a I i would like to ask you for it for a really large scale of 1 regular I solver I think there's a couple out there just don't haven't talked to anybody who is we could just recommend one do you I'll fly similar things with em nest and this one we were comparing against boosting by filtering which is a much faster version of boosting where you instead of touching the entire data set at every iteration you just touch a random subset of it and again you see similar types of results were about a hundred times faster similar accuracy but you use more features I'm going to skip this so this is I can't really talk about this part but I think it's neat that Intel may consider using this in something one day right so here's here's the lesson from this part of the talk so here's typically people the way people fit these nonlinear decision boundaries you run a minimization over the weights you run a minimization over the parameters and I just here's a caricature there's not mathematically the caricature of what we've done is is minimize over these weights but randomized over them they'll make us and prove that you get very similar reasons so for the next few minutes I want to talk about less baked things that I've been working on for the past six months or so unless our questions about about that stuff where we can so here's here's here's a neat trend everybody is doing semi-definite programs for everything and getting good results as long as they have 10,000 variables so it'd be neat to come up with a way to solve something definite programs faster so these semi definite programs typically take this form you want to minimize a convex function over matrices subject to a polyhedral constraint so this is a linear constraint on the matrix and you want the matrix to be positive definite okay so so this blue thing is is represents the cone of positive definite matrices and the problem is while it's it's polynomial II only polynomial hard to perform minimization is like this it's still it's still it's still hard to do it on computers today we don't have very fast solver so so it's a challenge to come up with with good solvers that that that can minimize things over this this comics come so a trick that GMO baczynski pointed out to me that they'd used in a paper is to replace this set in the optimization with the polyhedral set these are random polyhedral set that's the green thing over here okay so you just generate a bunch of random vertices that are positive definite matrices and you require X to live in that cone and it worked amazingly well for them for their application and and and they didn't know why it worked well and I run a bunch of experiments and and it looks like you know seems to work well for as long as your optimum is not on the wall of this count as long as it's not an extreme point of this cone it'll work amazingly well and if it is on the extreme point then you can still get within you know some epsilon of it with high probability so what can we say about this type of this type of thing so here's the theorem about it actually before the theorem let me tell you let me tell how one uses this trick so we just replace the positive definiteness constraint with this constraint this is the this is the constraint that X has to lie in this polyhedral cone whose vertices are these randomly drawn vis and and just to say that it's a cone means that these weights have to be positive okay so so now if F is if F is linear for example this turns into a linear program if F is quadratic this is a quadratic program you can solve all of these things really chaps and this graph is a simulation that shows that actually a lot of these matrices that you draw from this positive definite cone do end up being extremely close to this random convex polyhedral cone so the theorem is and it's still in flux I think some of it can be improved is that if you if you're given a target matrix X not so for fixed X not draw a bunch of random positive definite matrices from the Wishart distribution construct this cone okay so this is just positive combinations of these random wishart matrices then with really high probability the target matrix is close to this convex polyhedral cut as long as the number of random points that you drew is large enough and large enough of course depends on how accurately you want to approximate the target matrix and it also depends on on this guy which which quantifies how close the target the target matrix is to the boundary of the of the comics come okay so with this you you just you now have a tool to convert Harry semi-definite programs into into optimization problems over random polyhedral comes like like turning a semi definite program into a random linear program so that's that's one thread of research here's a here's another thread of research that I don't know if it's going to pay off but but it's all sci-fi and it feels good to work on it so it turns out if you take a normal CPU and you drop its voltage below the voltage that the instruction manual tells you to to run it out the CPU will still run but it'll make mistakes so and you save a lot of power power consumption drops somewhere between V squared + V cubed right but you get to drop the clock oh this dependence on the clock sure sure c-suite a feel so so wouldn't it be neat if the next processors that you actually i'm totally not allowed to say it that way wouldn't be me if in the laboratory somebody were allowed to somebody were to build a processor whose floating-point unit ran at a lower voltage saved a lot of power but made some mistakes here and there or made a lot of mistakes so actually that's that's explored this is this is not an entirely new idea people have been building have been prototyping these these circuits where were they they're designed normally and when you drop them at low voltage they have this this little shadow latch that detects whether the circuit is is misbehaving and if and if the circuit detects that is misbehaving then it'll flush an instruction pipeline and reissue the instruction anew and raise the voltage it a little bit higher okay so so this is this is the hardware approach at at resilience on under halted processors yes only for unit the unique and just deliver an N and I will take care of it later and he stopped not cause the active bug which I'm sure you know about either results are all I've heard about it yeah um yeah so there are various ways to notify the the software layer that that an error has occurred there are ways to mask it at the hardware layer by just reissuing the instructions and not letting the software worry about it but but here's here's another idea let's just get rid of the overhead of the shadow latch that's that's taking up power it's taking up die area and it may even force you to run stuff at high voltage just to get the shadow latch the word correctly and let's expose all the errors to the software the floating-point unit will will not just return Nan's when it's made a mistake it will just return the garbage that it computed it'll just say a plus B is equal to something totally random but now let's design our algorithm so that they can tolerate that type of air so here's here's the idea so let's start with a classical combinatorial problem say bipartite graph matching okay so this is standard problem let's express it as a as a linear program and then convert the linear program into an optimization problem that's unconstrained and then so none of this involves computation this is all pen pen and paper transformations and then to solve a bipartite graph matching problem let's just toss this unconstrained optimization problem into a stochastic gradient solver the reason is that stochastic gradient we know can tolerate noise and the gradients so whenever we compute the gradient which is where you spend the bulk of the computation when you're doing this type of minimization drop the voltage feel free to compute a really noisy gradient and then do the update at normal voltage yeah so it depends it depends what regime urine if you're in a regime slightly below design voltage the errors that you get our timing errors and their random only in the sense that that they depend they're hard to model in that they're the result that you get out of the FPU depends on the previous result of the FPU return but if you're far below that then you actually get transistor noise which which is more naturally modeled as a stochastic source and here's here's some preliminary results this is quite preliminary so we built these this this Hardware simulator that actually has a spark processor on an FPGA and then the FPGA injects noise into the output of the floating-point unit so here's a least-squares solver this is just the least squares silver from opencv if you drop the voltage of the cpu and inject all these errors billy squier solver starts returns returns really noisy results so this is this is the the difference between the output of the square sauver lining at low voltage and the optimum okay and you just get these very large residuals but using our sagacity gradient solver no matter what the error rate you just nailed it result eventually similar thing with bipartite graph matching if you use open Seavey's earthmovers distance solver at at low voltages it'll do quite poorly whereas ours does well it doesn't get a hundred percent yet because there's a bug but basically its performance doesn't depend on the amount of error that you inject so we really just cut we are taking longer to compute to compute these results because the gas the gradient is obviously slower than then say the simplex method or or the SVD in this case but at least we're getting robustness right now yes would you need a single arrow to multiply the result by a by a few hundred by a few hundred orders of magnitude yeah it doesn't happen or you can recover or luckily it does happen and we can recover Wow yeah that's right yes do you have a comparison between like the time loss and the amount of power insane so what I can what what what I do know is that from here to here corresponds to about power savings of so this uses about two percent of the power saving the two percent of the power and this uses about a hundred percent of the power okay so it's a fact over 100 in power savings the amount of time that you spend is just ridiculous this is just not a worthwhile technique right now so you'd end up waiting you end up using up a lot more power right now just because you keep running this stochastic gradient solver 442 you end up using more energy because you waiting for your computation but the trick is so this is a motivation for for us to develop faster stochastic solvers right instead of just following the gradient let's let's do let's let's try conjugate gradient methods or or second order methods or anything other than the gradient direction probably will help you there are some very low resolution floating point numbers like a terrific data at about eight bits yes so there was codex if we do that yes under I power may be interested that's right that's right so an alternative is to just compute have your FB you be be narrower and then stitch the output together later I wanted to before before I get on my pontification slide I wanted to acknowledge some some collaborators from from various universities and a lot of people who have talked to about this stuff for the past couple of years we've been very helpful so I part of the flavor of this talk was about randomizing things instead of optimizing things and just about generally doing less work and hoping that your random number generator will will just get you the right answer and turns out we can prove that it often does and I don't I like digging back into the back Olivia literature and finding the root of some of these ideas like you saw with the neural network picture and I was trying to figure out why more people don't randomized things and my literature search there took me to the original source and this support for both ways of doing things of optimizing really hard or just throwing caution to the wind and I can open for questions a few if you'd like I probably won't be able to answer listen don't ask yes but before but if you use words by thirty percent i can see reduce power x factor of two but the center of 20 either it's megahertz always black magic well so certainly um if you drop voltage by a factor of two you're you're dropping power by a factor of four so um but you also get to run your clock more slowly under this scheme final I can see you I can see underclocking loop your users power yeah very stuff I'm sorry I yes yes um the rest of it is either black magic or wasn't all the megahertz well so I I I failed to actually manifest any black magic here because I admitted to you that the D stochastic gradient solvers actually end up taking a long time right so so don't don't be too impressed by these results and don't don't think I'm some voodoo master this is just a first step toward toward getting to tortoise an algorithmic way to tolerate noise in numerical algorithms um the the rest of it this idea of making of becoming resilient to undervolt that's that's standard in classical people have been solving this at the circuit level for for a decade.the the innovation is to do this at the software level using tricks from from the machine learning community yes sting versus randomization experiments you mentioned the booting thing didn't actually look at the whole data set look at a random subset I was wondering why that didn't help more and also related to improve things about randomized screens yeah so the actually um Joseph Bradley is the one who came up has all the results on on randomized boosting schemes boosting by filtering is his work he has bounced on on how well it does and there again you get the 1 over square root of T type of thing and my sense is that you just when you'd when you're gonna train a weak learner you just need to look at the data if you're going to pick the weights using the greedy a taboos method that's that right so that's almost read right so that's that's their trick right so so if you will they are picking there there weak learner randomly just like just like I do except they pick it by looking at a random stuff their randomization is by looking at random subset of the data the way they add these weak learners to the final function is is by the stage wise thing and I'm still insisting that that the stage wise thing is what kills you the stage wise things only you're fitting on the entire dataset once you pick the stage wise thing is that when you pick when you pick the weight the optimum weight for the weak learner that you just learned your your again looking at at a subset of the data set you're always just looking at a subset of the data set in the stage wise thing yeah but when you're is based in the future no and when you're paying um in this work you mean yes so there's no testing and training here this is your earth thank you in your passion yeah here no this is just a randomly generated lease cars probably it's there's no there's no don't think of it as a machine learning problem son the stochastic gradient is a machine learning tool but there's no data fitting going on okay", "t_JcmSjsTCM": "[Music] [Music] take [Music] [Music] [Music] [Music] take good [Music] [Music] [Music] your jug is a heartbreaker [Music] hey girl [Music]", "vVaEc64VZDU": "Somebody's Heine' Is crowding my icebox Somebody's cold one Is giving me chills Guess I'll just close my eyes Oh yeah Alright Feels good Inside Flip on the telly Wrestle with Jimmy Something is bubbling Behind my back The bottle is ready to blow Say it ain't so Your drug is a heart-breaker Say it ain't so My love is a life-taker I can't confront you I never could do That which might hurt you So try and be cool But when I say This wave is a waterslide away from me That takes you further everyday So be cool Say it ain't so Your drug is a heart-breaker Say it ain't so My love is a life-taker Dear Daddy I write you In spite of years of silence You've cleaned up Found Jesus Things are good or so I hear This bottle Of Steven's Awakens ancient feelings Like Father Stepfather The son is drowning in the flood Yeah Yeah-yeah Yeah-yeah Say it ain't so Your drug is a heart-breaker Say it ain't so My love is a life-taker", "QAkbtBShyp0": "hey everyone how's it going Eric here with peer pressure we're gonna be showing you our top five tips and tricks for getting the most out of your flower rosin so whether you're pressing at home on a hair straightener or using an industry-leading press like our peg speak rosin press we've got all the tips and tricks so you can get the most out of it tip number one quality in quality out this is the most important thing that you should know when you're making flower rosin or any rosin the better the material you're starting with the better your results are going to be and the fresher it is the better the clarity as material tends to age it's gonna oxidize and you're gonna find that that rosin is gonna come out darker so if you're looking to make the best flower rods and you possibly can get stellar material and press it as soon after its dried and cured as possible tip number two relative humidity makes a huge difference with flower rosin now you can go check out our other video where we did the full in-depth test but what you need to know is that your flower should be at least at 55 to 62% humidity before you press it this makes a massive difference in your yield so just make sure that your material is properly hydrated before you press it tip number three choose your temperature based on your desired results so we encourage a lot of experimentation and you're gonna find that most people are either pressing on the lower temperature range which is about 160 to 190 degrees Fahrenheit or the higher temperature range which is anything from 190 to as high as 250 degrees so depending on what kind of consistency you want to achieve that's where you're gonna set your temperature on the lower range you're gonna be looking at more of a butter on the higher range more of an oil or a shatter so when you have some material that you're playing around with we encourage you to try both ends of the spectrum and see which one you like the most tip number 4 choose the right micron bag type now you can absolutely press flower outside of a bag and get great results but that's usually only a gram or two you're gonna want to press in a 90 or 115 micron bag or at the most 150 we recommend not going any lower than 90 because that's when you're gonna see a huge drop-off in yield so if you're pressing flour in a bag make sure to stick in that range and finally tip number five if you're looking to make the highest quality flour on possible you absolutely don't need to overdo it on the pressure now you can definitely over press your flour rosin by applying way too much force at your bag and in fact we've seen great results not only with yield but of course with quality pressing at the lower end of the pressure range anything from around 550 to 1600 psi at the bag so when you're applying way too much pressure in the many thousands what can often happen is that you're pushing out plant fats lipids you know the waxes as well as all of those undesirable plant materials into the rosin really over pressing it so if you're looking for the highest quality rosin we recommend trying the lower pressure range and seeing your results thanks for watching we hope that you found that was helpful for more tips and tricks make sure to subscribe to our YouTube channel and follow us on social media at Goa peer pressure thanks again for watching [Music]", "WE4H-gXnGJA": "yo what up boy bacon babe today I just wanted to do not really a Q&A but kind of respond to some of the questions that I've been getting in the comment section some of the comments as well we're not necessarily questions they're trying to tell me different ways to do things so I'm not opposed to taking advice only a fool would be but I am opposed to taking advice from people who are just talking but don't do if you don't actively press rosin don't tell me [ __ ] about how to do it because you don't know how to [ __ ] do it yourself I actually do it one of the comments that I had was that on one of my videos where I left a pre pressed puck inside the plates for too long while I was trying to prepare the camera so he did the material up a lot and when it came out it was a little bit darker than normal so a lot of people like to comment on that one and a lot of people are you know one person told me use less less pressure because you get more plant material in your end product and just let the heat lose it all out well that time [Music] Oh [Music] whoa whoa all right I'm still not over being sick completely sober all right so that's what I was saying um that's Bob that's backwards straight-up backwards you want more pressure less key you're not going to get more plant material in your product by using more pressure that's just not how it works the rosin bags that I use are 160 micron there from rosin evolution and they work perfect with my pre-press but you guys have to remember that when you're pressing out rosin the filtration bag excuse me those bags are not for filtration actually they're guesswork [ __ ] look they're just for containment so when you have that puck of flour and you go to press it it wants to expand and as it expands it will reach outside of those pressing plates and then you're not pressing that material anymore right so what the bags do is they don't filter to keep the plant material inside they just they keep it so that the plant material doesn't expand too far out because you could press nugs without any bags and still get all the oil without even collecting plant material the plant material only stays right along the very edge of the Google oil so again those bags aren't for filtering anything they're just for containment and make it a little bit easier what else pressing temperatures so temperatures - depending on your material you're gonna start as low as 180 and go anywhere up to like - 30 to 40 the fresher the material is the less heat that you need to use also the less time you need to press it if material is not the right humidity you're gonna have to press it for longer and you're not gonna get as good of a yield usually 60 to 70 percent relative humidity for your but it's good to start with and I'll usually press it fresh material like starting around 185 190 and if I'm not getting the yield I want but I'm getting the quality then I'll increase the the temperature by about 10 degrees until I start seeing the yield that I want now when you increase the temperature got to remember it's like a it's like a sliding scale right and right where they cross those two lines cross that's the perfect like settings right temperature time pressure so what you're trying to do is you're trying to get to keep high enough to make all of the oil lose out without degrading the actual material right so it's heat that degrades it not pressure so ideally you want as much pressure as possible without blowing out your bags right or breaking your parchment with as little heat as possible necessary to facilitate the flow of oil out of the material usually that ends up being around a thousand psi on your plates so for example my plates are 22 square inches right now to hit a thousand psi you know you're gonna have to get up there around like 13 14 tonnes because you take the max pressure that you're putting out so let's say you're putting out 10 tons that's 20,000 about 20,000 pounds of pressure now you divide that by the surface area of your plates which for me is 22 right and so I end up needing to be somewhere around like what is it I think like 15,000 psi or 15 tonnes will give me about a thousand psi so do the math on your own the base of yes the max pressure you want to get to and then you just change your temperature based on that so the higher the temperature the faster your product will start to flow right now this is where it starts getting tricky and it's a sliding scales because your temperatures too high you degrade your product but if it's too low then it has to sit in there and between those heated plates were too long even though it's a lower temperature it has to sit in there too long in order to start flowing and get the final yield so now you end up with the degrading product so like I said it's a sliding scale you want to have a high enough temperature so that you don't have to press too long but you don't want to have it too high to where it's degrading the product and then your pressure you're just going to keep ramping up the pressure until you don't get any more yield out of it so that ends up being like like I said fresh material 180 to 190 and about two and a half to three minutes for fresh stuff and then for older stuff it's usually I go a little bit higher and press rike around 210 or so for about three or four minutes but it also depends on the type of rosin that you want so a lot of people would see this and be like oh man that looks like like a big bunch of doodoo but this is your super Turkey stuff that's been pressed out at about 180 185 so you're gonna get more of the oily sheen on there right you're gonna get a lot of Terps and it comes out more like a sauce and this is cured this has been cured for about two weeks now and so this is just super super stable and this is really [ __ ] bomb but a lot of people don't like material that looks like this and keep in mind this would be super golden if I were to thin this out but a lot of people want like ghost rosin which is the stuff that is so clear you can almost see through it but people are into the purple rosin now there's a lot of trends but you can't let the color dictate everything because it's not just about the color right because of this stuff fresh bud pressed out at 180 there's a little bit darker and more Turkey than stuff that I've pressed out at higher temps of the same bud I pressed out at 2:30 you know and I've gotten slightly different looking material it was more shatter consistency way lighter color even though it was a way higher temperature right almost 40 degrees higher same pressed time actually was a little bit shorter with the higher temperature because it started to flow faster so I didn't have to press it as long you know this is some more 190 190 180 but you know you can get all different types of colors and consistencies and the point is that it doesn't necessarily dictate the quality it just dictates the parameters that were used when they pressed it so let's see what else you guys want to know cover filter bags temperature pressure time collection don't collect on anything too cold I made the mistake of collecting on top of some dry ice there was a protective surface in between but still the cold got the the rosin too condensate and create some moisture on top of the rosin and that's just because it got too cold so now I added moisture where I don't want to I want to remove it so that's a conditional time to cure and get that out properly but make sure that you have I would say like a couple is regular ice packs and then put it like a cookie sheet and I still look on mat on top of that and that should be enough to keep it just cool enough to collect easily but not getting moisture on it Oh a lot of people asked me about the the paper I use PTFE squares so I don't use parchment the reason I don't use parchment is because 90% of the markets parchment paper is covered in silicone silicon is very very easily broken down by a lot of different terpenes specifically D limonene which is very high and a lot of Turkey buds and so when you go to press you'll notice you have your little circle your puck of Bud and then around that puck there's like little box where it looked like it seeped through the paper or the bud is eating through the paper and that's actually the the terpenes eating through the silicon and then getting through the parchment paper so what happens when that happens is the silicon gets broken down by those terpenes and then when you're scraping up your rosin you're actually scraping up bits of broken down silica that get into your rosin and then when you when you ignite that or vape it burn it whatever that's toxic silica is extremely toxic when ignited and ingestion so those of you that are pressing on silicon treated parchment paper still chances are they're silica in your material you may not notice it but it's there now it seems a little bit easier to collect material off of parchment compared to PTFE if you do not have the correct temperature to collect it on if I were to place ice packs underneath this and then collect my material it comes up extremely easy way easier than parchment nothing gets stuck to this it's the most nonstick one of the most nonstick surfaces known and unlike parchment paper that is only heat treated or heat rated up to like 400 450 degrees PTFE is heat rated up to 500 degrees and pressure rated up to 500 pounds I think it is which is pretty significant nothing breaks through this in my experience I have not broken through a single PTFE sheet so that's why I use these I got a pack of one thousand four by four squares for 125 bucks from turkey roof comm or you can check out the version PTFE roll if you guys are doing bulk compressing with larger presses get a whole roll of this this is 14 inches across and I think this sheet was like maybe safety ET or something like that and this was like I think 100 something bucks it's worth it so think that covers and yeah it's depressing show me what you guys got if you guys have any questions comments please drop them below this was a very impromptu video I was gonna do like a whole little script and make sure I covered each bullet point but dabs happened and then I just started talking and here we are now so that said you guys have any questions about processing material growing anything like that drop them below I reply to almost every single comment if it's worthy of a reply show me what you guys would like to see in the next video and if you guys have any material that you want processed and you know the Southern California area hit me up and I'll press your stuff for you and I'll show you what it's like happy dabbing I'm about to hit this and I'm gonna be out of here [Music] [Music] we're hearing all the stuff in my email probably today because I'm impulsive and I have money right now all I need is another control box similar to the one that I built for my rosin press a PID controller and a plug-in spot for the coil that I already have so it shouldn't cost me a more than about a hundred bucks and I'll be able to build the control box for for the email and I won't be torturing anymore because [ __ ] this shit's annoying especially because I smoke in the grow room now because my wife is pregnant and I'm not trying to have her getting all secondhand smokey up in this [ __ ] so I don't smoke in the living room anymore which is where I usually chill and play video games so I got a run in here to take dabs or smoke and then run back out there in between rounds or games it's like man I wish the [ __ ] was just always lit always torched up ready to go I could just come in here drop it down and be out but I got to spend like a good minute and a half [ __ ] torching this [ __ ] up to get it clean and then let me cool off but it's all good oh yeah the I don't have any packs in here with me but I did make these little gel packs I rosin in coconut oil this shake is for I take four of those it's a wrap a rat I was in Chicago for business with my lady over this past week and that was what I would do I didn't take any flour and dab anything to smoke at all all I took was those pills I was a pill-popping animal and she would hate me like 3045 minutes after I take them and it was just kale like chili Hey [ __ ] dog barking for no reason Cheers [Music] I can't wait to not be sick [Music] whoo [Music] alright that's it for me they can be about catch you guys next time", "ezRzBxPQpbI": "when you know that this is $75 and you're getting a big scoop of it I'm thinking to myself and that's like eight bucks right there [ __ ] it this video is for mature audience if you're not a mature audience this video is not for you what's up everybody Cubans here and today we are going to get into some rather expensive concentrates I picked up a gram of some hash rosin today that was priced at $75 and it is one of the most expensive grams of concentrate I have ever purchased so let's take a close look at what we got I've had a gram of Summit connoisseurs genetic and concentrates Tiger's milk and this stuff looks like a nice little slab of peanut butter I'm pretty excited pretty excited abot both so you can get a look at the packaging here this is that Tiger's milk and it's testing at about seventy nine percent THC and then it does have a little bit of CBD a in there but you know what I am so excited for this let's just take it down where are my clothes at let's get a nice nice little scoop here out of the email all fresh and clean and I'm gonna see how it does does [Music] um yeah that was really good I'm gonna actually throw that on a courts now I'm gonna throw that around on a few nails and see if we like it but just to compare I picked up some cheap dabs this cost 75 a gram for some hash rasen 75 bucks for this it's pretty expensive most most of my concentrates I usually find deals and get a lot of good looking stuff like this this is some live batter I got for about 15 bucks a gram one-fifth of the price now sometimes I find live sugars live diamonds of batters around that 15 to 20 dollar price if I'm getting it on a deal it's not the everyday price so begin the deal you can get this stuff cheap so it's really interesting to pay five times as much for a single gram of concentrate I can't I can't believe I did it it's still interesting to me that this costs five times as much as this other gram of some delicious looking stuff but some hash rods and it's hard to come by especially here in Colorado Springs it's not super common to find rosin hash rosin so I guess you got to pay that top dollar because it's so rare but let's get some more dabs of this cone I'm not done with this yet let's see what I'm gonna do some cold starts when you know that this is $75 and you get in a big school but I'm thinking to myself and that's like eight bucks right there okay [Music] it's throwing out a couple bucks in that [ __ ] it [ __ ] it and get a little bit more I need about three-fitty oh we got it okay okay just pricey tab right here damn it all right let's get a nice cold start here and see if it's super delicious on the court oh damn well damn okay okay that's good stuff so I just stabbed it a couple of times here and it's pretty good stuff but is it is it worth it is it worth paying five times as much as I do for some good live banner is it worth is it worth paying five times as much to get some solvent once that's basically the major difference is the fact that it's solvent list most concentrates are made with a solvent it's the easier method it's the most common method but with rosin they use heat and pressure to make it so they just basically squish the THC or they squish the extract out of the nugs or out of hash most of the time they'll make a hash and then they'll press the hash and that's what this is this is a this is hash rosin so it's a little bit more expensive and my understanding is the yield is a little bit lower it's it's a process to make the hash and then when you press the hash you wouldn't get the same type of yield as if you were doing like a butane or propane extraction so I think part of the process is it's a little bit harder to get high-quality product and you're typically going to get less with whatever amount of plant material you go into the process with so I think that affects the price it is pretty rare out here in Colorado Springs not a lot of people are pressing - I can find it in a few places some places have rosin and half chosen but it's not as common as BHO and pho and there's so many dispensaries that grow their own bud and they do their own in-house extractions so something like this I can I can pretty much get really cheap any day of the week if I'm looking around so for me to pay five times as much for this it's a little little far-fetched I honestly I'd rather have five grams of this and haven't done but yet and it's just I like volume I like to do a lot of dub so I'm gonna go back and forth on these we're gonna compare I just want to shave this I got this the same day I kick it costs literally one-fifth the price I'm just gonna see if it's if it's it if it's 20% it's good like it doesn't make oh god I just paid so much money but anyway anyway anyway yeah that's that's solid for the price that's sell it for $15 grand look um I I think it's good I think that that the rosin is good I'm gonna dab the rod let me have a bucket that was that was the I don't even know of the other strain is [Music] [Music] I mean it's definitely better [Music] the Robins better smoother and tastes a little better you can't really compare the highs because I'm better than both back for it but it's all about the taste for me and it is smoother is it pay five times the price smoother [Music] I don't think so because it's not it's not five times as good yeah I don't even know how you would get five times as give it something like this this isn't the best this isn't the best cheap batter that I've got but for something to be five times as good as this would be pretty impressive so this is not five times as good probably not worth five times the price if I see it on deals I may get it but I don't think it's worth anything that's worth three times the price two times the price even thirty forty five bucks I get why it costs so much it's a it's a little bit less common not as many people are making it and you get lower yields with it I get why it it costs more but it's not currently it's not at the price point where it's super worth it for me to only buy rosin maybe if I had a press and I could get amazing yields myself but I don't grow and I don't have any interest in growing so it's like I don't have to buy all my nugs anyway to smash it so kind of interesting I like finding deals so maybe I'll get excited about finding a deal in the future but yeah this isn't actually the most expensive gram of concentrate I've ever purchased a few years ago I remember purchasing a I purchased several grams actually a full melt bubble hash that I got remember the dispensary but I got some full melt bubble I think it was about eighty eight bucks a gram I think I remember paying just under ninety a gram it was really really good stuff it was like some of the craziest stuff I ever spent I I gotta go back and look for some pictures of it cuz it was a few years ago those full mount bubble hash grams I got were like pretty good but after I got a mouse this isn't worth eighty eight bucks a gram what the [ __ ] did I just do so I kind of feel that way here what the [ __ ] that I just do but we made a video that's what the [ __ ] we just did so let's do a few more dabs of this [ __ ] might as well I've only taken out several dollars off of it I don't see if we can take several more I'm interested in the cold start here so let's do that [Music] here's a few dollars [Music] yeah this is good stuff still I wish I could afford you dabs well this was actually pretty fun and I'm pretty blessed after all those naps and I'm glad I finally got to try this rosin up and see it in a bunch of dispensaries even if I did pay money for it but if you liked this video be sure to subscribe for more content in the future and check out my other social media I'll have links in the description below and for my live streams tune in to twitch at twitch.tv slash Cubans if you tune in at night I'm probably live right now getting high dabbing some much cheaper than than $75 a gram rosin unless you tuned in very shortly after this is made that I might have some of this left but uh come hang out and we'll see you next time [Music] I would say perfect balance but I'm going [Music] [Music] [Music]", "xU1y9mX6jio": "here we go what's good YouTube welcome to another live stream can already take a few dabs and hang out it's just got done prepping for another hash wash damn it getting cold in here too because of that I might have to go grab my jacket but um yeah got the AC cranked to 62 degrees that the ice-cold water in the reservoir I've got a bag of ice for the washer and I've got six more bags ice out in the freezer I need to get a bigger freezer and all the materials well so later tonight after I get the kids down spend some time with the wife I'm gonna come back out here and knock out that bubble hash run uninterrupted probably gonna do somewhere between six to eight runs on it depending on how it looks and how everything is coming out [Music] anyways let's go ahead and get a - go in here I hope you guys are having a good day [Music] that was not clickbait by the way for those of you this joint this is a 104 grant or 100 Ford's a hundred grams now mixed strain hash frothing some cookies some sweet cream some zukie and what else sweet cream cookie sweet cream zoo keys and orange cookies that's what it is [Music] sorry I had someone from work pop-up I don't look out real quick [Music] all right cool [Music] anyways let's go ahead oh it's so good yeah I need to go ahead and fill up a little bit char with that [Music] this was some 60 micron parabola for edibles [Music] this one still has a cush cake in it and this one has on a fish cake so those that's my head - cooks cake in a jar [Music] earth is [ __ ] cold and her alright so I would like you yet in my day did you imagine dabbing that whole thing just like here we feel [ __ ] half the Banger ease and the easily I wouldn't even try it it'd be a waste and a half that's good enough for me enjoy that for the week [Music] the 160 press was man very little in there so that's what that looks like when I end up putting it in the jar and then a lot of people would ask me how did you get that [ __ ] rossington to stick to the bottom of the jar look so neat [Music] here's I take the glove and you're just going to push it down around the edges towards the center you're doing a cold room that's good rosin a clean glove and just like that now it looks like that [Music] with that's hard to see but yeah that's how that comes out when you end up doing that push it down it comes out real nice and clean looking perfect for grams or so it's upload a bird [Music] all right so I gotta move my dad station over now I'm not pressing it back over here well I like it Kiyo is : grab yeah you can do with parchment - I have a little bit more control with the glove I feel like and it rarely gets sticky but parchment works to a more sticky stuff I will use parchment I'm gonna go grab a hoodie real quick [Music] [Music] whoa [Music] [Music] [Music] [Music] [Music] [Music] [Music] whoo my tears my eyes [Music] Oh [Music] oh man the big dads always get my nose my eyes [Music] Oh [Music] about three days away from the harvest yeah we're on day 54 honestly I think I could pull them down tomorrow or the next day I want it probably gonna go ahead and harvest them on Monday [Music] today's Thursday tomorrow's Friday yep I'm go ahead and harvest them on Monday [Music] five six out of the gate 58 [Music] [Music] I got some pictures of the plants I haven't even posted to Instagram yet pausing to share them with you guys real quick [Music] [Music] turn down the lights already this whole side is just [ __ ] Mileena should be up here these ones dried back a little bit too much almost too much but not too two months still prosti of [ __ ] 12 cars black out there my Spade going over there [Music] [Music] [Music] [Music] [Music] [Music] [Music] so that's the current status in the Guardian you know just like oh my flesh for usually you're about seven days it's that but you know [Music] I had issues this run just like Ross Leafs his issues because he's in cocoa they're harder to detect early and so he faced the consequences of the buffering that cal-mag provides in coma cocoa I wasn't in at nursery iam so I was able to turn my issues around a little bit faster but you know basically what I'm saying is you know I had issues this run and the crop is still coming out pretty good so next run you know those issues will be corrected I'll make a video covering the issues that I ran into this grow and how I plan to correct them and I'll throw it up on the patreon hopefully you guys can learn from some of my mistakes you know I talked about on the show doing your own research and shape one of the things I researched if I should have used top-down broward rivers and when I talked to mango tech Abadi he told me to use two gallons per hours to avoid clog and that's what each use is but then I ended up getting water channeling that is too fast on the float and I cost a lot of issues I never really fed high enough of an ECE to have the easy build up nor did I ever push the plants that hard to cause them to have any lockouts it was all because my irrigation was wrong from the get-go with the water swim [Music] are you taking off right now that's one of the changes I'm implementing is not only am I going to get rid of the 2 gallon per hour but I'm going to go all the way down to like a point 3 gallon per hour dripper that could eat drawstring and stuff up here so yeah that'll fix a lot of issues right off to that that I believe uh Charles and pastor [Music] that Daffy got me hired balls rolling in on me now [Music] oh my bad I look here looking up and showers my phone [Music] there's no broad my issues going on in here all it is is either a they were different cuts or they were microclimates right he talked about the water channeling so you know phosphorus deficiency can learn to purpling stems and the plants that express themselves differently also has purple stems where I was the ones that didn't have the red bugs didn't have purple stems so I think it was more of a microclimate issue water channeling issue causing different uptake different nutrients and it just got exacerbated due to the lockout because of the water channeling so next one I'm doing is with wedding cake and I should be getting those clones in about two weeks so I have about two weeks to prep get the plants chopped harvested and everything I'll flip them right in the room that they're in right now I'm basically going to push the tables to one side of the room and then I'll go in and use a light rail for this up against the wall and I'll cut down on the one plant out of time and hang it on a trellis hanging from the ceiling and then I'll just hang all the plants from one light all right [Music] when my moustache hairs on the know it wouldn't happen I got trim my mustache geez so yeah basically just go one light at a time right down hole lights worth of plants hang them go to the next one cut those down hang out on a different trailer that's there the next one so I'll have three trellis nets hanging on the side of the room I'll have to retry this netting on that side of the room so after I cut down the whole left side of the room and those plants are hanging up there on that left side I'm going to disassemble the tables and walk them out of the room and then I can push that whole other table completely over to the middle of the room hang up all those buttons on the back side of the room or wherever in the room right and then basically I'll be good to go I'll get everything hung I'll turn off the shaker fan I'll just keep one of them on probably maybe turn it down to low or something and I'll just have to DEET you and AC going in there I'll be able to scrub down the you the four by twelve that's in there now scrub it down and get it reset and start making my adjustments to the room while I wait on my new 4 by 12 American Girl products they're probably they told me it was going to be issued early June like June 8th or 6 but they might be able to get it in sooner so I mean worst case scenario the room will be down for about three weeks while I wait on the new table but I'll just match all the plants on one table while I'm doing that perfect and then when I get my new table in I'll be able to go ahead and transition the plants over throw up my trellis mats and pretty much switch so you know that's the plan and like I said I'll document all the changes to the room it said of everything that I'd be like want to document the changes and yeah my keeper fee no cuts from honest genetics should be here in the next two to three weeks and solved a whole room full of wedding cake in there veg and flour that while I have a child my keeper he knows from our genetics and my new keeper you know caked up cherries wedding cake crossed with cherry something f3 I'm going to do the three-tiered setup but I'm waiting on him to send me the updated invoice for the new poles and as soon as I get those I'll be able to set it up and I'll have pretty much what I was thinking of doing them what happened to rekey poofy nose and I could basically put all of them on their own tier if I wanted and then have clones next to it or I could just put all the moms on one and then since I'm doing two tables at a time I'll just pick you know I'll have two tiers for clothes you know cherry cookies have three thank you thank you thank you I never remember like my keeper he knows from honest I don't remember what I was are either blue hurricane 27 crossed with white sunrise for the blue lion keeper and then scarlet sure is chocolate frosting crossed with something I think [Music] I trust my man on his genetics he tells me yo this is a [ __ ] you should run for a hash let's fire fire fire legs all right now you got it [Music] like I remembered it enough to purchase it and you know that's all that matters now that's all that matters now I need to show up in the garden [Music] so stoked I have I'll have the blue Hawaiian which is gonna have exotic super crazy unique blue Terps it's gonna be highly sativa dominant as far as the high goes and then the truck not the truck cherry but the cakes of cherries that's gonna be my cherry Fino right there so I'll have a blue of a cherry and then I have the gassy out of scarlet shirt so it's gonna be dope [Music] this is real the kind of light that I use for clowns are right behind either the raging raging kales and I actually got sponsored by science LED yeah so I will be getting now raging candle LEDs probably twice a year during two biggest ways that I do and it couldn't be [ __ ] dope it can really know and then every other month or so I'll be giving away a bunch of science LED swag hats ashtrays shirts things like that all be put on there grower spotlight on their homepage pretty shortly here I have some pictures that I have to take for them and type up some some information and what else asked me to leave to help them do research spectrum tuning and hash production so I'll be doing that so and then also the big thing I'll be an authorized reseller for their lights on my webpage as soon as it's up in a few days so as soon as the page is up I'm going to try and get them on there and I'll be able to resell a life through my page so that way you know they don't do they don't do coupon codes like people in the industry you got to use to coupon codes because of for flex and all that but what that does is it makes it so that you can't sell that product in brick-and-mortar stores anymore and in the hydro industry almost need brick-and-mortar stores still surprisingly to you know expand and survive because still a big commercial facilities go through these hydro stores and you know direct to these manufacturers a lot so they don't do coupon codes they're like let's say they offer a 10% discount code here in California right and somebody purchases a light from you know from me and using my promo code but they're in Oregon or something and or somewhere else where they maybe don't charge sales tax and now not only are they getting the 10% off but they're also getting like seven or eight maybe taking even up to 10% off of that because of no sales tax right whereas if they were to purchase it in the store you know the store would be able to make a profit but in order to compete with that price that they would get you know from using the coupon code the brick-and-mortar store would basically have to not turn a profit so it doesn't work out for them to be able to give people who coupon codes stacked as representatives of the brand so instead I'll be able to be a reseller and then just a very very small portion of the profit that they would make from that light will go towards me instead so I'll be an authorized reseller and then basically helping do research and you know passing that information you guys like I have been this whole time so anyways this is what we're working with they bought this is delicious delicious it's called cookie dough and it's amazing [Music] I don't suspect this is gonna last long you won't need a code to let them know who sent you because you're gonna go through my website to purchase they're working on other programs like referral codes and things like that but [Music] yeah I guess I should keep this on camera so people don't think that I'm like lying and I say I got a grandfather drive here we did well today it's real I wouldn't had a couple more but honestly you guys want to get jars of the grape Cosby and literally all I have left of this [Music] that's it but in this drawer it's maybe maybe seven grams six seven grams of the grape Cosby I think the other jars been scraped empty already yeah scraped empty great cause be gone on that one and then the hell a Koosh cake [Music] there's like maybe two to three grams to scrape out of here [Music] and this one I think is scraped completely empty yeah there's maybe like point two in here it's empty so there was 70 72 grams of the La Push cake and 60 grams of the Cosby [Music] but every time you come in here you [ __ ] promotin be real oh no no no who was it last time it wasn't beaver oh it was your boy the other one that's not good Bernhard that's who it was last time [Music] about your ass sorry yo for real alright here's the funny story that's how you guys are quick so the reason this one got called rape Cosby because [Music] my wife and I dabbed it and she got so [ __ ] high off of it it was like she got roofied like straight roofie I could have done anything I wanted to so I was like mother kali shake great cosby it tastes like grapes and knocks you the [ __ ] out like you did this it was between grape Cosby and roofie colada you know think of work 12 but you guys got to make sure you do it right [Music] it's great cause be like that lowercase G Capital are [Music] all right let's get [Music] a little credit given to me I need a label hood [Music] it's not for grams but whatever [Music] you know [Music] so we're gonna get some cookie dough on there a menace [Music] they push tape on there [Music] [Music] [Music] me up a man popular science [Music] my episode 5 right there alone [Music] [Music] there was a five [Music] [ __ ] bones knock out maybe this is episode [Music] all right hold on okay [Music] but right now it is [Music] but few days that idiot that ain't the one all right well [ __ ] it I'll get it uploaded I was trying to do it right now but I'm having a little bit of a hard time finding a you write for [Music] oh here it is [Music] [Music] that's not it either I have like a lot of different files in here that I have to go through [Music] that's three lighting maybe this is the right here number four [Music] okay yeah here it is and as this little my everybody [Music] alright I'll go ahead and get this uploaded to YouTube right you know [Music] I'm kind of done editing these I'm just gonna start uploading the RAW files and if people don't like it they can kiss my ass I don't have time to edit them together anymore that's why I put the PowerPoint how I did you know if I do the PowerPoint I don't have to worry about putting an intro and all that into the video it's just boom okay boss [Music] alright crap dude my science for plant nutrition are a cool description I'll edit that later now don't care playlist crafted science done no it's not made for kids don't have to restrict [Music] monetization please do [Music] public [Music] boom what is instant premiere oh [ __ ] we could watch it together at the same [ __ ] time let's [ __ ] do that right now I did not know we could do that that is really cool we can watch the video together yeah three minutes five all right it's 321 I'm gonna go in at 3:45 and help the wife for the day because that's the way it is [Music] I find on finishing of the loose jars oh that's the one section of illustrator [Music] this is it makes a great Cosby and Kush cake [Music] I don't know what I did mother two minutes [Music] okay a little poison for 30 minutes okay no accident since a wait okay I'll be in by say like 3:45 okay okay [Music] 59 seconds left [Music] very good dogs very good slowly giving them a little bit more freedom and hoping that they don't [ __ ] pee anywhere [Music] green jeans with Superdome [Music] Greenjeans was sooo Oberto 11 seconds 8 seconds that's called your video will premiere what's uploading and processing finished as well as the hundred percent uploaded is 95 percent process I have no idea how this is going to look for you guys I don't know how this works I've never done this before it is [ __ ] top-down so we're gonna check it out oh my god dollars people my DM right now [Music] all right you guys let me know what happens on your end when this is done hopefully I mean it doesn't take forever right now it's an remember you two processes videos one definition at a time so the low definition stuff is going to process and then like for whatever and then 720 and 1080 [Music] my phone right now it's just doing the load F [ __ ] the standard definition version we'll see whatever the [ __ ] back [Music] I'm really curious to see what this does though man that was such a good [ __ ] good [ __ ] episode and then episode 5 I'm glad we took a break because we were really [ __ ] burned out both of us and we needed a skip a week we came back and we didn't even have a PowerPoint or anything but it was still an amazing discussion Garrett jumped online from science at levy and he just [ __ ] to talk about what we wanted to talk about which was great just you know [Music] we just talked about troubleshooting it was [ __ ] awesome you know he get his perspective on things like walking into like a huge commercial facility and how you would troubleshoot their [ __ ] to figure out what's going on why certain part of the crop might not be doing its good versus another part things like that so it was really cool to get his perspective because you know as far as lost leaf goes he's a commercial facility but he's a one-man army and it's a small commercial facility it's the size of a house basically a one-story house and so and that's that's no diss whatsoever [ __ ] it's amazing what he's doing I wish I could be at his level but in the realm of commercial grows it's relatively small so you know his perspective is one and then Maya the home grower is another and then Gary brings a whole different perspective being that he's kind of more on the large large scale commercial and so it was really cool you know and go with it so unfortunately during this whole Cove 8:19 thing all of the HD recordings for webinars are being limited and only being allowed standard definition so this is not going to be best of qualities for the uploads that are going on right now from the webinars and there's nothing that we can do about that so you know [Music] check it on the patreon real quick make sure you guys got your post real quick [Music] are you guys waiting on me to take another dab is that what's going on [Music] I guess [Music] it's not what you guys I feel like a little [ __ ] a dab [ __ ] for you guys it's all you guys wanna take your dad show me his plan it's never hey SB Andy how's your day go what'd you do today need some help no I'm just [ __ ] one shot I already forgot what this one is again that's the coach tasted positive and he's more paper towels real quick cuz my house got so high I forgot to clean the [ __ ] banger after the last job that's [ __ ] I I guess I Cosby myself whoever said don't cause be yourself I can't be myself go clean it out real easy paper towels to clean [Laughter] [Music] yeah all of the rapidly science episodes go on patreon pretty much the same day or minimum a week earlier than YouTube a minimum and Matt doesn't post on lost leaf as a poster or anything like that out of respect for my patreon supporters he knows he waits again he's a week two to three weeks even suppose I mean it's not that I don't want the information out there for everybody but I wanted to I want to give some [ __ ] love to the people on patreon and economic access all that and they can pick my brain talk about it on live sessions or whatever brain different [ __ ] you know [Music] all right the tip of my II now is 395 and I need to grab another roll of paper towels real quick [Music] [Music] damn [ __ ] oh yeah [Music] oh [ __ ] [Music] in the filter [Music] oh Jesus son 525 that is a very very high [ __ ] gasps Jesus [Music] [Music] so the reclaim catcher works by basically a when you draw on the dab it gets stuck down here because all that reclaimed can't travel back up pretty much so what you end up with really is just tiny tiny pieces of the vapor that condensed back to rosin in here versus a shit-ton of replaying and if you have a really good deep-dish banger and you do low temp doubts your your reclaim actually can look really nice how well that looks but [Music] seeing us a thing FPV not everybody's email setup is the same not all coils are created equal so you got to get you know a good coil setup with this I can go three eighty and still get clouds [Music] you need some sort of cloud [Music] all right [Music] my emails from 7-10 coils and you have promo codes with them but up go my man go in the house [Music] did I even bring the [ __ ] Paper Towns yeah I did oh yeah okay don't really need to take another dad titanium holy [ __ ] [ __ ] son if you don't swap that over some glass you can get like a full email kit for around in 250 years [Music] [ __ ] [ __ ] that olive oil will get you bro oh alright you [ __ ] I'd have been dying for this dad for some reason or she dies or more yeah I got ahold of the giveaway winner and he commented all over my YouTube video and everything that following morning Jesus sweet baby Jesus my nose is already like crying thinking about these Turks [Music] Oh titanium is not 2008 oh my gosh bro oh [ __ ] that's funny I've already got throat tickle mmm take him to any dab my throats already got that little scorch to it oh but I'm a man up and finishing [Music] [Music] [Music] we [Music] that's how you beast down and down right there [Music] fire [Music] [Laughter] [Music] I'll have a lot of patience going back up there to deal with kids right now oh yeah [Music] cruising today today is flying by I feel like I woke up two hours ago [ __ ] took five dives I blinked and boom it's already about four here had to clean that tonight [Music] just blowing our play too much [Music] the function on Agentry is crazy [Music] so FPV ahead I wasn't enough cloud for you where are these jars just ridiculous [Music] [Music] [Music] well can't even open it with one hand [Music] you're saying horrible with the [ __ ] up-close [Music] so so good [Music] this lighting is any different was natural lighting appear it's that cookie dough [Music] all right you guys [Music] I'll catch you guys on the next one I'm gonna call it a night I had a bunch of other [ __ ] I gotta get around to these", "ylc16phLqDE": "i mean this right here is some live rosin bro this is really what you want to do what's good everyone it's haikai as you can see we're at the dispo right now pick up some fire and i'll show you guys what we get when we get back all right we just got back from the dispensary we picked up some real nice stuff for sure she was expensive but um definitely like i'm just gonna say now this is honestly worth it to me um we picked up some rosin and rosin right now is going pretty crazy it's a solving this extract it's like the top-tier extracts in my opinion like because it's solventless and there's no butane or anything extra added into it it's just literally using heat to get the extract from flash flooding flower or regular cured flower it's some fire bro i've only tried it once before and that was literally last night that's why i wanted to make a video about it i picked up and i want to go back today and get some more and bro that [ __ ] was i did one dab of it i only take i only took one down because i wanted to save more for the video but dude just one like it just hit so much different like i can't even describe how like how much different it hits your chest and like your head it just like feels because the the high doesn't even feel like groggy like you would from a distal high like when you get high from just slit you just get like i guess high and then you just like want to go lay down and just like i don't know want to [ __ ] off for a bit but if you take a dab of some like real live browsing bro that should just like i don't know it just makes you feel so much more uplifted and good like it gives you the actual like i don't know feeling of high like it re it really gave me like flash faster like when i first started smoking because like right now my tolerance is pretty high since i've been smoking like every day for like a while now so dab of rosin is just like up here compared to like some shatters just like all the way down here this is completely different experience in my opinion at least solving this is definitely the way to go we picked up three different types of roses actually and we're going to get into like the different types of rosins you can get we got some aged cured rosin we have some live rosin and then we also have a full spectrum rosin yeah it's going to be dope video for sure because like this [ __ ] smacks bro uh we're gonna get some cool we'll get some cool milk shots and everything you know the vibes but yeah first let's get into the aged cured rosin from the well it's a hybrid one gram of solventless cannabis extracts that's what you like seeing um get a little shot of the back as well and as you can see it says whoops it's age cured rosin as you can see um which train is it uh melon rings and yeah on the inside it gives like a little uh biography of why i like solving this extracts are really the way to go so we at lowell love concentrates but we don't love the chemical solvents used to create low quality extracts we also know that cannabis is far more than complex in thc alone and that's stripping out the rich and complex variety of cannabinoids and terpenes found in the plant undermines the whole plant cannabis experience our artisans worked tirelessly to perfect an extraction method encompassing the whole plant using pressure and time instead of solvents and volatile extraction we knew with the cold pressing of cannabis which preserves all the natural properties we can finally create concentrates worthy of the well named alaskan fire bro um this is just some regular age cured rosin it's a bit different from regular live rosin this can be full spectrum anything can be full spectrum uh it can either be full spectrum or live i'm not sure if it's live or full spectrum because age cured is referred to um the process after that and the carrying process usually involves rosin being collected in a suitable jar heated and then treated with some variation of hot or cold temperatures and then stored in a stored in the jar for a few weeks or longer and then if done well can result result in some jam that can be like some of the most flavorful important and potent uh concentrates imaginable and that's what this is exactly and yeah dude this should definitely smacks like no other bro like i can't wait to get some more of this because it really isn't that expensive like i was saying um hq rosin is from the well i think that's like one of their it's like specialties i guess because i don't really see any other age gear rather for anywhere else but yeah there's just gonna be some fire bro we'll take a globber of this um let's get this shop set up [ __ ] globber boys [Music] smoke down smoked out positive squad protect dj smokey at all costs [Music] [Music] all right bro that [ __ ] just smacks so different i i can't even describe it like i'm i'm just like so much [ __ ] iron than i was before i i i like i didn't i've been all right yeah i haven't smoked like at all really today i've like to rip the card maybe once or twice but bro this [ __ ] just smacks me so differently than just like a regular like live resin dab or something like that like this [ __ ] just makes my whole body feel so different but in a good way just you could really taste the full flavor profiling and all the chirps in them and that dab it just tastes so good dude whoa i forgot which string that was i think it was like melon rings like you could really just taste that [ __ ] it tasted so amazing i can't wait to take some more uh take some more [ __ ] globs of that bro it's some fire like seriously this it was really worth it and i think the melon rings honestly wasn't even that expensive either like that for a gram was i think i think 45 or something and that's really like that's pretty cheap in my opinion for some rosin and it was it's still pretty good quality and like you can get some live resin or diamonds for about the same amount and this is solventless and i would honestly rather prefer this and it hits like so much so much better dude i can't even like i'm not over exaggerating or anything because like i'd be smoking every day and like and i'll be smoking some like like i think in the last video like it was like some purple punch like it'll give me high and it made me feel pretty good but it doesn't give me that like full body high like this is giving me right now and this is like it's just smacking for real but um yeah let's get into the next thing the which is this full spectrum hashtag so what basically what a full spectrum extract is is about preserving the natural ratios of compounds within cannabis while removing the impurities that can compromise the full experience and then full spectrum exercise can also be called whole plant extract just because it keeps all the properties of the flower it doesn't remove any of it but however because heat is involved in this uh solvent extraction method compared to like a regular um full spectrum extract with uh like bho or co2 however since this is a salt in this extract and the only extraction method is like through heat basically um it is a concern that full spectrum extracts going solventless um can lose some terpenes if it isn't done right and some other profiles of cannabis but if it's a well-known company not some like boof they know what they're doing for the most part and like have it down to its science really but yeah that's basically the difference between full spectrum and live ones originally coming from a dry flower and then the other one's coming from a flash flowers and flower which is live resin we just dabbed on with some cured aid frozen i accidentally got that mixed up with uh with this the full spectrum live rosin so everything i just talked about i was referring to this which is a full spectrum like i was saying like you get the full profiling of the flower and everything from full spectrum compared to my bras and you get my rods and you only get like parts of it because it does remove some um profiles we'll get into this full spectrum now i pretty much explained everything about the difference between full spectrum and uh live resin but yeah this [ __ ] looks fire as [ __ ] as well um i'll read the little things they have um this is from honey butter this is their brand uh the last brand was the wells like it's cool how like you can tell the difference about the brands that really care and i feel like these like saltiness extract brands because like there's so much more work goes into it i feel like um they really care about their products and stuff and you can tell like they have like a whole last thing it shows you like what temperatures you should be taking your dabs out of stuff and like if you have a puffco or i think the other thing is the aura or email it even says with temperatures to take it out and that's really [ __ ] helpful like no other thing does that and like it says you should be taking them at low temps and like low temps is definitely the way dude you can really taste the chirps at low temp and it's the move and then mainly it says thank you for choosing to support craft cannabis our family business small farmers of uh meniscento county from the soil to oil every step of our process performed with love and intent of delivering the best experience for you down to the last dab we are incredibly grateful to share our patient we are incredibly grateful to share our passion for this amazing plan with you thank you kindly the honey butter team and then message us anytime we're always here to chat honey butter rosin dot co so yeah this is that honey runner rosin this shit's fire bro i haven't even tried it i can't even say it's fire but it sounds fire it's peanut butter cup bro it's crazy i'm allergic to peanut butter so uh bro imagine this like gives me an allergic reaction i've never even like had peanut butter so i wonder it's gonna give me the taste but yeah this is that um this is that full spectrum rosin and like i was saying full spectrum rosin does come from dried plants not um fresh frozen plants so that's why it does look a bit different it also could be different press as well from the live resin but yeah it still looks fire as [ __ ] and smells it smells like it's gonna be super tasty um like dude the other the melanie smelled super good too but yeah we'll take a nice uh nice glob of this too i'm already [ __ ] hiding and i'm working like an hour so hopefully i can get this video finished [Music] check [Music] so that peanut butter cup um that was pretty fire too as well and that was the cheapest out of all of them there's only like 38 dollars before tax that's really not too bad for grandma rosin um but i can understand why it is the cheapest it definitely felt like the quality it wasn't as like refined as maybe the cured age rosin and i just felt like like i said i mean like in the name said this just feels more cured and like they like the feeling of that one more i feel but this definitely gave me a good uh full body high more in the head i felt like yeah but this for sure with some fire bro and again it's solving this and if you can get some cell phones the same price as um some bho or um other [ __ ] like that i don't understand why you wouldn't cop some solvent list because it's like honestly like better for you in the long run i felt like personally and yeah it's honestly it's just really some fire it's worth the extra couple bucks in my opinion and then right now we'll get into like the most expensive one which is the live rosin that i got bro that's just some fire i i can't even describe it that's definitely my favorite one i think it's just some um banana korean premium live rosin it's a banana oj um mixed with a creme brulee so it's a hybrid this is definitely the most expensive one at 85 before tax and which isn't even like too expensive for rosin because there's there's jams there that are like 110 120. i mean that might be a little taxed just because of the hype i feel like right now but like 80 80 to 90 is what you're usually going to pay for raws and like i feel like no matter what unless it's like i don't know for at least for some live rosin and like bro this just smacks differently so any products that are like immediately frozen like after they're harvested or called like live so that's why this is called live rosin it's referring to the flower that was immediately frozen once it was harvested and basically what live resin is is a solventless hash oil made with full milk bubble hash that was made with freshly frozen materials and as you guys saw in like my other video i was um smoking some full melt bubble hash it was no it wasn't full mount bubble hatch because i wasn't able to dab it if it says full melt you are able to dab it but if it's bubble hash um just regular bubble hatch you can't dab that it will like it just won't burn correct but you can put it on top of bullets or inside joints but yeah that's basically just like all trichomes and then you go from that to uh you basically take the water hash and that gets pressed and then that's what becomes a live rosin and that's basically what this is bro i'm gonna open this up now no time to waste bro just some fire um yeah it doesn't even have like a long description like the other boxes though all it says on the top is flavor as nature intended in a smaller batches let's fire around i mean this right here is some live rosin bro this is really what you want to be smoking if you can some fire bro um just take a bobber of this [Music] [Music] like i was saying that should just hit way different than like all of them honestly don't get me wrong it's not like the most insane high ever i just feel like it's like maybe the cleanest high that i've ever gotten from at least the the papa select this is the most expensive rosin that um that i picked up it was the live rosin and like it tastes great and everything but i feel like the chirps like aren't really there compared to the aged cured rosin and then the i think this was the full spectrum rosin this is definitely worth it and it gives you the i felt like it gave me the cleanest high it was 85 before tax which is pretty expensive for one gram if you want to know you're smoking like the best [ __ ] possible you can definitely be getting this um these two as well any type of solvent this extract is honestly gonna be so much better than any bho or a co2 extract which is like um like shatters and stuff you can for sure always still get good batters and i'm like still going to get that of course because they're always going to be cheap and everything but like that means solving this is just honestly like superior just like it's just a step above it's honestly really worth the money i don't understand the hype now but like out of all three of these i'd say that the cured age rosin was my favorite and it was only 45 dollars for the gram and like this should super tasty bro like you could really taste the chirps on it i'm gonna take another glove of this before i go and yeah that's gonna pretty much be the end of the video uh thanks guys for watching if you can try out some solving this extracts bro definitely try if you can uh definitely worth the money if you've ever been thinking about it like comment subscribe i'll see you guys next week", "4Fz5syoDXXw": "okay I'm happy introduce le Rahimi from Intel labs Berkeley hi I'm gonna talk about random kitchen sinks but before I get into it I want to just make sure everybody can pace themselves through the talk I'm gonna start with really lightweight stuff and then and then we're gonna ramp up slowly and then I'll do experiments and you can turn your minds off and then I'll hit you again with some math and then and then this is new work and it's not published so well try to breeze through it but it's still pretty Maffei I'm gonna start here so this is to give you a little bit of context about where all this work comes from there's a new trend in AI back in the day when we wanted to build smart things we would start with a really complicated statistical model like Bayes Nets where we're inference was np-hard and and learning was even NP harder and and we gave it some data and you know a few thousand examples and and we trained some intelligent thing and something happened in the late 90s where instead of models like this these really structured statistical models we started using more generic models nonparametric models like rbf's and in to compensate for the lack of statistical structure in these models we started feeding these models lots and lots of data domain knowledge started coming from here instead of from here the nice thing about these generic models is that the optimization problems tend to be convex at least they're in P but because we have so much data optimization problem in practice ends up taking a lot of time so this talk is really about tricks for making these types of optimization problems on these types of datasets go faster to support the new kind of artificial intelligence that that we're doing these days so I'll give you a few examples of this trend from here to here here's an example from Alyosha afros this is a problem where you want to given an image you say oh I really don't like these houses being here so you block them out and then you want to fill in just blotted out part with something pretty and relevant so alia these days takes a very data heavy approach you just crossed there millions of images through in Flickr and finds a little patches and substitutes the patches in here it's a very heavyweight heavy heavy data-driven approach contrast this with something he did ten years ago where he actually had a pretty sophisticated hmm based model not very data-driven but the model is a lot more complicated this works a lot better here's another here's another example this is work from Antonio de Alba here's each of these little cells is uh is a tiny image there's ten million images in here and he uses this data set to do object recognition yeah extremely data heavy the the operation that he goes through is just the nearest neighbor search and in these ten million images compare that with why did a while back where they actually build these this pretty complicated discriminative model that in that the took into account the spatial relationships between objects and wasn't trained on that much data this works really well here's another example from Greg Schaffner which here's a pose detector right here microsoft recently solved this problem than the xbox so you're the goal is to recover the 3d pose of a human body and Greg's approach here is he he takes a graphic simulator generates 150,000 examples of a fake person under random different poses and just matches this image and against this image using using a very simple distance metric contrast this with something the same group did ten years ago that involved actually reasoning about the 3d geometry of the shape in real time and trying to match it against the 2d image and we can recover the 3d shape of the body this works amazingly well in its fast my own motivation for this stuff is is building object recognition systems that you can train on that can recognize millions of objects in your real world so you know this is this is the system trained on about 30 objects and you know it runs in real time but as you scale the number of objects it goes more and more slowly and the accuracy drops so it would be neat to take systems like this and be able to scale them up just the same way those previous examples I showed you work part of the reason this trend is happening now is that we have access to a lot more data than we did before I think I'm speculating here we have really high fidelity simulators like the graphic simulator that Greg was using to generate these body poses we have the web that has lots of images and annotations on it and ever since the MacArthur Foundation started giving grants to people for building games there's a lot of and Mechanical Turk Stu there's a lot of hand annotated stuff that you get off of the off of the web so this talk is about supporting this trend and I'm going to show you two tricks that I've been playing with I'll start with the random features trick this is a way to speed up colonel machines so a little bit of background on colonel machines here's a classification task okay so this is a trick for for speeding up your classifier and I'm gonna tell you about the classification problem a little bit I'll tell you about the colonel machines and I'll tell you about how we speed them up so in this classification problem you got a space a bunch of points the points are labeled among the two classes and you're trying to find a decision surface between them linear decision surfaces don't always separate your two class as well so one would like to consider nonlinear decision surfaces and in kernel machine the decision surface the form of the decision surface that we use is is a weighted sum of kernels placed on your on your training examples okay so so there are n parameters here there's a kernel that we define you know maybe it's a Gaussian or something like that we place the kernel on each one of these points we come up with a good weighting and that that will describe a family of phenomena of curves in this space so this is turns out in me this is well-known a function of this form when this kernel is positive definite is equivalent to a linear function and in a feature i's space of the input and these features are such that they satisfy this relationship so the kernel effectively maps the features maps of your inputs into some feature space and then takes the inner product in those spaces okay so this is a review of of kernel machines and this is a really neat trick because where as you would normally be trying to fit a decision surface in an infinite dimensional space right so this this feature for example in general could be an infinite dimensional feature mapping whereas you would normally have to find an Omega in some infinite dimensional space this kernel trick lets you search for n parameters only so you can start implementing these things inside computers which is great and it even has this nice interpretation like I said in terms of instead of searching for curves like this you map your data into a potentially infinite dimensional space implicitly and then learn a linear decision surface in that space so this works really well the problem with these kernel machines is that they force you to deal with these enormous matrices if you have 10 million training examples one way or another you're gonna have to represent at 10 million by 10 million matrix whose entries consists of the kernel evaluated on pairs of your training data points I made this really big to take up the whole screen to emphasize how big these matrices can be so you can do infinite dimensional things in finite dimensional computers with the kernel trick but these things are still huge and so some researchers have come up with very popular tricks for dealing with matrices like this here's another trick the trick is well we talked about how these kernels actually compute inner products between feature eyes the inputs so what we're gonna do is instead of dealing with the kernel or with this infinite dimensional feature mapping we're gonna find a finite dimensional feature mapping in fact a low dimensional feature mapping such that instead of having to take this inner product in this infinite dimensional space you can just take the dot product of the feature eyes inputs okay so you know just like just like with with kernel machines your decision surface took this form with this machine because we're now using finite dimensional features your kernel machine just takes this form okay and to go back to this diagram the idea again is we're gonna to find these these nonlinear decision boundaries we're gonna map our data into some finite dimensional low dimensional space and then train a linear decision boundary there and this mapping is gonna be such that this relationship holds okay so instead of training your kernel machine with kernels and dealing with these enormous matrices we're actually going to randomly feature eyes your inputs and then train a linear classifier and this relatively low dimensional space and we're gonna guarantee that that the resulting classifier is gonna be close to the classifier we would have had we would have gone and I'm gonna tell you about two two different types of random features one of them are three random features and they're based on on the Fourier transform of the kernel and another one is a discrete random feature that's based on gritting up this space I'll go through both of them the proof for why this works is really simple it's four lines and I I think it's kind of neat to look at so I'm gonna just pop out pop up some mouths and I'm gonna walk walk through it because this works this is just really neat so the trick is we would like we were given a kernel I forgot to mention this only works with shift invariant kernel so so you have to be able to represent the kernel like this and at the bottom we're gonna get we're gonna derive these these random features such that this relationship holds we want the inner product between the random random feature eyes the inputs to almost be equal to the value of the kernel now I'll walk you through here okay so step one take the Fourier transform of u kernel okay so this is just the standard fourier transform that you learned about in elementary school step two so this is an integral we're gonna replace the integral so P is the Fourier transform right of the kernel we're gonna replace the integral by by approximating this this integral with with a with a with a with an average okay so treat this Fourier transform as a probability distribution draw samples from an yes no X's are still vectors yeah yes so this is so Omega prime is actually inner product between Omega and a vector X minus one you are awaited here or in the Sun no because I'm drawing from from from P of it so so what I didn't what what what I slipped under the the rug here is that because this kernel is positive-definite it's the free transfer zone here's a theorem the free transform over the positive definite kernel is positive definite yeah this is Buckner's theorem you don't learn that in elementary school for some reason but I I'm not sorry in in in systems and and signals and systems in Allen will skis book that has all these free identities this identity is not there unfortunately and it's a really powerful one so so the point is that we can treat this Fourier transform as a probability distribution it's positive you can sample from it so let's approximate this integral using using sample averages and now I'm just gonna rewrite this summation I'm gonna split up each of these terms into a product and then I'm gonna write this in vector form this vector depends only on X this vector depends only on Y and we have our random features okay so the Fourier random feature really what it's doing is saying if you want to compute K of X and y take X project that down into a random Direction W W is drawn from the Fourier transform the kernel so you take X you project it down onto a hyperplane and then you you compute a phasor from that okay so you just project it down and then you just wrap it around the complex circle and this complex number knot becomes your random feature and and and there's a squiggle mark here certainly this relationship holds an expectation so certainly this is true seems like something I'll give you better than others for this so the sapling scheme is given to you the sampling scheme is draw from the Fourier transform of the kernel you could draw non random samples yeah yeah so so certainly this says that a lot so so this says there exists random sampling such that these two guys are close to each other sorry a random sampling will probably produce something that's close to each other and that implies that there exists a deterministic sampling such that these two guys are close to each other the problem is I don't know how to come up with one I know how to come up with one by just sampling but I don't know how to how to construct one okay okay so so the point of this was to show you that at least an expectation feature izing your x and y and computing their inner product gives you something gives you gives you the kernel value okay I've also shown you how to compute the Z it's just draw a bunch of samples from the Fourier transform of the kernel and compute compute these phasers what I really want to want though is not these results and expectations we want to show that this actually holds throughout this space so let me let me go through that right now let me let me tell you what we what we know how to do so we know that that the inner product for a given X and y is gonna be close to K of K of X and Y and expectation well we can also show that the tails of this are quite liked okay so this is just by hosting so these guys for given x and y aren't gonna deviate very much with very high probability using the Union bound on this you can you can show the same thing on a discrete data set of endpoints but even more so using a covering number argument you can show that this holds throughout the whole space okay so so if you draw enough if the dimensionality of your random feature is high enough then then the this inner product will approximate this kernel for all the points in your space with very high probability okay so it's not just the result in expectation this is actually a result that holds with very high probability throughout the whole space in fact let me reinterpret that theorem for you so it says that with probability at least one minus P P some probability that you given the inner product of the whole space approximates the kernel over the whole space with with probability at least one minus P as long as you have enough the dimension of your random features high enough as long as you sample from the Fourier transform enough times okay so this depends on linearly on the dimension of the space this is a standard epsilon squared dependence on the error over there and the dependence on there's a dependence on the curvature of the kernel as well as you might expect to see in fact we'll see I'll show you experiments in the second part of the talk that I'll compare the the cost of this D versus the cost of choosing these features optimally yeah yeah why don't I what am I just when we get to the experiments oh so here's another random feature so that was the furry random features as a totally different class of random features that that one can also construct so you give me a kernel and and my job again to remind you is to build a function possibly a randomized one such that the inner product between the feature eyes inputs is equal to the kernel okay so so this random feature works like this grid up your input space your space of X's so just lay down a random grid I'll tell you how to pick this the pitch of the grid in each bin of the grid a sign a sign a sign a binary spit strength binary bit string is just the representation of the number of the grid and unary okay so a grid one gets a one over there good to go to UK and then the random feature representation of a point is just its grid ID written in unary okay that way when you compute the inner product you're basically just 1 if you're in the same bin or 0 if you're not in the same day and now all that's left is for me to tell you how to compute the these random grid grid pitches and in the same way that we picked the omegas from the Fourier transform of the kernel here we define a hat transform of the kernel instead of sinusoids it's in terms of these these hat basis functions ok so so you randomly sample your your grid pitches from from the Hat transform and the kernel that you're trying to approximate and again you get the same same theorems and same results as with the free a 48 kernel so let me let me show you how this looks like in code it's very simple right if you want to train a data set with a with an l2 lost you want to train a classifier using an l2 loss with free random features you generate a random a bunch of random w's so these are the you just sample from the Fourier transform of say the Gaussian kernel Fourier transform of the Gaussian kernel is again a Gaussian so you just draw a bunch of Gaussian w's and then you pass your data through to the random feature so this is the complex sinusoid and then and then you just now fit a linear linear solver you just fit fit a hyperplane to in this feature I space and that's just the least squares problem isn't backslash over here ok and boom you have your you have your hyperplane that's that that's training in three lines of MATLAB code and then for testing you you map your input vector through the through the random map and you you evaluate the inner product with respect to the Alpha that you just liked so let's get to the issue of dimensions that you brought up so here's a bunch of datasets that we run this the Sun so typical dimensions anywhere from 21 dimensions to 127 dimensions on these datasets I'll show you higher dimensional datasets data set sizes range from a few thousand to a few million and generally training is really fast with these random Fourier features and here are the typical dimensions that pick these are much smaller than what the theorem would predict so the theorem is quite loose the theorem would predict well so depends what epsilon you want but there's a 1 over epsilon squared so if you wanted accuracy of 1% it'd be 0.01 squared right that's 1 over point 1 squared so it's 10,000 okay so so we're getting into so the theorem is loose because of this guy right here okay so in practice you know we get typically better than state of the art of performance on on various heavily tuned SVM libraries the free random features work on most data sets on some of them like the forest data set it didn't work so well and so so this data set has this characteristic that if you actually were to train an SVM with RBF kernels on it most of the points become support vectors I'm sorry that's Aaron yes sorry there's two different flavors there's the two class version and there's a seven class version respect this is the to class version given the rate yeah I am I took the the version of the horse cover from these people here from the core vector machine I just grabbed it from there and ran everything else on it anyway so so the point of this was the point of this line is to tell you that that there are that that there are the two free if we the two random features are complimentary in some sense these are really good for learning really smooth decision surfaces these are really good for doing nearest neighbors types of search surfaces yeah so that 500 see if it works and then if it works really well then you set it to 100 if it doesn't work really well you said it 2,000 and maybe play the combining I have I have yes so you just you can just stack up the random features for the two guys and things work quite well yeah you tend to that I mean so in the sense that but if you run it on all of these guys you get basically the same performance well so there's an approximation going on the approximation was in terms of the kernel and not in terms of the decision surface or not not even in terms of the ideal decision surface we really are learning a different service it just tells you that that the RBF representation isn't the best representation and it could be the hinge Allah you're also DZ regularized least square classification right yes um so you you get basically this I've run all these stuff with all these things with a hinge losses one basically get the same results and it stops it stops to matter what loss you use when you have very large data sets okay so uh let me let me just tell you a few of the properties of these things so as you would expect this is on the force data set if you the bigger the data set right so big data sets help but you knew that and also as you would expect as you increase the dimension of the random feature your error also drops so this is error dropping quite fast in training and testing time not increasing very fast so so in practice these things tend to tend to have desirable properties let me um okay so this is this is random features this was about training kernel machines faster let me generalize the problem this is this is where the random kitchen sinks come from so we were learning these feature mappings based on a kernel that you give me but why have Y start with a kernel in the first place so back in the day this is this is a picture out of a out of a paper by block from 1962 we had these neural networks and and there was this idea just from day one but there should be some randomization that happens at the first life okay so so this idea of of having some randomization in in your training algorithm is classical no we don't draw our neural networks like this anymore here's here's maybe a more modern way of doing things here's your input it goes through some nonlinearities then on the areas have some parameters and then you wait the output of the nonlinearities and what you're what you're learning during training is these weights and the parameters of the nonlinearities and actually this is also out what did we just write this now our neural network is is a weighted sum of nonlinearities and their parameters and we just learned the weights and the parameters so let me focus on one popular way of doing it of training these parameters so so when you do a to boosting you build dysfunction stage wise you train the alphas you train the omegas and then you do that for the next step in so you have T of these stages in random features we were also training a decision surface of a similar form our omegas were random and we were just training for the office in Colonel machines we're also doing something similar except instead of a finite sum it's just an integral and we're learning this function alpha okay so so a lot of these so basically the world of machine of shallow network machine learning just all is focused on learning decision boundaries of this form and I'm gonna focus on one particular way of training these and that's the greedy method which goes back to well it goes back about 50 years so the idea and I'm gonna focus on a function fitting framework okay forget the data set for now somebody gives you a function f star and says please approximate it with a function of this form you get to choose the Omegas in the alphas but I want the resulting sum to be to be close to this function in some and some norm in a function space so you're given a function a target function to approximate and you asked to come up with a bunch of weights and a bunch of parameters such that they're such that the way that sum is is close to the to the target function so the greedy approach you know which looks like a DES boost looks like matching pursuit looks like a lot of these other things goes like this start with the function zero and then we're gonna find one term we're gonna add one term to the function and that's going to be the term that gets the one term addition that gets us closest to F star and now we have a new function and then you you iterate again for the next for the next addition for the next term in the function you again come up with the one that that minimizes the difference between the residual and the target function and you do this two times okay so this this this has the flavor of a Duluth spring and we know a lot about the performance of a function fitting algorithms like this in fact this result goes back 20 years 25 years 15 years 15 years so suppose you want to approximate a function of this form this is our target function it consists of an of infinitely many terms and we want to approximate it with a t term function that we built as in the previous slide so what's known is that the distance between the approximation that we built as in the previous slide and the target function decays as 1 over square root of the number of terms and there is a constant here that measures in some sense the norm of the target function so so that the l1 norm of the Alpha is is is the norm on on functions at this point ok all right so this is proved by by induction over the over T let me let me write this down graphically for you it says it says that if you for any function in this in this space with an infinite expansion there exists a function with a t term expansion if you're a lot to pick alpha and omega that's not too far from that function within 1 over square root of t so so it's a statement about for all functions in the blue there exists a function in the purple that's not too far ok that's that's about as good of a rate as you can get this this this rate is tight all right so so here's what here's another idea this is the random kitchen sinks idea for for fitting functions you're again given a target function and you're again asked to come up with alphas and omegas such that this this T term approximation is close to f star but now we do something much simpler just pick the Omegas from some distribution just randomly instead of instead of going through this greedy algorithm there's nothing greedy about this now just pick them randomly and then to pick the alphas just just solve this this convex problem okay so this looks like a least squares problem for example all the ones it's a batch optimization over T alpha okay so now and then just return the alphas and the omegas in computing so so let's see how well this does and you would expect that that the performance guarantees would somehow depend on the distribution that you used to sample the omegas right and so and so here's here's a result if you remember the the reason the theoretical result for for the greedy algorithm dependent dependent on the l1 norm of of the coefficients of the function we were trying to approximate let's see over here okay so let's define an analogous norm for the target function we're trying to approximate we're gonna call it the P norm and it's going to depend on the probability solution they use the sample your your your parameters okay so you could think of this as a as an important sampling ratio between the alphas and the distribution that you're using to sample these guys okay so so the theorem says if somebody gives me an F start to approximate using the the the the algorithm I just showed you on the previous slide then then the T term approximation with probability at least one minus Delta for for any Delta that you pick also drops as 1 over square root of T so we have the same rate in terms of the number of of terms that we need in the expansion as we do with the greedy algorithm but here we just managed to sample the parameters randomly and then there's this dependence on the on the importance ratio between the office yeah so here's so here's here's here's what this theorem says okay so you fix I have to star and then and then we're saying so you fix F star and F star is drawn from this big set that looks like that it's it's infinite expansions of of your weighted features and we say that if you consider this random set so this is a random set consisting of all alphas all weights but then these features are drawn randomly it says that with very high probability the difference between this fixed F star and this set is drops as 1 over square root of T okay so whereas before we were making a claim about for all points in the set there is a point close to here in this case we're just saying for a fixed point in this set and that's all you need to talk about to talk about function fitting after all somebody gives you a function to fit and then you draw stuff you don't need you don't need to approximate the whole space ever you just need to approximate the function at hand that you need to approximate and that's why we managed to get this 1 over square root of T this constant could be substantial yeah you could pick a sampling distribution for the week learners for the for the features that's terrible for the given function yeah it's easy to construct it's just if you use a direct Delta for example for your omegas you just learn a really crappy classifier but at least the theorem is correct and that that crappiness is reflected in this and olives right now your so you just pick already once me if you keep out some fraction and replace them some things we can do a little bit more yeah what works really well is if you start with this random thing and then just do a few iterations of gradient descent on the Omegas in the office yeah so that works incredibly well of course it's all in neural network and just just can't say that out loud yeah so okay so what what what what is neural what is neat about it is that it's a neural network that you initialize with random weights and you have guarantees about how far you end up from from the thing you're trying to approximate well so it depends where you start with the training from scratch I'm not very good at it even though I've tried very hard I often get stuck in local minima generally you end up doing quite well if you if you do this and the ingredient descent in the experimental report I don't do the refinement gradient descent refinement I've informally just tried training starting from zero or various parameters that I thought might be good and it works ok but nowhere near as well as this I mean it's much slower because because you need to run greetings sent for a lot longer to the random sampling rate because I've tried stuff like we're the fees from a PCA of the dataset and that didn't work well yeah yeah yeah so this is random samplings completely independent of the data and that that's right that's right that's right so it is a design issue so the reason this stuff ends up working well in my intuition about all of this stuff and what these theorems mean is that is that really it doesn't matter what the nonlinearities are but just put a lot of effort into figuring out what their weights should be that's where the magic is not in here necessarily that's that's that's how I view this this this the this result yeah just even here it's not hurting you because of the way we pick the weak learners right right because they're random right so you're talking about the Dale Sherman's result of of yeah yeah so yeah yeah you can't you keep right so if you pick your you're double your own so Dale Sherman's result is if you pick the Omegas from from boosting you don't want to go back and refine your alphas only you got to go back in there okay all right so this theorem is in terms of some norm define and I mean we can come up with a much stronger form of this theorem in terms of the LM fitting norm of between the functions but but these but these features have to be sigmoids like this okay again this is this is if you're if you're gonna nitpick about about my choice of a function norm here this gives you a result in terms of the element failure so you buy that it's enough to to just fit a fixed function in the space that you don't need any universal claim about the whole space did I did I convinced you that this is a good enough thing I'm gonna skip the proof the proof idea it basically boils down to coming up with tale bounds for for a zero-mean random variable in a Banach space it's a bounded Z or mean random variable in a Banach space just replace this in with this random variable and and then you apply standard results from there um so everything I told you about so far about the random kitchen sinks was about fitting functions but really we're going to be fitting data and using a standard decomposition of the empirical risk sorry of the defect between the empirical risk and the true risk you can show this bound so if if F hat is the T term expansion that you derived by looking at n data points and minimizing the empirical risk then the true risk of F hat compared to the true risk of the of the best risk minimizer decay is as follows okay 1 over square root of t plus 1 over square root of the number of data points that you looked at so so the 1 over square root of T comes from the previous theorem this 1 over square root of n is a standard result from from learning theory this is no it's not it's not a uniform convergence result this is this is a convert result about this optimizer okay it uses the uniform convergence for this part of the decomposition this part as that was argued as I was talking with over about is only needs only needs a point wise ok so let's go over some some experiments here's the adult data set it's a relatively small data set but everybody uses it this is the number of terms that we add in the expansion this is a de boosts testing err so after adding a few terms about 40-50 terms is enough for a to boost it plateaus out to about 15% air for us we need to draw a lot more random features to get the similar error rates okay so so ADA boots got there faster with many many it was got there with many fewer terms we got to use a lot more terms to get to the same accuracy but we got there much faster our optimization problem is much much faster it a boost does this pretty heavyweight iterative thing where it has to touch more or less the entire data set at every iteration we just touched this at once in our in our least squares solution so this is the runtime as the number of feature increases it abuse takes a lot of time this is on a log scale we take very little time and in fact let me combine these two graphs together did this graph this is the amount of training time versus the amount of error that you get okay so even though we ended up using a lot more a lot more terms in our expansion we're still much much faster because our optimization procedure is much faster for a given area here's another data set this is data coming from an accelerometer from hip worn thing that detects your physical activity stop they deduced after about a hundred iterations because it just was taking too long too long whereas this thing just the random kitchen sink kept kept on kept on ticking and again you have a couple of orders of magnitude less time that you spent for the same error rate another standardized they just that similar thing again a few orders of magnitude for four similar areas and that's consistent across the board here's uh here's a face detection experiment we compared Ada boost with haar wavelets against the fourier random features what's neat about this comparison is is that you can't train for your random features with a tableau story well it's a hard weak learner to fit it's hard to fix sinusoids to to data but we're picking them randomly in our case so that's easy so so part of the benefit of this random kitchen sink trick is that you can start using features that you wouldn't be able to use with a DES boost because you no longer need to fit them to data that's convenient so we get slightly better performance than ADA boost on on on our test accuracy training is much faster seconds instead of minutes but we do use about a factor of 10 more and in weak learners and again here's your point John about about how at run time D is what's important in these types of experiments we were hoping to have a fast trainer and there are lots of tricks that we started using in a face detector that we built that can that can avoid you having to slide the window but the the detection window over the whole image there's a there's an optimization that happens where you can prune a lot of the search space for for the sliding window the space so that's how we get around that kind of slowdown yeah Sparsit of us and then get rid of those random guys you've never used yeah yeah so so that's that sort of experiment I don't report on them here but one idea was instead of least-squares just use Pegasus and hope to get sparseness out of that I can't find a good setting of the parameters of Pegasus to get as good accuracy as I get with with least squares so yeah but then those problems become huge and I don't have a I I would like to ask you for it for a really large scale l1 regularize solver I think there's a couple out there I just don't haven't talked to anybody who has just recommend one do you similar things with em nest and this one we were comparing against boosting by filtering which is a much faster version of boosting where you instead of touching the entire data set at every iteration you just touched a random subset of it and again you see similar types of results were about a hundred times faster similar accuracy but you use more features I'm going to skip this so this is I can't really talk about this part but I think it's neat that Intel may consider using this in something one day right so here's here's the lesson for from this part of the talk so here's typically people the way people fit these nonlinear decision boundaries you'd run a minimization over the weights you run a minimization over the parameters and I just here's a caricature and there's not mathematically the caricature of what we've done is is minimize over these weights but randomize over the they'll make us and prove that you get very similar results so for the next few minutes I want to talk about less baked things that I've been working on for the past six months or so unless our questions about about that stuff where we can so here's here's here's a neat trend everybody is doing semi-definite programs for everything and getting good results as long as they have 10,000 variables so it'd be neat to come up with a way to solve semi-definite programs faster so these semi-definite programs typically take this form you want to minimize a convex function over matrices subject to a polyhedron strength so there's a linear constraint on the matrix and you want the matrix to be positive definite okay so so this blue thing is is represents the cone of positive definite matrices and the problem is while it's it's polynomial the only polynomial hard to perform minimizations like this it's still it's still it's still hard to do it on computers today we don't have very fast solver so so it's a challenge to come up with with good solvers that that that can minimize things or this this convex cone so a trick that diamo Basinski pointed out to me that they'd used in a paper is to replace this set in the optimization with the polyhedral set these are random polyhedral set that's the green thing over here okay so you just generate a bunch of random vertices that are positive definite matrices and you require X to live in that cone and it worked amazingly well for them for their application and and and they didn't know why it worked well and I ran a bunch of experiments and and it looks like you know seems to work well for as long as your optimum is not on the wall of this cone as long as it's not an extreme point of this cone it'll work amazingly well and if it is on the extreme point then you can still get within you know some epsilon of it with high probability so what can we say about this type of this type of thing so here's the theorem actually before the theorem let me tell you every time how one uses this trick so we just replaced the positive definiteness constraint with this constraint this is the this is the constraint that X has to lie in this polyhedral cone whose vertices are these randomly drawn V eyes and and just to say that it's a cone means that these weights have to be positive okay so so now if F is if F is linear for example this turns into a linear program if F is quadratic this is a quadratic program you can solve all of these things really fast and this graph is a simulation that shows that actually a lot of these matrices that you draw from this positive definite cone do end up being extremely close to this random convex polyhedral car so the theorem is and it's still in flux I think some of it can be improved is that if you if you're given a target matrix X naught so for fixed X naught draw a bunch of random positive definite matrices from the Wishart distribution construct this cone okay so this is just positive combinations as these random Wishart matrices then with really high probability that target matrix is close to this comics polyhedral cut as long as the number of random points that you drew is large enough and large enough of course depends on how accurately you want to approximate the target matrix and it also depends on on this guy which which quantifies how close the target the target matrix is to the boundary of the of the comics come okay so with this you you just you now have a tool to convert hairy semi-definite programs into into optimization problems over random polyhedral comes like like just turning a semi definite program into a random linear program so that's that's one thread of research here's a here's another thread of research that I don't know if it's gonna pay off but but it's all sci-fi and it feels good to work on it so it turns out if you take a normal CPU and you drop its voltage below the voltage that the instruction manual tells you to to run it out the CPU will still run but it'll make mistakes so and you save a lot of power power consumption drops somewhere between V squared and V cubed right but you get to drop the clock oh because there's this dependence on the clock sure sure see yeah so so wouldn't it be neat if the next processor is that you actually I'm totally not allowed to say it that way wouldn't it be me if in the laboratory somebody were allowed to somebody were to build a processor whose floating-point unit ran at a lower voltage saved a lot of power but made some mistakes here and there or made a lot of mistakes so actually that's that's explored this is this is not an entirely new idea people have been building have been prototyping these these circuits where were they they're designed normally and when you drop them at low voltage they have this this little shadow latch that detects whether the circuit is is misbehaving and if and if the circuit detects that it's misbehaving then it'll flush an instruction pipeline and reissue the instruction anew and raise the voltage a little bit higher okay so so this is this is the hardware approach at at resilience on an under halted processors yes yeah so there are various ways to notify the the software layer that that an error has occurred there are ways to mask it at the hardware layer by just reissuing the instructions and not letting the software worry about it but but here's here's another idea let's just get rid of the overhead of this shadow latch that's that's taking up power it's taking up dye area and it may even force you to run stuff at high voltage just to get the shadow latch to work correctly and let's expose all the errors to the software the floating-point unit will will not just return Nan's when it's made a mistake it will just return the garbage that it computed it'll just say a plus B is equal to something totally random but now let's design our algorithm so that they can tolerate that type of error so here's here's the idea so let's start with a classical combinatorial problem say bipartite graph matching okay so this is standard problem let's express it as a as a linear program and then convert the linear program into an optimization problem that's unconstrained and then so none of this involves computation this is all pen pen and paper transformations and then to solve a bipartite graph matching problem let's just toss this unconstrained optimization problem into a stochastic gradient solver the reason is the stochastic gradient we know can tolerate noise and the gradients so whenever we compute the gradient which is where you spend the bulk of the computation when you're doing this type of minimization drop the voltage feel free to compute a really noisy gradient and then do the update at normal voltage yeah so it depends it depends what regime yeren if you're in a regime slightly below design voltage the errors that you get are our timing errors and they're random only in the sense that that they depend they're hard to model in that they're the result that you get out of the FPU depends on the previous result of the FPU return but if you're far below that then you actually get transistor noise which which is more naturally modeled as a stochastic source and here's here's some preliminary results this is quite preliminary so we we built these this this hardware simulator that actually has a spark processor on an FPGA and then the FPGA injects noise into the output of the floating-point unit so here's a least-squares solver this is just the least-squares solver from OpenCV if you drop the voltage of the CPU and inject all these errors the leaf square solver starts returns returns really noisy results so this is this is the the the difference between the output of leading square software running at low voltage and the optimum okay and you just get these very large residuals but using our sagacity gradient solver no matter what the error rate you just nailed it result eventually similar thing with bipartite graph matching if you use open CDs earthmovers distance solver at at low voltages it'll do quite poorly whereas ours does well it doesn't get a hundred percent yet because there's a bug but basically it's performance doesn't depend on the amount of error that you inject so we really just cut we are taking longer to compute to compute these results because the casted gradient is obviously slower than then say the the simplex method or or the SVD in this case but at least we're getting robustness right now yes it does happen and we can recover so what I can what what I do know is that from here to here corresponds to about power savings of so this uses about two percent of the power savings two percent of the power and this uses about a hundred percent of the power okay so it's a fact over 100 in power savings the amount of time that you spend is just ridiculous this is just not a worthwhile technique right now okay so you end up waiting you end up using up a lot more power right now just because you keep running this this stochastic gradient solver for two you end up using more energy because you're waiting for your competition but the trick is so this is a motivation for us to develop faster stochastic solvers right instead of just following the gradient let's let's do let's let's try conjugate gradient methods or or second-order methods or anything other than the gradient direction probably will help so an alternative is to just compute have your FPU be be narrower and then Stitch the output together later I wanted to before before I get on my pontification slide I wanted to acknowledge some some collaborators from from various universities and a lot of people who I've talked to about this stuff for the past couple of years we've been very helpful so I part of the flavor of this talk was about randomizing things instead of optimizing things and just about generally doing less work and hoping that your random number generator will will just get you the right answer and turns out we can prove that it often does and I don't I like digging back into the back of literate literature and finding the root of some of these ideas like you saw with the neural network picture and I was trying to figure out why more people don't randomized things and my literature search there took me to the original source and this support for both ways of doing things of optimizing really hard or just throwing caution to the wind and I can open for questions a few if you'd like I probably won't be able to answer listen don't ask yes when before when if you were youth voters by 30% I can see reduce power by a factor of two but a factor of 20 either black magic well so certainly if you if you drop voltage by a factor of two you're you're dropping power by a factor of four so but you also get to run your clock more slowly under this scheme I'm sorry I yes yes well so I I failed to actually manifest any black magic here because I admitted to you that that the D stochastic gradient solvers actually ended up taking a long time right so so don't don't don't be too impressed by these results and don't don't think I'm some voodoo master this is just a first step toward toward getting to Tortoise an algorithmic way to tolerate noise and numerical algorithms the the rest of it this idea of making of becoming resilient to under volting that's that's standard in classical people have been solving this at the circuit level for for a decade the the innovation is to do this at the software level using tricks from from the machine learning community yes bridges randomization experiments you mentioned the busine thing didn't actually look at the whole dataset just look at a random subset of wondering why that didn't help more and other related to improve things about randomize reasons yeah so the actually Joseph Bradley is the one who came up has all the results on on randomized boosting schemes boosting back filtering is his work he has bounced on on how well it does and there again you get the 1 over square root of T type of thing and my sense is that you just when you'd when you're gonna train a weak learner you just need to look at the data if you're gonna pick the weights using the greedy it was nothing that's that right so that's all right so that's that's their trick right so so if you will they are picking their their weak learner randomly just like just like I do except they pick it by looking at a random sub their randomization is by looking at random subsets of the data the way they add these weak learners to the final function is is by the stage wise thing and I'm still insisting that that the stage wise thing is what kills you the stage wise things I mean you're fitting on the entire news the stage wise thing is that when you pick when you pick the weight the optimum weight for the weak learner that you just learned you're you're again looking at a subset of the dataset you're always just looking at a subset of data set in the stage wise thing in this work you mean yes so there's no testing and training here this is your thank you yeah here no this is just a randomly generated lease it's there's no there's no don't think of it as a machine learning problem it's not the stochastic gradient sama Xin learning tool but there's no data fitting going on each year Microsoft Research hosts hundreds of influential speakers from around the world including leading scientists renowned experts in technology book authors and leading academics and makes videos of these lectures freely available you", "g5N8C2egsE0": "that was really suck okay I'm just so much quicker haha so sorry well be auditioning it wasn't like I went to a family reunion one time and they thought I was my brother's girlfriend Oh eyes and so oh my god oh my god yep so I was washing dishes with Austin and I see mama Phoenix start going no no no no no like what's wrong she's like I didn't put the dry ingredients in with the wet ingredients and now it's in the oven already it's already baking and so we had to pull it all out pour it all into a bowl but I'll dry ingredients in and it's gonna taste delicious and then was the oh ah state allen you took the salad bowl was gonna wash it for the South Boll had the dressing in for the salad so he pours all the salad dressing into the sink oh my gosh my penis goes no we didn't have any more lemons so Johan's applying lemons elliott and the third one was oh so Michelle was like I'm gonna come help and solve everything Watson it starts cutting garlic into the desert then already been messed up oh it was just the garlic skin loser like skin so she had to fish them all out Mary culture like there's so much like self-sacrifice and martyrdom like floating around that you just get in the mindset of what God's never gonna do jack crap for me like this is what I okay and the worst thing about being an MK is that I'm extremely interesting and some ladies just say wow what a weirdo", "GY953v5SBUk": "what's up YouTube it's bacon babe wanted to bring you guys an updated shorter and correct version of how to wire the do-it-yourself rosin press is the PID controller that controls the heaters this is green I think it's green genes to do it yourself on YouTube I'll link it in the description but with that said and his awesome tutorial this is the one part that he skipped so I'll give you guys a quick walkthrough basically the L mean down here is for live in neutral and we're going to act as if these are the cords coming in from the wall so if you see this side right here with the thermocouple which is the silver line this is the line that's coming from the wall this is the line down there that has the plug on it though so that's my live and my neutral coming in the black and the white okay and I'm using the black as the neutral in this case or I'm sorry the black is my live in this case so it will come in and it will go to a three-piece way go which will break off and go to number 10 on the PID and then it will go to this bottom portion on the relay caddy corner of the negative terminal so if I come over here I find my black cable coming in it goes to a three piece way go it goes to this end caddy corner the negative end on the relay and then the other one goes over here to number 10 on the PID okay so that's the live so now the neutral which will be my white cable I'll show you that one coming in and it's going to go to a three piece way go and then go over to number nine on the P of PID and then over to the heaters so if I come over here find my white cable coming in goes to a three-piece way go got three-piece juego breaks off and comes over here to number nine on the PIV this red one here as well as going back out to the other white one in the middle which goes to my heaters so this one white one comes in through teeth juego nine on the tid and the white one in the middle is the one that goes back to the other heaters out this side right here okay so this one is the white one that went out to those other heaters so then you'll have your two heaters daisy chained together on the back ear plate right and then those will connect to any cool thumbs up right here so this is that white one that's coming out that connected to this way go over here so it's going to go to your two heaters now daisy chain together and then the other end of the heaters it's going to come back on this end right here the black end right and that's this right here okay this black one so this black one coming back in from this is the heater side now right this is that cable right there I'm sorry a little tangled this is that one now this is about black cable okay again that black cable now from the years right here and that's just going to go straight over to the relay directly below the negative terminal so you can see this three for the two piece way go right the black one comes in goes through a two piece way go and then follows it over to the terminal beneath the negative and that's that okay and then you just have the regular number three on the relay which is right here the positive end goes to number eight on the PID and then you have a negative end of the relay which goes over so number six on the PID and then you just have the thermocouples which is right there okay and that's three and four three on top four right here and then the two ground wires which is the two greens coming from each of these so let's just go into a double way go together and that's it that is the entire wiring diagram you guys have any questions please feel free to shoot me an email so sat down in the description and then just to show you guys so I have all connected here that it works and that is actually the correct wiring take you over here and click it in and bam that's it it's that simple now just needs to be configured and the thermocouple installed into the plates as well as the heaters on the heater ends and then we'll have some heaters going this is the same exact circuit that you guys will use to create your own email if you want and instead of using cartridge heaters you use coil gears and sixteen millimeters the coil size that you would use for your standard quartz banger this little pro tip there so that's it rosin press is all wired up working heaters will be here Tuesday it's time to get present all right peace out", "HHTl6SxkHuw": "so today and we'll be talking about rosin something that I enjoy because I like concentrates but um like healthy concentrates and Rosen is pretty much as healthy as it gets so just gonna talk about some things that I like to do for instance I like to get a little pre pressed bucks just put it into a pollen press and make little pre-press books you don't have to worry about it if you're doing key for shake or hash or anything like that I'd recommend like getting the little mesh bags that way don't get sick messed-up stuff they're still gonna be a little bit of stuff in here because I'm not gonna do that but these are all bugs that are actually just pressed just outdoors so don't expect the greatest return you get what you get you know it's kind of like you get what you pay for in a way where you grow it's not gonna be as good as some thing that you might press in the club or some high-grade stuff it's just some outdoor still pretty good outdoor I mean it's totally pretty sticky yeah you don't ever want to press anything that is dry so I recommend getting like these uh 62% packs the community packs and just go ahead and put it in there keep everything fresh anyway good practice so yeah and then I just use the Reynolds parchment paper and I'll show you some results so yeah when you're all done you'll be left at this which you can still use to make butter and whatnot and I press everything twice it was notice that you still get really good product and I like to press around 230 degrees but yeah it's about uh almost seven grams of product that I squished and I with this outdoor I'll probably end up with maybe a gram try a little bit under always kind of berries but using outdoor don't expect a huge return but yeah I hope guys like it like it share your comment subscribe let me know what you'd like to see anything you guys need help with so you think I could do differently to help me let me know I'm always open to it thanks guys girls", "PwwxFh01X-U": "what up YouTube we are gonna be doing that rosin press video today this must set up some what we're looking like when we charge all my product in there what we have right here is about 2 grams of some outdoor pineapple Thai what I've been doing lately because I've been using these right here instead of parchment paper I've been getting a little better return and also [ __ ] hell of a lot easier to to collect it off of these the natural parchment paper so this is just what I've been doing baby I'll get it over it up like this talk about that you can see I'm trying to do all this show one hand cover it up like this and I'll set it on the heat pad for a minute close them like uh that these get a little warm for about 10 seconds but I've been doing then just give it a press let it do its thing for a little bit and get my get my product in return about in I don't get none of that crazy using [ __ ] out of the sides I don't press that much so I'm not gonna give you guys like a little run like that the most I do in this little setup right here this is a half ton it's a thousand pounds a thousand psi is right here so pretty much half ton so long only do about two grams sometimes I'll do four if I'm being lazy I'll throw two sets of pucks like how you just seen in that one I'll throw two sets in there and smash it if I'm being lazy and but other than that I just do about two grams out of time and seem to be getting a fairly good return well I guess the puck felt already nice but that's what we got nice is a fairly good return I just liked it it was immoral over there but that's decently I'll give you guys a scrape and then show you what we're looking like I'll be back what up we're back I'll show you the next step that I do um I'll give one of these ice packs out the freezer and I'll just set this thing on here for about two three minutes let it get cold just cold enough so that that'll oil is nice and hard and then we'll move it to the table and we'll get the screen I'll see you in a couple minutes we're back it's been a few minutes since this thing has been sitting on the ice so uh let's peel it off and see what we get you know just a little bit stuck to that so we'll collect a little bit first and we'll move on to the better yield and get a better better - look at this and then when we're done we'll get a I give you guys actual close-up on the clarity and all that other good [ __ ] stuff that you know matters more but yeah I found this way right here to be the so far that for me the best way easier for some reason this particular strain no matter what temperature I ran it at when I ran it imparts me it was just [ __ ] soul oily that didn't want to collect and we'll just drive me [ __ ] crazy just having to go back and forth back and forth to try to get something that it's just always doing this smearing so I try to figure out easier and better method and figured might as well give these ago he compares on them so I don't know I don't see why you can't smash with him we're gonna do this bigger one now I'm getting nice little collections he will be got going in there get all this little [ __ ] on the outside first just so that I know I don't gotta keep [ __ ] witty I just like to click my [ __ ] so I can get at the dabit I got more in the backroom of this actual strange who that for some reason so I'm [ __ ] reason that [ __ ] turns it turned green when it's in parchment when it some parchment paper that's another thing this oil right here will turn green it wouldn't get this clear wouldn't you nothing it was just straight green so it's another reason why I'm trying to try not new things with my prayers you know I just dyed it so I'm learning regardless so all I gotta say it's a lot better than my first few runs I was [ __ ] up when I was first learning wasting product smashing that to higher temperatures I'm getting it out in it's getting there but uh let me get this all finished raped up and then I'll get back to you guys when it's all collected and I'll show you the finished result all right we're back do you with the final product see if I can get it to come up real quick another reason why I like these I could get this whole little hold it all opinion nothing too big but you can see ladies they're gonna use any filters when I do never ends on time I use filters and when I'm smashing [ __ ] I'm smashing hash only time I use a filter but what we're looking like what I'm dabbing on pineapple tiny outdoor stood that about maybe like an ounce live for this I'm trying to just dab dab slowly what what we're looking like I just wanted to show you guys a little clarity video thanks for watching rate comment subscribe and get your little close-up view of this thing well clearly you look good before you get out of here mmm thanks for watching", "3KR4x4EUZAU": "and badges and showers early [Music]", "9ZbZxhnYIjg": "this is captain dink we're in the Spanish lab again today I'm going to be showing you how to load bottle textile rosin bag extraction what we're going to use today is the nut Smasher 3 point 5 gram 160 micron bags and again we're going to be loading it bottle textile versus slat style like that and bottle textile first of all we have to invert the corners and as you can see here adjust it a little bit with your fingers but that's what you're looking at the corners are going to be overlapping to each other so as the extraction happens it's going to fold in on itself so let's load up three - grams of this beautiful fortune cookies here fresh flour and as you can see there there's 3.5 grams as beautiful once again this was donated to us by the folks up at Bullis farms in that a city California so a bottle textile again what we're gonna do start dropping our flour into our bag and what you want to do is build a foundation as you load your flour I want to fill on the air gaps as you put it into the bag smells beautiful and as you can see we've built our base here we're filling in all the air grabs with the flour so once again on the bottle Tech or Barrel textile whichever you prefer to call it this particular style that happens is again during the extraction itself the bag is going to collapse in onto itself and we're gonna have a nice little round puck left over again on barrel tick you want to leave maybe a half inch at the top of your bag everything else you cut off it doesn't suck up any of the extraction itself and then once you get it to a half inch the other part of barrel tech is you're going to fold the bag over like you what a present one side to the other and then the rear like this kind of load in to the machine this way so let's go over to the note Smasher touch here I'm going to load our bag into the machine as you can see how it folds in and it will accordion in onto itself but instead of going through the whole press process I will show you the result this is the fortune cookie three and a half gram press as you can see beautiful golden rosin and that's what you're looking for is a puck like this using the bottle textile with a nug Smasher is going to give you optimal press and optimal yield using the nug smasher bags once again this is captain dink here in the smash lab until next time", "Qi1Yry33TQE": "good morning I think we are ready for the next talk if this this actually works so not every nips has a test of time award so it's a test of time even to get a test a time award this year has one and the criteria that we decided or the program she had decided on for for awarding it was ten year back nips papers so we formed a little committee and I must say that it was not easy all the ones that did not get their test of time award don't feel too bad this was not an easy decision but we did arrive we did arrive at a paper that we definitely felt was was was really really good and we're happy about it so just let me introduce it in the author's own word random features I trick to speed up supervised learning algorithms so they can operate on big that as millions of data point data says traditional learning algorithms work too hard because they optimized parameters that don't action need to be optimized instead one can randomize over some of the parameters and quickly optimize over the rest thanks to the concentration of measure of phenomenon this still provides excellent accuracy this recipe makes for accurate learning algorithms that run extremely fast and are very easy to implement so the paper introduced random Fourier features sampling to efficiently approximate Gaussian kernels it also introduced random painting features to approximate separate separate separable multivariate shift invariant kernels later they followed up with random stamps for boosting and random support vectors for empirical kernel Maps and from the later variations of this paper follow a very popular term the random kitchen sink so without any further ado in case you shouldn't have guessed what paper it is it has random features for life scale kerning machine colonel machines by ali rahim a I've been briefed and here both of them let's give them a hand and Ali will be giving the talk thank you would you mind oh thank you okay it feels great to get an award thank you but I got to tell you nothing makes you feel old by getting an award called test of times it's forcing me to come to grips with my age Ben and I are no longer young so we decided to name the talk accordingly we're giving this award for this paper this first paper up here but this paper was the beginning of a trilogy of sorts and like all stories worth telling the good stuff happens in the middle or at the end not at the beginning so if you'll put up with my old man ways I'd like to take you all the way back to nips 2006 when dinosaurs roamed the earth and Ben and I were young spry men deep learning had just made it had just made a splash in mid-2006 the training algorithms were complicated and the results were competitive with linear methods like PCA and linear SVM's at the workshop some of us were kibitzing and somebody pointed out that this should maybe be compared against long non-linear methods like SVM's but of course at the time you couldn't scale SVM's up to that size there's the data set Ben and I had both been working on randomized algorithms separately Ben for compressed sensing and AI for sketches to speed up bipartite graph matching for computer vision once we got home it took us just two emails to come up with a way to speed up kernel machines these two emails became that first paper the idea was simple normally when you train a kernel a machine on a data set you slap one kernel on top of each data point and an associate of weights to each one of those kernels and you let your optimizers tune those weights to minimize the loss Center training there here's the here's the observation in the paper most of the kernels people were using at the time can be approximated as a straight sum of the product random functions if you plug in that approximation in that first equation you get a linear combination of a straight sum which is just a linear combination but one with fewer coefficients to solve for and that's something the optimizes at the time could handle the paper provided ways to approximately popular kernels as randomly like this and also provided guarantees about the quality of the approximation if you have a kernel anyone are approximated with such and such fidelity here's how many random features you need and in practice the method worked very well now we'd set out to provide a baseline for deep learning so they could compare against nonlinear methods but we can find any code to compare against this was before machine learning was reproducible the way it is now so we compared against what was reproducible at the time which was boosting and non an accelerated kernel machines of various cons during our presentation at the the poster session we handed out this leaflet to showcase how easy it was to train large-scale kernel machines it's a hey buddy you want to you want to train a chroma machine here's just four lines of MATLAB code a little bit of a guerilla marketing for for an algorithm now there's something a little shady about the story up and telling you according to the bound I just showed you and that bound is a fine bound in order to approximate a kernel accurately with these random features you need tens of thousands of random features and yet in our experiments we were getting away with using just a few hundred features and getting good results even more strangely in some of our experiments our approximate method was producing lower test errors than the original kernel machine we were trying to approximate it's always weird when the thing you're trying to approximate does worse than your approximation this is relevant because back in those days machine learning had just finished transitioning from what Samer always used to call an ideas conference into something more rigorous during the poster sessions you could see this roving band of what I usually call the nips rigor police they would come to your poster and make sure that your arguments were airtight I'm thinking of of naughty cerebro oh for deckle some of Mike Jordan students if you're unlucky shy been dahveed and if you're really unlucky Manfred warm booth but anyway we decided to send the paper out as is with this da genus in it and brave than the nips rigor police but to do right by them we eventually came up with an explanation for this phenomenon and I'll share it with you here's the algorithm without any of the kernel flim-flam straight up you just draw a bunch of functions independently from your data set you weight them and you tune those weights so that you get a low loss on your training data set okay in the second paper we showed this in the same way that Fourier basis provide a dense base basis set for for an l2 ball of l2 functions or in the same way that three layer wide neural networks could approximate any smooth function arbitrarily well so to do a bunch of randomly drawn smooth functions approximate densely I hope function in a Hilbert space arbitrarily well with high probability so now you don't need to talk about kernels to justify these random features they don't have to be eigenfunctions of any famous kernels they're a legitimate basis for learning into themselves in the third paper we finally derive generalization bounds for the algorithm I just showed you if you have a data set with this many points and you want to achieve this kind of test error on the data set here's how many random futures you need now at the time but by this point we were now just no longer thinking about kernels we legitimize using taking a bunch of random kitchen sinks and combining them together and you don't need to justify that the approximate kernel isn't it anyway so it didn't really bother us if we were underperforming or over performing kernel machines or if we had to use fewer parameters or more parameters than a caramel machine we'd set out on this journey to provide a baseline for deep learning and we can do it at the time but since then the field has become much more reproducible and various folks have provided baselines in speech where random features are competitive with with deep learning to this day I myself still use random features at work I'd like to get creative with the random functions I use and adapt them to the problem at hand when they work well and I'm feeling good I say to myself Wow random features is so powerful they cracked this dataset or if I'm in a more somber mood I might say huh this poem wasn't hard at all even random features cracked it it's the same way I think about nearest neighbors when nearest neighbors does well you can either marvel about the power of nearest neighbors or you might conclude that your problem isn't hard to begin with both of these algorithms are good solid baselines and a way to get a diagnostic on your problem it's not 2017 and the field has changed we've made incredible progress we are now reproducible we shared code freely and use common tasks benchmarks we've also made incredible technological progress self-driving cars seem to be around the corner artificial intelligence tags photoes transcribes voicemails translates documents serves us ads billion-dollar companies are built on top of machine learnings in many ways we're way better off than we were 10 years ago and in some ways we're worse off there's a self-congratulatory feeling in the air we say things like machine learning is the new electricity I'd like to offer another analogy machine learning has become alchemy now alchemy is okay alchemy is not bad there is a place for alchemy alchemy worked alchemists invented metallurgy wasted died textiles are modern glass making processes and medications then again alchemists also believed that could cure diseases with leeches and transmute base metals into gold for the physics and chemistry of the 1700s to usher in the sea change in our understanding of the universe that we now experience scientists had to dismantle 2,000 years worth of alchemical theories if you're building photo-sharing systems alchemy is okay but we're beyond that now we're building systems that govern healthcare and mediate our Civic dialogue we influence elections I would like to live in a society whose systems are built on top of verifiable rigorous thorough knowledge and not on alchemy as aggravating as the nips rigor police was I missed them and I wish they come back I'll give you examples of where this hurts us how many of you have devised the deep net from scratch architecture and all and trained it from the ground up and when it didn't work you felt bad about yourself like you did something wrong please raise your hand this happens to me about every three months and let me tell you I don't think it's you I don't think it's your fault I think it's gradient descents fault I'll illustrate I'm gonna take the simplest deep net I can think of it's a two layer linear net so identity activation functions and the labor labels are badly conditioned linear function of the inputs on the left is my model it's a product of two matrices that get taller and they just multiply the input and on the right is my loss function and this is the progress of gradient descent in a various variants of it it looks like gradient descent goes really fast at the beginning and then it just seems to Peter out you might say this is a local minimum or a saddle point that's not a local minimum it's not a saddle point the gradient magnitudes are nowhere near zero you might say it's hitting some statistical noise floor of the problem that's not it either this is not a statistical noise floor I can compute the expectation of the loss run grading descent and get the same curve this is not due to randomness here's what a better descent direction would do this is Levenberg market it gets down to zero loss it maids nails the solution in a few hundred iterations it gets the Machine precision zero if you haven't tried optimizing this problem please take 10 minutes on your laptop and just try it this is our workhorse this is what we're building models around you might say this is a contrived problem that badly conditions something something just the the columns of this a matrix are are correlated that's the only weird thing about it this is not a contrived problem you might say gradient descent works just fine on more complicated larger networks two answers first everybody who just raised their hands would probably disagree with you and second this is how we build knowledge we apply our tools on simple easy to analyze setups we learn and we work our way up in complexity we seem to have just jumped our way up this pain is real here's an email that landed in my inbox just a month ago I'll read it out loud for you it's for my friend Boris on Fridays someone and another team changed the default rounding mode of some tensorflow internals from truncate toward zero to round to even our training broke our error rate went from less than 25% error it's almost 99 percent error I have several emails like this in my inbox and you can find similar reports on various boards on various bug reports on the public internet this is happening because we apply brittle optimization techniques to loss surfaces we don't understand and our solution is to add more mystery to an already mysterious technical stock batch Norum is a way to speed up gradient descent you insert batch norm between the layers of your deep net and gradient descent goes faster now I'm okay using technologies I don't understand I got here on an airplane and I don't fully understand how our planes work but I take comfort knowing that there is an entire field of Aeronautics dedicated to creating that understanding here's what we know about batch norm as a field it works because it reduces internal covariant shift wouldn't you like to know why reducing internal covariant shift speeds up gradient descent wouldn't you like to see a theorem or an experiment wouldn't you like to know wouldn't you like to see evidence that batch norm reduces internal covariant shift wouldn't you like to know what internal covariant shift is wouldn't you like to see a definition of it bachelor on has become a foundational tool in how we build deep nets and yet as a field we know almost nothing about it machine learning is sick in a new spot in society if any of what I've been saying resonates with you let me offer just two ways we can assume this new position responsibly think about in the past year the experiments you ran where you were jockeying for position on a leaderboard and trying out different techniques to see if they could give you an edge in performance and now think about in the past year the experiments you ran where you were chasing down an explanation for a strange phenomenon you'd observed you were trying to find the root cause for something weird we do a lot of the former kind of experiments we could use a lot more of the ladder simple experiments simple theorems are the building blocks that help us understand more complicated systems here's a second second thing we could do right now are mature computational engines that run on commodity hardware are all variants of gradient descent that's what we have that can handle tens of billions of variables imagine if we had linear system solvers or matrix factorization engines that could handle tens of billions of variables and operate on commodity hardware imagine the amazing kinds of optimization algorithms we could build imagine the incredible models we could experiment with of course it's a hard math problem and it's a hard systems problem but this is exactly the group that can solve these problems over the years many of my dearest relationships have emerged from this community my affection and respect for this group are sincere and that's why I'm up here asking us to be more rigorous less a chemical better Ben and I are grateful for the award and the opportunity to have gotten to know many of you and to work with many of you and we would love it if we could work together to take machine learning from alchemy and into electricity [Applause] I thank you so much for this talk I hope that we'll be able to give you a new award in 10 years for a net from now for the return of the rigor in the nips community we have time for probably one question or two since we are since since we all just stunned by by them versus here which we all appreciated so much I just want to invite been up here as well so I can hand you the official certificate for the test of time award [Applause] here by it is the end of the test of time award and the program continues on I think there's a break now so we you", "50Ym59hm-MM": "if you're not 18 please [ __ ] off thank you what's up guys we do mister boy came mark as you can see we out here and the wife-beater we out here different seeing different changes scenario change of scene reason being as hot as balls it's a very hot summer day so we all hear out my porch and we're gonna do a new review one interesting item that we got today guys a reclined collector quarter eclectic let's check it out so we got the chem one water hash chem 91 oh yeah nice and Sandy yes so my goal is to be your only dabbing products like this and this in order to see like pow come on what kind of reclined we can get you know see if we can get that beautiful flavor and since because of the collectible this rink line will not be touching any type of water will not bounce around and burn all crazy due to it being laced with water it's gonna be nice and let's see you know let's see what will you do let's see what we can get but we'll have the results of that probably like you know two three weeks from now of daven in another video but for now let's take a nice clean to having a new nail of some beautiful exotic rare wax [Music] Oh [Music] very nice smooth that little nail is like it's a small little coarse finger it's very small so it's good it's great we decided I had to decide to come in the room I was already kind of high so yeah I was gonna I had to decided to change the scenery once again back into the dungeon so we here and this time it's time to take a fat dab of this that was the Percy guys taste amazing goodness it tastes like nothing but wow it's it's unquestionably OG crossed with sour tangent get another hit like that holy [ __ ] got the snot out of me guys holy [ __ ] good alright guys let me know what you guys think about the reclined collector I'll leave a link in the description down below where you guys can purchase it alright ah back into this outro yeah I know what the [ __ ] deal subscribe I know you guys see this I know I guess I don't get it [ __ ] of a shitload of views do get a few amount of views I don't know for a fact you guys been watching this [ __ ] so it ain't hard just [ __ ] click on that Bell and click on the subscribe button what's so hard about that come on now click on that [ __ ] stop playing with me not be seeing my [ __ ] but you don't subscribe subscribe down there below let me know if you guys know of any other good ass flavours of some percy or some water hash cuz right now that's my [ __ ] i love right now on the hash hype let me know there's other forms of hash that I can dab that are really delicious to try out and like always yard notes by going on [ __ ] like [ __ ] subscribe very no fun going down do it", "AZRT8rSeN4w": "how's it going everyone get Nick on the IG live right now cool thank you all for tuning in hey what's up Nick reading brother how are you hey I'm doing great just gonna get my camera set up proper here what are you smoking on right now dude secret cookies times Cushman's whoa that sound like a mean cross it's if it's Annecy bro it has that like yeah that that the anise flavor that I love so much to like kind of hello to a lot of people that know that you know that string flow oh yeah flavor in this one that was DJ shorts right yeah yeah yeah okay awesome yeah I'm a big fan I used to buy blow back at uh when I was a medical patient in Boulder ages ago as like one of the best daytime strains I love it you get like um I feel like you get a lot of that anise ace from the the dur being that come from the cookies you know it means bringing some of that back awesome well thanks everyone for tuning in I'm probably gonna say this about ten times throughout the course in the broadcast here we've got legendary hash maker Nick T with us founder of essential extracts I believe the first licensed and approved solvent with specific lab in the world is that right yeah yeah yeah as far as the regulations go in the United States the first tax paying companies tax pay okay that's a good distinction so super excited to have Nick on the show today we're gonna be doing a mix of you know answering some questions but I've also got a lot of great questions for you Nick you know we go back a little ways shot some awesome videos back in the day and we're just really focused on educating and talking about solving lives I mean Nick how long have you been making hash for for everyone who's tuning in give us a little background about you know how'd you get into it why'd you get into it yeah so um I'm blessed with grown up in the Bay Area in Northern California where cannabis was really prevalent upon my upbringing you deserve you know ever since I can remember ganja's been around I think I remember smoking my first spliff with a German foreign exchange student when I was born now for that I wanted to know how to make the hash you know so as a teenager I remember driving up to Mendo picking up trim and material to process into bubble hash as far as I you know growing up in Northern California bubble hash was really really prevalent you know not a lot of places throughout the world could you get bubble hash yeah you know there was isolated in Europe but then other than that you know a lot of the water ash that was being produced was being produced out of Northern California so you were part of that whole original wave bubble hash ice water hash makers out in NorCal and you came out to Colorado in mid-2010 what what prompted the move actually I originally would Colorado in 2001 Oh lot longer before older so I think they cared a little bit of time I see you both are together but on your snowboard team shout out so originally I moved to Colorado in 2001 and it's because I actually fell in love with the snow and I wanted to find a college that had a good psychology department that was also near the snow snow boarder so yeah first you know coming out to Colorado I brought some of that you know technology making water hash in Northern California out to my Colorado house and started making hash you know during college and really honing in on my skills at that point in time I started traveling to Amsterdam and learned a lot of oral tradition from milla milla mosh Queen and from there we've just really been trying to control variables and produce the best quality product possible good that that's awesome so you've been at it not only a long time what prompted essential extracts when did that come into your mind when did that all get started 2009 2009 is when essential extract was thought up by 2010 we were licensed we were an LLC licensed entity in the state of Colorado and yeah it's just crazy you know still to this day I'm I'm so blessed to retain that name because I feel like it's such a powerful name essential extracts especially as we move into this Rona situation in cannabis yeah so we've been essential you know so the trip you know and even looking back at my first logo designs back in 2009 to where we're at today not much has changed and it's almost like I knew about rosin you know in 2009 when I made this first logo almost look like a press the prophecy yeah it's really a trip man it's really a trip to see how far we've come dude that that's awesome super excited to have you on today thank you everyone who is tuning in if you've got some questions that you want to ask us go ahead open up the question box at the bottom and we will start going through some of those it's a lot easier to answer them when people ask them through the prompt going through the comments that are going through the feed you know that that's a little more difficult so we're just gonna be chatting about hash today what are you up to these days I know you've been world traveling making hash all over the place what what are you seeing out there man it's really really cool to get a peek into everyone else's lab in these last two years I've like you started I've traveled all around the world I've spent a lot of time in Spain I've seen some labs and helped build out some labs in Jamaica I'm processing in Canada I mean you name it it's been it's been a blessing to really get to see what other people are doing you know I've been stuck in my own lab for a decade straight working on our own R&D but it was really really cool to travel the world and see what other cultivars people are running and how you know just the little differences that we see you know as all my travels even just from the power you know the power grids in pain compared to the power out here and learning you know how to you know convert the power and use different adapters and stuff like that you can because it's not really just hash making anymore a lot of equipment and the technology and the environment that you're playing so it's giving me you know a lot more knowledge just by playing in all these different environments throughout the woods so when you were in Spain for a while because I know you've done a couple stints out there making hash with a bunch of famous European hash makers and just really seeing what's going on what what's different between you know what's going on here in the States and what's going on over in Europe first and foremost let's big up Europe for like really pushing water extraction you know isolator awesome ice because they were really the you know the trendsetters in that game I've just kind of followed suit and played with no variables and you know kind of created our own little variations on the product but the cool thing about Europe is Europe has this access to be no hunting that we don't eat too much out here on the level that we see in Europe we're seeing them plot you know millions of seeds in Morocco and taking some of those you know Fino selections to to the market in Spain so I think that's one of the coolest things that I personally have seen is the inability to really really you know hunt on a large level and find those cultivars that are crushing for action and you know one of the guys that I always like to bring up is La Sagrada and that Barbara but you know the premier found it's just something special I mean those were the first that was the first time I saw a guy is doing whipped melt you know not even fascinating the melt because they were getting such a clean product that and a lot of that has to do with the cultivar so you know the you know be no hunting that's going on in Europe is pretty rad and they're just pushing the boundaries you know they're pushing it just as hard as we're pushing it here in the United States and it's cool to see because what four years ago in Spain it wasn't nearly what it is today and they're just progressing so fast right now so ok so that's cool I mean it sounds like there's a lot more on their that and some of us realized and I know you've been going to spannabis for a while and it seems like that's where everyone is really connecting over there and then it all kind of blew up with the koban situation this year yeah you know the off the span of this events are really really key that's finding factors of rosin came into play and you know spring just treats me really really well I'm just blessed to be you know to have a second home out there I'm sure the food and the hash are on can wait in Spain Turks Turks that's awesome I'm gonna go ahead we're gonna go through a couple questions here and then I've got some more for you so pop this one up advice to dinner hash maker so this was one I was planning on asking you anyway you know for people have tuning in that are making hash people that are just getting started what advice for you have for them to you know get into the game and be successful yeah first of all take initiative you know industry even though I'm talking about being in it for over a decade now this industry is still brand new there's emerging markets everywhere throughout the world now take initiatives take the power into your own hands because if you're not pushing the movement forward and who's gonna push it forward you know so something I really recommended people just trying to get into the industry is um you know putting yourself out there you know bringing up new tech and just trying new things because if you come up with something brand new that some of us haven't played with before we're gonna want to talk to you you know we're gonna want to learn I'm I'm means I'm learning just as much as I am teaching that's awesome and I know that I I don't know who was technically the first person to start doing solvent less sauce and diamonds but you guys had to be among the first if not the first I mean I remember early on when we started doing some videos with you guys we went over to the lab and you know you had done some mechanical separations and separated off some charts and we're making some diamonds was that just part of the RMD process or you know when you guys were doing some of your own are indeed you know how'd you get into that no we were definitely not the first we might have been you know one of the first to really make it commercially available ok good distinction really what we pride ourselves on is you know scaling up a lot of these technologies and bringing them to the forefront bring them to the people via you know regulating their routes yeah but um you know the cool thing about me being able to travel around the world is I think I actually saw the very first mechanically separated th CA and this was before they even had test results on it so these guys were like yeah it's th CA and I'm like huh where's the tech dogs I don't really believe it me being able to travel allowed me to see things like that I've been like bringing this idea back to Sam Samwise Gamgee who was yet back here in the lab and I'm like bro I just seen this crazy [ __ ] white powder stuff what is this have you heard of this and he starts doing this you know research and stuff and then we just started playing in the lab and that's how a lot of this has come up I really don't claim to be the first sure do a lot of this you know water hash has been around for generations and centuries you know but uh we just really try and hone in the variables and being able to work in this regulated environment with you know the ability to control temperature humidity and write everything down and keep track of all the SOPs has really allowed us to excel and push this industry further you know yeah that's awesome and I'd say you know it's really refreshing to talk to you and you know your honesty about the growth of the industry and where it came from I just feel like so many hash makers that you can see online on YouTube you know they're all about claim and credit and being the first to this and the first of that but it sounds like you take a little bit of a different approach you know you're all about learning and trying to see what's coming next 100 percent man I'm still learning every day bro and that's the coolest thing about this solvent this community there's really no peak to what we can do and what we can learn and you know what we can create what five years ago we didn't think separating th CA was possible ten years ago fifteen years ago I was working in a hot barn you know in and making black Milton smelling booth so uh you know we all came from somewhere and I think all of us still have a lot to learn yeah so on that where do you think the industry is headed right now with solvent list like where do you see solvent list going in the next year the next couple years I mean what's the future outlook and your what is the future outlook in your eyes well number one what we've seen a huge trend in in the last five years I think I actually wrote that little section and when it rose installs books about five years ago about the heavy hitters of hash it see I know that article friend of finding specific cultivars that worked for specific processes so first and foremost allocations are huge you know some cultivars some strains don't perform well in water hash some strings don't perform well with hydrocarbons in realities so really finding the correct allocations for the strings that we're growing is gonna be huge and from that is where we're gonna really be able to devise you know what strains are gonna help with what you know needs we have what about the ailments whether that means finding strength that have really really high THC to then create THC a from that as an end product so I think that you know rather than you know trying to manipulate all the different cannabinoids and you know separate everything like they're doing in the hydrocarbon world right now I think we should as a solvent this community be focusing on that input because that's what's gonna create the end product really yeah so who are some you know genetic companies some other hash makers that you're a fan of right now you know who do you think is out there pushing the boundaries and solve unless I'm sure the list is almost too long to you know it's too long but who were who are some of your top people that you follow and that you get inspiration from I mean I still get inspiration every day from Mila you know like she is still targeting the [ __ ] world reading but you know I learned from her almost 20 years ago now I've been awesome and you know she's a huge inspiration by leveraging just seeing her positive post every single day her comments and stuff on all of our Ashley's expose it's cool to see her just still charging all my European family as we mentioned is just crushing la sagrada HQ my brethren slight doc aged over there and his wife it's a power couple right there just crushing out fire out there and a lot of the breeders so I'm really looking up to right now everyone from onei seeds the third gen to Mike from exotic genetics from I mean you name it there's so many different breeders out there that are focusing on breeding for hash production so big up each and every time because that's really the future is finding those strains that do what we want them to do whether it's an appearance thing whether it's a cannabinoid profile thing you know the breeders it starts with the Seidman ya know that's awesome and you know there's there's so many good geneticists out there's so many good people trying to make crosses item Sultan list I would HIGHLY encourage everyone who's tuned in now you know follow as many influential hash makers as you can because you'd be surprised how much knowledge gets dropped on Instagram I'm sure most of people who are tuned in right now or all on top of that but you know it doesn't hurt to expand your follow list I am gonna sprinkle it in with a couple questions here because you've got as much knowledge as just about anyone in the hash game what is the best way to store live hash rosin you know what what's your take on this if it's fresh crust if you're storing it for a long period of time I recommend the freezer you mean if it's fresh pressed and you're trying to consume it you know in the next couple days a wine cooler it's probably good that brings the temperature down that doesn't have the t-midi d levels of the refrigerator as reality I've really really been liking some of this pressure tech or some of the jar tech or shelf stability because after you've kind of done that you know a lot of these different styles and tax with pressure and temperature we were able to see you know a lot more shelf stability and you don't have to keep that product in the refrigerator or the freezer I've had some of this jar tech that I've been playing with just sitting out here in my living room in ambient temperature and it has not changed one bit well that's really the advantage I see to the jar tech over the fresh best okay well that's a great question and something I definitely wanted to dig into a little bit more and every time we've done an IG live you know we've had about ten questions about jar tech it's a topic right now that everyone is learning about trying to learn about and I think something you just touched on I'd really love digging as well is you know shelf stability you know we're talking about products selling a product that's consistent and having a product that can stay the right consistency for awhile without needing special conditions you know walk us through jar tech a little bit have you been have you been playing with that for a while what's your take on jar tech it's really cool because jar tech really kind of came to fruition about two years ago as I was really touching the road as I was getting out of my lab you know that I was in 24/7 I'm starting to get into other people's labs and so I got into got to learn jar tech on the road and smooth everyone's different techniques as far as temperature as far as duration etc etc so I've been kind of in an interesting position where I haven't gotten to really really hunker down in my own lab as much as I'd like to in the last couple of years but I've had the experience of seeing what everyone else is doing throughout the world so if you had got to play with some jar tech in Spain at the HQ lab with slight I got to play with a little bit with la sagrada and now in this last you know couple of months during this interesting time I've been able to get back into my own R&D lab and start to play again I think but I still have a lot to learn myself with the GR tech I'm just just starting to delve into it you know you could say ya know that's awesome so it sounds like you've seen how it can be done right and you're saying that you know what you've seen some of these other patch makers when they're you know using jar tech that they are getting some shelf stability that's you know pretty pretty high-end and and really able to make that consistent product yeah you know I'm not seeing any change if you want the jar tech right it's really really cool to see you know the shelf stability yeah plain and simple that's the main difference that I'm seeing between fresh breasts and relative gotcha that's awesome so for everyone who's tuned in right now like I said at the beginning gonna do this shadow of just a couple times so everyone knows what's up we got legendary hash maker Nick T with us today this video will be up on YouTube probably early next week so anyone who's tuning in at the middle didn't catch the beginning just tuning in now you'll be able to go back and watch the full thing we'll also have it up on our Instagram to watch the whole thing when we're done here got about another 24 25 minutes left we're just going through some questions and talking with Nick about hash washing and solvent list so what are some strains that you've been really liking lately do you have any you know recent modern winners that you've gotten to wash press both I just asked me this question I go to like my personal favorite I've really really been liking these lemon terpenes what for actually finding viability so let's back it up 10 years ago okay and there was this strain called lemon G that came out of Ohio Ohio okay I love the strains terpenes it was like the first time I really got to experience these piercing lemon crying turkeys rather than like the super super lemon haze lemon cleaner terpenes which people called lemons for years I you know the soon as I found that lemon G I was like whoa this is the lemon I've been looking for since that lemon G there's now been you know the resurgence of the lemon tree or the Oregon lemons or whatever that you know came from originally and that one yielded a little bit better in water half so it sorry look skipping ahead a little bit let me measure Cooper Airy it created these big colas that never really look that great in flower form and didn't wash well in water hash mmm bass board you know five six years or so and we're seeing the lemon tree it yielded a little bit better in water hash but still wasn't the most viable for water extraction gotcha just in the last couple years we've been finding some strains you know that have that lemon turf profile that also do some some washes so yeah those are those are the ones that I'm looking for right now those are some of my favorites yeah I also just keep falling back on the Ken lines the ken lines just do really really well for us as far as stability as far as yield as far as terpenes as far as cannabinoid profile you know it's just a nice all-around washer and then you know man you know keep going on those those citrus types herpes a lot of these orange flavors and pane terrine flavors we've been finding washers it's like the Tropicana cookies all the stuff purple hash some really really cool stuff out there people are saying papaya in the feed and it's funny you know like we we selected those papaya washers back in 2011-2012 you know big you know big screen in the wash game right now it's not like my favorite thing because I'm looking for things that I really like in flower form and in hash form and the flowers of papaya just don't have the oil but I'm looking for yeah so this is a great question for you when you're selecting a strain that you're hoping to wash get a good yield out of start working with it do you do any test washes is it a visual inspection I mean before you go all in and commit to washing pounds of fresh frozen how do you what's your selection process to try and find where that's an amazing question and I I feel like I answered almost every day now but thank you for answering it again for us here I love talking about you so first and foremost we really start from the cultivation we really you know want to help consult with our cultivation to start with those you know washers to begin with we also you know help to teach you know the cultivation to not touch the plants it's a big part of it you know I've seen a lot of people going in there and just manhandling the plants in the growing process and in the harvesting process just be really really gentle now decides that that I do want to set out you know some plants aside so that I can actually touch that you mean that's a real big part of my selection going out into the field and not manhandling all the plants but picking a specific plant and just feeling what those trichomes feel like smelling them getting a feel the turf profile and I can really devise a lot by the field I like to scope the trichomes as well and look at the density of trichomes plant how many trichomes there are actually you can see a little bit of the structure of the trichomes of the scope yeah well really get 85% of them if I know it's a watch or not it's feeling it and you're gonna feel differences like grease eNOS you're gonna feel tackiness you might feel sandy and grittiness and so we look for a lot of those different features okay so you've got a pretty artisan level selection process before you're gonna wash you know out of curiosity what water some strains that you found that or some lineages that just don't play super nice with ice water hash are there any that people on or should you know maybe cautiously avoid or you know look a little more closely at before they commit to a big wash I always use the example of blue dream for a lot of people that screen is you know over and they're tired of seeing and hearing it in reality that spring is still being sold and being grown throughout the world whether I like it whether you like it or whether we like it or not but that strain notoriously and the cut gets going around that yields two plus pounds of light in milder form just as not yielding water hash form we're seeing like less than 3% yields you know and that's variable there's some cuts of blue dream out there that I've seen crush so you know it's everything is to be taking with a grain of salt but for the most part that blue dream is not gonna do the numbers you want to see in water hash but you allocate it to flowers you've allocated it to be a tone it's going to do amazing numbers 20 plus percent yields and DHL ready percent or less in water hash along those lines I really don't recommend people washing most haze cuts mm-hmm few things you know we found dinos of ghost train Hayes that did great numbers for us so there's always going to be those outliers but most of the time phases of there's not going to be the best washers okay know that that's awesome advice and you know one thing that a lot of our customers ask us it's kind of like you know how do I make sure I'm washing stuff that's gonna yield what I needed to yield and you know we always like to tell them you know before you go all in on pounds and pounds and pounds you know maybe just do a small walk before before you really make that commitment and end up having some crazy price per gram you know production cost so you can go back to the old school mason jar text you know when I literally have this clear vision of me doing it in my kitchen in Boulder Colorado in 2004 just playing around and shaking up jars of ice water with a little bit of plant material and you can really see the chart on this fall and you can get a good idea that if that strain is gonna for you or not I'm just doing a small you know ounce fresh frozen mason jar ice water tech thank trichomes will always sink plant man always rise to the top whether you're talking about utilizing bags and separation or gravity no that's awesome and you know as you were talking about that we had a couple people comment that exact same thing shoutout to Rubin I think giddyup was in here we've got some real other high-level hash makers tuning in so thank you all for joining us we are hyped to have you here I'm gonna go through we've gotten a bunch of questions we probably won't get to all of these because so many came through but if you got a question that didn't get answered here shoot us a DM we'd love to try and help you out so pressing tins put you on the spot a little bit what's the largest yield you've had from frozen material what's your what's your yield range look like and joints and fine product you know the largest yields we saw in my personal live in Colorado we're upwards of eight percent a little over eight percent that's on our Kenzi oh nice um however I saw upwards of nine percent doing some collaborations over at the field lab field extracts lab in San Francisco I asked the gorilla princess their profile so we ended up not using it in the collaboration but that was a collab between skittles I believe and field extracts that release that but I happened to be there doing some of the test washes thing was throwing some numbers so yeah that's awesome we've you know you hear a lot of things a lot of people want to throw out big numbers about how they're you know hash wash this or it dump that I mean nine percent is massive you know the guy who asked his question prescient ins he sent us a message saying that he checked the math three times and was looking at around an eleven percent on one of his that he watched which women that's like a Guinness World Book of Records fish coming out of the lake all right pressing Tain's yeah you might have to keep that one under wraps just another quick one that came through here and the feed because I think that you're you know tapped in a lot more than I am some of these what are some books you recommend for anyone who's trying to learn about hash you got any authors I mean Edie Rosenthal I mean who should people be looking for what kind of book material is out there that's worth buying look at anything that Robert Clark's is written he's an OG you've been studying trichomes for a lot longer than I have so right yeah Robert Kyle Clark Edie Rosenthal has you know a lot of these beyond buds and that series and stuff things get really a lot more into depth you know I'm trying to think of other books actual you know hardcopy books that are out there I don't check out Robert on Clark that's that's my og as far as hashish check out you know the book Ashish just to get some background awesome all right Kevin check that out I'm about to order the copy myself because I'm always trying to learn to and I haven't really heard of him or that book so that sounds like an awesome one and one other quick one that came in here someone's asking you know how our yields solo with such good starting material this is Chris 1111 quick one here you know we're talking about 9% 10% we're talking about a fresh frozen wet wait to a dried material wait so you're fresh frozen yeah we've heard hash makers tell us anything from three times to five times your weight of a fresh frozen pound that's water weight so you know you take that 8 percent that 9 percent that 5 percent whatever it is multiply that by would you say three four what's your what's your gut on that consistently have seen 75% to 80% water weight in everything we plan for the last decade you know we've helped consult the Department of Revenue to put in a fresh frozen button into metric what [ __ ] five six years ago on my first day that was so you know we have a lot of data showing that that percentage of water weight you know vary a little bit depending on one last time you water one last time of course cultivator watered the plants etcetera how much water weight that plant was carrying but yeah seventy-five to eighty percent so yeah so Chris just so you know you know take that number multiply it by four you don't have something that's a little bit more realistic out the other end so okay cool go through a couple more questions here got a little bit of time left what do we got cool all right here is it here's a good one hand a machine wash for starters a lot of options out there I know that you've done a whole bunch of both what's your take on the current state of machine washing verse hand washing this is a is a hot topic into the fully regulated industry with the health department the fire department and having to have ul listings on all machinery within these licensed facilities you know a lot of the plastic washing machines are going away you know even licensed facilities unless you have one of these brand new agitators that are coming to market every day now new companies are coming out of the things unless you have a new ul listed fully functioning insulated agitator I think man washing is kind of the way of the future and the way to go right now so with hand washing one thing that we've heard and seen quite a bit of is you know doing multiple washes and having a hand wash to really be able to control whether you're trying to make melt you're just going for full-spectrum you know that that's really your end product is gonna dictate how you wash or is it the other way around you do your wash and then you find out what it gives you or is it both it's gonna be a little boat but I'm always gonna think about the end product what the consumer what you know cultivate or whoever's giving me that product whatever exposed to be as far as the end product that's what I'm gonna be thinking about we've been beginning as far as how many bags I'm using is how far out how much I'm washing how many runs I'm doing how I'm separating that out allocating the time you know cuz you know as we move if there's a client that just wants rosin we're gonna agitate a little bit differently we're gonna separate a little bit differently than if we were trying to create the fullest melt possible okay so different endgame if you're just making a full spectrum rather than product versus if you're trying to get that melt out the door yeah that's awesome but temperature and humidity control is this huge environmental control is just big breath the second year Labrador facility or your home goes above sixty five degrees we've mentioned that that temperature for years now you're gonna start seeing you know it harder to separate the glands properly harder to pull all the moisture out before you either go into your freeze dryer before you do your sieve method before you do your micro play method whatever method you're utilizing but it really just push so I remember you know the handful of times we went back to your lab you know in the winter nohe everyone's wearing winter coats so what do you think are the most important environmental controls I mean is it making sure your waters you know reverse osmosis is that the temperature is it all of it I mean one of the questions we got asked that this you know ties together pretty nicely as you know what tips are tricks for a beginner who's pressing you know if you're trying to control your environment what do you really need to be thinking about temperature fire means the first thing because if you can't work in an environment that's below sixty five degrees I wouldn't recommend you mmm yeah edit it seriously can be a headache to watch especially if you're working with high quality material mm-hmm you're working with lower quality material it's gonna be a little easier for you and you won't witness the same headaches but I think all of our goal right now is to create the highest quality possible and if you can't control your temperature first and foremost I would recommend doing it somewhere else okay so the margin for error with premium quality starting material is razor thin whereas if you're working with you know mids for a lack of a better term you can definitely get away with a lot more with your environment yeah because you know but you're also not gonna everyone keeps saying how do I get those colors how do I get that consistency you're getting you're not gonna see that where it's so good point you could you can only make so much with material that's not primo yeah so how would you answer this for someone who is a beginner or brushing and pressing I mean what are just some high level tips and tricks' control sterilizing your environment you know if you have animals in the area that you're making hash idea ash is really really sticky so anything from your environment even the humidity in your environment will get sucked into that hash you know temperature humidity sterilization really sterilized all of your equipment and all of your surroundings before use and break out any material but I really recommend you know after you've talked about the actual temperature and humidity of your environment sterility is huge on my list so cleanliness is absolutely critical and I mean it makes a lot of sense why because you know if you got dust if you've got things floating around in your lab that's gonna end up in your hash and that's not what people want schmuck nope and you know taking all these macro images and working with all these photographers you really see everything you know in in your hash so highly recommend sterilizing everything in your environment your washing machine if you're going to use the washing machine your bags if you're going to use bags and your counter space your floors even because guess what even if you never touch the bottom of your bag to the floor the back the bottom of your bucket or your washing vesicle or your you know whatever you're separating in is going to be on the ground so just clean the cleaning the floors the countertops make sure you're buying itself yeah yep well said so for everyone who's tuning in and may have missed the beginning of that you want a pro tip that doesn't get brought up all that often it's you know the materials big temperatures big human he's big make sure your environment is crystal clean because otherwise that's just gonna end up your hat on your ashes that is no bueno you don't want a bunch of cat hair floating around getting getting into your plants or anything so we've got about four minutes left here I would love to have you back on the show in the future neck I mean love hearing what you've been up to go through another question and then I'd love to you know finish it off with whatever you want to talk about so let's go with this this is this is a great one can post-processing I think he meant to say ad instead of an value but can post-processing add value to product I think I know the answer but I'm gonna let you take it away we've been doing it for years there's a lot of different things you can do with that material afterwards that create viability larger margins and a slew of other products what are some of your favorite you know solvent list products that require some post processing I mean one of the easiest things to do that we've done for years you know and we've swayed away from that a little bit we've kind of created a little bit more of a refined process but the easiest thing to do at your own home post-processing toss that chunk of you know fresh frozen material after you've washed it Inc with some coconut oil mmm boil off the water strain out the material make some capsules yeah okay so that that's something we you know I think a lot of hash makers are really taking advantage of and that some people are just starting to wake up to at home is you know multiple washes and using all parts of the Buffalo as it were to get you know you got your primo you got your full spec or your mid and that could be a super high-end product and then having more of your food grade at the end of your wash is that what you guys are typically doing you know multiple washes multiple products yeah yeah multiple washes for sure depends on the end goal 100 and gold awesome cool so got a couple minutes left here for everyone who's tuning in this video will be up on YouTube soon it will also be available on our page so you can watch it for the next 24 hours and then Alex who Nick you remember came in the lab he's gonna be cutting this up editing it and we will get the full transcript so people can watch and get the whole thing so with a couple minutes we've got left I mean for everyone who's tuned in what would you like to share what should people be thinking about what's what's going on in your world man I just keep thinking genetics you know right now is a great time you know all everyone's locked down Topsy's fine find something unique you can come up real fast if you find some a big washer that housing you need to profile though yeah I did that right now is that the seed no that's awesome so silver lining here is if you got some extra time inside right now do some clean you know hunting see if you can find some winners I mean you know Mac has made the rounds GMO making the rounds there's just these strains a lot of cookies and cream you know there's some of these strains that are just legendary for ice water hash and now everyone's running them so cool well thank you all for tuning in Nick great to see you again glad you're doing so well it's awesome to hear about your travels we'll definitely have you back on you know the appetite for solved education is pretty much unlimited so thanks for coming on in thanks but miss Rodri for EDD you blue-ice deck cuban everyone give thanks for watching thank you guys for having me much doesn't respect cool alright hey have a great rest of your day everybody we will catch you in two weeks with Fox and we'll be talking about CBD so thank guys talk to you soon [Music]", "2E61WWEHeWk": "hey guys this is Jane long from the mikrosil research tech team today presenting autodesk revit server 2014 this is the third generation of Revit server products from Autodesk allowing firms to collaborate not just within their own land but within multiple different geographically disbursed locations we're going to talk about revit server 2014 improvement enhancements and we're also going to be talking about some best practices that you should implement for your firm so what's new in Revit server 2014 first and foremost it is support for Windows Server 2012 and I is 8 the installation process and the prerequisites are actually the same as the previous versions as well as the need for silverlight at making Revit server admin page a trusted site dotnet 44.5 is supported on all platforms including windows server 2008 and 2008 r2 replacing a model on Revit server where the newer version retains the original gilad Revit server will detect when disk space on the server is running low and display a warning dialog most of you that implement Revit server already have server monitoring tools on your network in our model management services department we actually notice that there is better model file size management going from 2013 to 2014 so we noticed a Revit model we have that's about 500 Meg's it actually went down to a little less than 100 Meg's after the upgrade so there's also coexistence a previous version available so if you have a Revit server environment for 2012 and 2013 you can insult Revit server 2014 on top of the same server of course you have to keep the cash in a projects folder in separate locations so let's do a little review of understanding the Revit server roles so first off the rolls do not change in 2014 the host still stores the central models the accelerator relays model changes between workstations the Revit server hosts between different geographically dispersed locations the admin portion is just a website that the BIM manager will go to do tracking and model management it's HTTP / / server name / Revit server admin 2014 again since this is silver light dependent you do have to make this a trusted site as you've seen in the previous picture there can be several hosts and several accelerators but does it mean that every host can be an accelerated same time not necessarily posts need better resources there's more computational load on hosts and thus there is more read and write on disk the data recovery strategy so if you have disconnected models it must be replayed on to the host and then resync to accelerators you'll notice that if you have more hosts then you have to replay those models which means that there will be more complications also it can be confusing to Revit users when Revit starts up and a user wants to start a Revit session using Revit server if you have multiple hosts that means that all the hosts will be listed down when they go to file open to open a model from Revit server the breast practice is actually to have an accelerator at each geographically separated office and have hosts in centrally located locations so that means also if you have a data center in North Carolina you have a data center in Virginia etc etc those are key places where you want to place the hosts and then you can segment that off and have accelerators in New York in New Jersey and Connecticut and Boston etc etc etc so how are changes sent to Revit when a user opens a model file open on the left hand side scroll down to Revit server that they see the host that they have access to in essence they will click the host and then they see the models those models are in essence taken from the host but are facilitated through the accelerator which caches the hosts information so when a Revit user opens model it would actually open it directly from the accelerator the accelerator in turn is connected to the host via a wide area connection and it would sink information from the host Delta's with its peers so we just mentioned that the Revit user the workstations actually have a connection to both the accelerator and the host so what happens when the accelerator are disconnected well the workstation will bypass the accelerator and passing through the land send changes into its Revit server host you'll notice that when the user does this performance will be diminished all right so right now we're going to be doing a quick demo on the installation of Revit server on a server called Revit server one these are saying prerequisites for the 2013 version I recommend actually having a wiki page the Autodesk Revit server wiki page when you do the installation open so that way you can actually go down the list and follow along I'm doing this rather quickly I'm after speeding up the video so I'm going to scroll down double chat and install and scroll make sure there's not too many warnings and install silverlight which is from microsoft click download you actually have to allow microsoft com as a trusted site in order to have your server download and run the executable as you can see here our installation takes a few minutes nothing major so I'm click Next install and verify that silverlight install and now I'm going to go into the setup for Revit server 2014 just going to install tools and utilities click accept next uncheck everything and check Revit server and drop down i'm going to be able this will host and admin as well as accelerator distance it's a test and you notice that the install button is grayed out you have to pull the button back up and right now it's installing pretty simple straightforward click finish and close windows out and we want to do is actually open the web admin portion via Internet Explorer again it's server name / rabbit server admin 2014 see we already installed Silverlight but you will have to make this a trusted site so we'll do that right now alright and to refresh and you see that this is a Revit server admin 2014 webpage there is no rsn the ini file so we won't have to add that in right now but before we do want to uncheck hide extensions because we're gonna have to have a RS and that ini file not a txt file you'll follow along here the file itself only have the host just a host if we press that five units see the Revit server one is there and the roles installed that's it thank you all for watching this video I hope you guys found this helpful be sure to check out our blog and YouTube channel for other tips and tricks again this is jane wang from the microsoft resources tech team signing out", "mQlzdDTRJpU": "all right so uh there is this issue around what we've been talking about which is thinking about matrices and their decompositions and we've talked about quite a few decompositions right we've talked about svd qr eigen decompositions we've also talked about other decompositions that exist like shore hesenberg there's all kinds of decompositions that we do in linear algebra with the goal of solving often ax equal to b and more in increasingly there's more a focus on data science right so there's this idea of taking matrices which are basically where your data lives and finding patterns in the data so oftentimes that might be something like what we did in previous homework which is take a matrix and look at the dominant patterns which are oftentimes constructed from the sbd in other words that's often called a principal component analysis and so what we want to do is take these matrices and do these decompositions okay but what we're going to talk about today called randomized linear algebra and this is a a continuously growing field and sense that a lot of the things we're going to do today are really starting to think about matrices and by ends where m and n are massive so we're often going to collect data we're going to put these giant matrices and what we want to do is still do these decompositions to look for patterns in our data and then the question is what happens if that m n is so big that the data itself won't even fit in my memory forget even thinking about computing something just even putting the data in memory doesn't work so what we want to talk about is how do we do linear algebra in some sense at scale and specifically matrix decompositions at scale so this is where this concept comes in what we talked about last time in doing these iterative techniques it was all about how do i do computations and solve x equal to b at scale given the fact that every time i increase my matrix size a lot of the operations the bottleneck has always been in linear algebra is that the computation of these decompositions scales like order m cubed so every time you double a matrix size the computation goes up by a factor of 8. so this is just not scalable especially when you start getting to really large matrices you can't even fit inside your computer so we're going to talk about these randomized decompositions and this is just fantastic stuff and typically it's not taught in a linear algebra course because in fact you know if you look at traffeffen uh the book is old enough that a lot of this stuff wasn't really uh developed at that point and so this is starting to get us a little bit more modern in terms of what we might do to go to scale it's not just go to do an iteration scheme maybe there's a whole other strategy here and that's what we're going to talk about now let me tell you some places that are very interested in randomized decompositions randomized linear algebra uh think about places like google or facebook where their databases are so large you don't have a big enough computer to put that data on to look at patterns like do a pca analysis on customer data for instance just way too much data so they were some of the earliest interested in these algorithms in fact there's two groups that sort of really develop some of these randomized linear algebra strategies and each of them sold their their algorithms one sold to facebook one sold to google and why did they do this because google and facebook wanted to be able to do decompositions of data of data that was so big that you couldn't even fit it on on a computer okay even their computers right which is they have essentially almost infinite resources but they still couldn't fit the data there but they still want to mine the patterns and so this is a way to get at that all right so that's the prologue of what we want to do in terms of starting to frame this randomized linear algebra all right so here we go so what is the goal the goal is to take a matrix right and our goal in doing a decomposition is to approximate the matrix a by two matrices e and f let's just make it like this very generic so this thing here is m by n this thing here is m by r and this is r by n so this is like an outer product so think about this as being some matrix a typically it's going to be more like this so for instance let me give you an example suppose i have a bunch of uh 4k video frames so or even if i take hd or 4k camera pictures then let's say this is the a reference each column is a representation of a picture well in 4k this is like 24 million pixels just for a single picture and maybe have many pictures so maybe i have a thousand pictures but i each pictures 24 million pixels right or it's actually 8 million pixels but it's rgb it's it's red green blue and so you have to specify three color layers and so this really ultimately ends up being 24 million in that direction so m is massive and maybe you have lots of pictures thousands tens of thousands and so it's a matrix like this and i don't think any of you with your personal laptops could go in and put in 24 million vector by let's say thousands of pictures and do an svd on that okay those are really big matrices to try this you're going to just run out of memory okay and what we want to do though is replace this by the outer product of this e matrix where this is n by m and this is going to be m by r columns in other words i want to figure out a way to pick out a matrix e where the columns i represent here spans the column space of the matrix a and i do an outer product with f which is r by n so that when i take the outer product of this i reconstruct the matrix a so i want these two a tall skinny and a very short fat each of our columns or our rows and the idea is that those r columns span the column space of this matrix and the r rows span the row space of this matrix so what does actually mean so what we're really going to do if we're going to be able to do this is we're going to make a major assumption here is that within this data there exists some low rank structure so in other words r is the dimension of the low rank structure uh let's call it features often called is called the feature space right so the idea is there's some embedding of the data instead of thousands of pictures maybe there's some directions i can use are of them that is much smaller and that allows me to do a low rank embedding now you already played with this in one of the previous homeworks when you were playing with the yale faces and yale faces right you do the svd and you find there's a smaller subspace there if you represent these faces in this in the u's in fact we're going to do this later show that this is just basically svd essentially right if you represent it in the u then in fact what you have is a low rank embedding of this high dimensional data so that's the picture okay the question is how do i achieve this if you were going to do this with the svd it means you're going to take that massive matrix and do something like order m cubed operations for you to produce that decomposition and that's exactly what you can't do when you have matrices that are so large can't even fit them in memory so even though we know what we want to do there's just a practical limitation of what kind of algorithm could actually do something like this given this doesn't even fit in memory okay so that's what we're going to go after so this is where these probabilistic approaches or randomized algorithms come into play and they are just these fantastic class of algorithms that have emerged partly to directly address this issue of how do i go to scale without trying to say well i uh you know i'm gonna just do iteration methods i'm gonna do something a little different here okay so hopefully that sets up our framing of what we want to do decomposition this is what we've been doing the whole class and now let's go about to figure out how to actually do it and the board is still bad okay all right so [Music] how are we going to actually achieve this there's a two-stage process that happens with these randomized alloys so first stage called stage a that's how we refer to it in the paper that i sent you the first stage is to basically say i want to find a way to do this rejection uh in fact and find some low dimensional representation of the column space of a okay so i have a lot of columns here n of them in fact and what i'd really like to do is find some way to do a a nice representation of that column space so the idea is to construct some matrix q and q itself is going to be uh some matrix okay that's going to be uh sorry m by k so it's it's the length of this matrix so if this matrix like this it's that length m but now instead of n of them there's going to be k columns okay so the idea is i like to trade out n columns where n could be this it can often times be very big down to k columns which is much smaller okay so that's my goal and by the way notice that i called it q and that's for a reason because it's going to give us it's going to get us right back to qr decomposition so i'm going to leave that on the table now i've already like foreshadowed the result that we're gonna look for that cue okay and then finally uh stage second stage stage two is if i can find in fact a good matrix q that does a good job of spanning the column space then what i could do is i could actually just say well construct a matrix b which is equal to q transpose a in other words take the a matrix and project it into that column space right so this is never an id if if this has exactly some fixed rank r you could exactly embed all the a into that space but the fact of the matter is especially when you have noisy data you this is going to be full rank right this is always if you notice from like yale fastest this is going to be a full rank matrix but the idea is to find some r-rank truncation in which you can say i'm going to go ahead and take a and embed it in some low dimensional rank r subspace okay and this here in fact let's just work with reels for now k by n okay so now when i do this i'm creating a smaller matrix b which is my approximation to the matrix a by basically projecting all columns of a onto q transpose remember q is an orthonormal basis set then i'm going to go ahead and say this orthonormal basis set spans the column space of a and there's k columns so if i take a and project it into each of the directions in q i'm going to now get a new matrix b which is much smaller than a in fact if i only take k columns and m is really big but k is something moderate then i i've actually basically taken this down to a very much much smaller uh matrix to be working with okay then the idea is to say okay once i have it here now i do my decomposition of b i start working directly with b and then outputs this much smaller matrix i now do things like the svd and that b i do the qr whatever i want to do i do it here in b whatever my decomposition i want to do we're going to specifically look at the svd i do it here in b and then after i've done that i can go back up to my original space so that is the philosophical perspective of what we're trying to do with this randomized algorithm okay two stage process find a column space embedding project into that column space do your stuff come back okay so a high-level way to think about this is through a figure  all right board sucks now didn't have that many problems with the board the beginning of the class but there it is again just have to push harder i guess so here's the idea you start off with a matrix a which is very big right and one possibility to do some kind of matrix decomposition is just just directly do it so like for instance you could do the spd and right and over here you have let's let's say we want to do the svd then you get some approximation of a by just directly doing this now this is something like an order m cubed operation if this was square it's like order m cube so this is very expensive and the idea is if you have really large data matrices you cannot take that pathway there's no computer big enough in the world that'll allow you to do that but i still want my low rank features okay so how would you do it so the idea is to transform this to a new matrix b which is an approximation to a and how do we do it well we just basically say okay do that right i project onto some column space with only k columns or r columns really maybe the rank of this thing project into this matrix b now do the svd and let's call this now some utilda sigma tilde feet tilde star so i've been working in this lower ranked subspace and now i come back how do you come back you multiply by q that's the idea i can't take this path directly to the decomposition but i can first project into a lower dimensional subspace which spans that column space then do my operation then bring it back and this pathway is actually computational computationally feasible okay and so part of what we're trying to do is figure out what's the best way then to get me from a to b than where i can do my work and come back because if i can figure out how to get in i just undo how i got in okay i got in with the q transpose multiply i can do here and multiply by q i can come back out so once i know how to get in here i know how to get out and then i can do my work here and then the question is we're going to focus in how do i get into that space okay all right so one last piece here what makes a good low rank embedding one thing that we want to do is to recall that if i have a matrix a what i can do is say all right so what i'm going to try to do is take a and project it into this q space suppose i have this matrix q and this is my uh orthonormal uh columns uh matrix that spans uh that spans the column space okay so i have that and so now the question is how do i evaluate how well this approximation does here's the thing you start to look at a minus q q transpose a norm now what is this object think about this by the way this is the same thing you would do with svd by the way how do you know you have a good approximation with your svd a low rank approximation you you you star a same thing what is this doing it's saying take a and when i do q transpose a i am taking a and i am projecting it into the column space of q okay i'm now in this low rank subspace when i multiply by q i come back into my original space so q transpose multiplied by a takes me to the low dimensional space right so if i have k columns of this thing then it's going to project me to a k dimensional space or if i want to truncate it some r rank r i could say project it into the r rank subspace i just take this times this it's going to now put me in an r dimensional subspace to come back up into my original high dimensional space multiplied by q so this operation here i want this ideally i'd love this to be zero if i have a perfect embedding in other words if q i didn't do any rank truncation q is just the column space of all the data then this here would be zero because all you do is take the data make an orthogonal basis project into it project back out there would be no loss of anything but here what we're trying to do is project into an r dimensional subspace come back out how well did i do how well this is my low rank approximation versus the actual data that is what i want to do this is exactly what they do in the svd to evaluate how well that approximation works okay so let's go about figuring out how we're gonna do this projection okay this is where randomization comes in so what we're going to do is what is q trying to do for us q is trying to span the column space of a okay so in some sense what i want to do then is i want to sample the column space so what i'd really like to do is figure out how can i sample the column space of a and sort of like let's say an efficient and not too expensive a way and one way to do that is to do what's called random projections so what a random projection is is i make up a vector let's call it vector omega and what i'm going to do with it is and let's call this omega j because i'm going to have many random projections so omega j is a vector that is in fact i'm going to multiply by a so this thing here is an n by 1 of random numbers okay so it's a vector that's going to basically have a random weight against every column and that's going to produce for me some vector y sub j so what i did is i just simply said i have a bunch of columns of a i can simply say go take a linear combination of a randomly chosen linear combination of a to produce some y okay so that's what i've done and that's uh so this y here is sort of a random sampling of all the columns and what i'm going to do is do a bunch of these so in other words j here is going to go from 1 to all the way there's a k so what i've done here is a bunch of random projections now here's something that's very important every vector omega j probability says with high probability the omega j are orthogonal to each other like omega 1 and omega 2 which are randomly chosen are going to be with high probability orthogonal i'm going to show that to you right now okay we're going to bust out some matlab everybody ready hold on your seats okay let me do a screen share uh desktop share boom i'm in matlab and my goal right now is to show you that in fact if i if i have these random matrices or random vectors that they're orthogonal to each other so let me make a matrix size let's go 10. so it's a vector of size 10. so this is not going to be very big but i'm going to just show you some stuff here and so what i want to do is make some vectors let's call them uh let's call omega is a collection of vectors rand n and let's make uh uh m by 10. so i'm going to make i want to make up what i've just done right there in that command is i've basically said i'm going to make uh i'm going to make 10 different vectors okay so 10 columns so the rows are m we're going to play with the number of rows so in other words it's a 10 dimensional system i'm going to take 10 different randomly made vectors column vectors and then i'm going to ask the question how orthogonal are they to each other well how do you know when vectors are orthogonal if you take their dot product if it's zero they're orthogonal okay and if you normalize it then if it's one they're exactly perpend they're exactly parallel to each other so what you're looking for in the dot product is for it to be zero so what i'm going to do is compute the inner product of this thing let's call it uh uh i don't know i don't know call it a matrix a is going to be omega and we'll go ahead and transpose that so i'm going to basically by transposing uh times omega so i'm taking the inner product of every single one of those vectors against each other so i've i've given you 10 random vectors and i asked the question how does each vector have an inner product with all the other vectors so that's what that a is and i'm getting a number if they were all orthogonal to each other then in fact a would be the identity matrix okay be just a diagonal all right so let's actually do the following let's do a p color of a so we're just going to look to see all right this thing should be diagonal but if if if some of these vectors are not orthogonal they're going to give me some non-zero score and i can look at it and that's what the p color a is going to give us so let's run it see what we get all right let's put a color bar on that and every time you do it's random so there you go this is what it looks like so along the diagonal by the way right you can see that there is kind of a like you know that's essentially the diagonal is the length of each vector but notice uh there's a lot of vectors here that are not zero like i don't know if you can see the color scale here but this is negative 10 all the way to 15. we could actually plot the absolute value maybe that would be a little bit cleaner boom there you go all right notice here so this is the diagonal and then these different colors are and so as it starts to get darker blue it starts to look uh that what the darker blue means is getting closer to zero but you can see that okay there's some bigger values but really a lot of stuff sits it's kind of uh it's not zero but notice the following we're going to work here with vectors that are very high dimensional so n is going to be big m is going to be very big so watch what happens as this m so i picked it to be 10. let's do 100. look at that this is telling you that look that diagonal sits there and all the other terms are starting to look like closer to zero and in fact as you make this fairly large these random vectors basically are orthogonal to each other so this is my statement that i just made if i look at this here and for a being m and n and they're both being large i can just simply pick random numbers and make a vector out of it pick another random vector make numbers out of it and those two vectors are going to be orthogonal and i'm just showing you this with this little piece of code okay in fact probability theory says if you make this big enough this will be exactly the identity look at that now it's ten thousand by ten thousand you look at this thing there's a diagonal and everybody else is zero these are orthogonal vectors okay so that's a nice feature this is the feature we're going to take advantage of in this calculation which is i can just simply randomly sample column space with different omega js and i just make them up randomly and guess what these are all orthogonal directions which is exactly what i want when i sample this i want to sort of sample in orthogonal directions to construct y's so the way to represent all these different js random sampling and y's is in the following way you can say like okay i'll make a matrix y is equal to a omega so this omega is all the columns are spanned by this the omega j's y sub j are their corresponding random projection and by the way there's a ton of really interesting work on random projections and there's this famous uh well-known uh results on random projections called the johnson linder strauss transform that's what normally what random projections is associated with and there's a lot of deep thera work around random projections that are kind of fantastic and this is what we're really capitalizing on is that i can just randomly sample column space to produce this new y okay now what i'm going to do with this y so i i randomly sample my matrix and notice when i do this i don't have to store all the data that's a really important thing when i do this if a doesn't even fit in my computer notice i can pull in parts of a when i do this computation and just keep track of adding things together and so a never has to be in my computer at any one time parts of a have to be in there but not all of a's directly okay and now here's what we're going to do i have y which is my i sampled my column space i have this this is supposed to be some okay i've randomly sampled all this column space so y is in some sense inheriting the structure of all that sampled column space and what i'm going to do with y is what you might have predicted which is to now do a qr decomposition so now y is just these you know vectors there's no there's no uh enforced structure about them being orthogonal in fact generically they're not orthogonal they're not unit length qr now gives me q which is an orthonormal basis set spanning the k dimensions here remember this is this is uh m by k now and so this here is n by k but it's the m by k i want this is the m by k whose columns are orthonormal okay so this is how we're going to do this this is the whole game is to say all right so i want to figure out a way to take a which is way too big and find a way to represent it in a lower dimensional space where then i can actually do some operations on it so now the idea here is that k right i've got k columns so this random sampling of column space is much less than n this is the key now i have this much smaller matrix i've chosen k appropriately and we'll talk about how to choose k k is usually picked a little bit bigger than the rank r if there's some actual rank r of the data or that you know approximately so you want to over sample that by a little bit so k is like r plus 10. in fact there's some really interesting results on this too there's a lot of interesting mathematics behind everything that i'm telling you here okay all right so that's the idea so now let's come back and talk about this in the context of the svd are we good with this this is where the randomization all happens that's how i'm getting by by random sampling with orthogonal vectors that it just that i don't have to compute i just random choose them random sample column space qr it i now have the first incr important step of this algorithm which is i can produce a q which is going to allow me to take the data matrix and embed it in the slower dimensional subspace that's really critical that's the key piece of this whole algorithm is to find that and now we'll talk about what we can do with that okay so i want to now move on to talking about uh the svd and it's going to be essentially what i'm doing with this randomized linear algebra is a it's a series of three lectures okay so i'm going to do three lectures on this what i'm going to present right now is the comic book version another is the pictorial version of what we're going to do uh what we're going to do in class on monday is we're actually going to go and program matlab we're going to just walk through and start taking data doing this randomized reduction showing where it works where it doesn't work and seeing how this actually works in practice and then the following lecture we'll be talking a little bit about some of the details or theorems surrounding these randomized algorithms in other words this isn't just stuff that people have made up we have bounds we have good understanding there's deep theoretical results associated with this so i want to put some of that in place and that will finish our lectures on randomized linear algebra and then we'll move on to tensor decompositions to finish off the class okay so what you're going to see now then is pictures but my hope is that if you understand these pictures you kind of have everything this is the this is gonna so the pictures are are are really kind of important to understand okay i'm going to take a data matrix a so that's where we're starting and this is m by n and remember the idea is that this data matrix is massive like i said you don't have to go far to find massive data sets even if you just had video frames from 4k video or 8k video 4k video this will produce a row space that's 24 million rows 8k is even more right so you're in a 50 million range okay so this is massive and if you're on a video frame maybe you have uh maybe you have thousands tens of thousands of of columns here okay so that's the kind of data we're talking about these algorithms aren't used on small matrices you don't need to on smaller matrices you just put it on your computer and your laptop and you go okay here it's like i can't even fit this in memory it also could be that it's just very large and takes a long time if i know i'm looking for low rank features and there's low rank features in the data actually the randomized linear algebra algorithm is just a much faster way for you to approximate the low rank feature space so what do i want to do with this what did i want to do with it well i decided to multiply by that this by some random projections so here's my random projection matrix this was this omega and the omega was n and it had k samples so k random projections of a so this is the first step take your data matrix multiply it by some random projections notice i never have to bring this into memory i only have to bring in you know when i do the multiplication i just have to bring in well this times this this times this i can bring in row by row or i can take the first row in multiply by each column here then bring in the next row so i can all i i only have to bring in pieces of a to do this computation a does not have to sit physically in memory completely now i bring that up because we haven't really talked about that right we just made all these assumptions like i can compute these things of course i can fit it on my computer the idea behind this is like you sure you can fit in on your computer but if you can't well this algorithm allows you to work with data of such volume and size that you don't need to actually put it all in memory remember when i do this multiplication what you end up with is a new matrix y and this matrix y is m by k so obviously this is not exact i mean you know k here is supposed to be much much smaller than n so maybe this is ten thousand samples hundred thousand samples and k maybe is fifty right so this would be massively fat and this thing here is really small okay but you know i can't do that on the board very easily so you just have to play make believe here so k is much smaller than n okay so that's step one take your data random say sample column space next step qr so now you're going to take this q this y and you're going to qr decompose it and you're going to make sure to do the economy remember there's a the qr decomposition can take this thing and build you a full a queue which is in fact would be m by m you do not want to do that what you want to do is just make yourself an economy spd at qr which is going to be q is going to be m by k times r and that's just upper triangular and that's k by k okay so there there's your qr algorithm on an economy scale you just simply go through and you say take the first column normalize it that's the first column here take the second column right we we learned how to do this right you can do this with household triangularization and then boom pops yourself over here and now you have the q the q this is your orthonormal basis spanning this randomly sampled column space of a okay so that's what we did this is stage a find your subspace stage b let's go do some decompositions now well how do i do that well what i've got to do now is now that i have the subspace i want to take my data matrix a and project it into the subspace well how do you do that well you multiply q transpose times a remember this is m by n and this is k by m so you basically are taking a and projecting every column onto q okay so it's going to tell you how does a look when you embed it in this subspace which is k-dimensional versus n-dimensional there you go there's the computation and we're going to call this matrix here b and this thing here is k by n [Music] okay this is the matrix we were after because now we're ready for stage b i took the matrix a i found the embedding i embedded it now i have b so b's ready to go look how small b is it's k by n okay versus what i had before which is m and remember that k is also much smaller than m okay so i have this matrix what do you do now well whatever you want to do and let's talk about what we want to do here we're going to do the svd so if you do the svd of this now you're going to get a decomposition u sigma v let's call it u tilde sigma tilde v tilde there's your uh singular value decomposition remember this is a diagonal matrix but this is now an orthonormal or a unitary transformation right that that's actually telling you essentially looking at the correlative features among those columns in this new reduce space and v tells you how every single column projects onto these feature spaces so these are your like pca modes or svd modes right so when we did eigen faces or we looked at the yellow faces right we were looking at the dominant features they're sitting right there but this thing here is now k by k this is critical so this is where you you went from looking at a feature space if you did this svd right you'd have a feature space which is m dimensional right let's say in pixel spaces is 24 million dimensions whereas if i compress it down to let's say a hundred like i said k was 100 then i'm looking in 100 dimensions versus 24 million dimensions and so this is a trivial computation i can do it on my laptop and now i do the svd and then if i want to go back up how do i go back up there all i got to do to go back up there to find actual u u itself is equal to q util that's how i project back up and what does it look like in actual pixel space let's say then you take each feature here multiply by q that takes you back up into the higher dimensional space and that's equal to u so that is a really important picture to keep in mind and where the randomization comes is right here random projections to produce this to start the process office this is how i'm going to find a good embedding by randomly and orthogonally sampling column space to build y and from y everything proceeds right because then once i get y i qr it i project the data into that same basis then i do the svd and then i can come back out if you don't get a good y here you know none of this will be very good either okay so this is really counting on this matrix being able to produce a reasonable column space approximation to the actual data so that is in some sense randomized linear algebra in a nutshell and this is specifically towards the svd which we will basically start doing some examples on in in the next class you", "8GGBVehqNcY": "brand-new kitchen it sucks see it's sexy it's new avoid costly mistakes check our latest expert reviews from fridges to financial services which KOAT UK", "nM-6o8bgHRQ": "great so yeah the title of this is sketching for linear algebra basics of dimensionality reduction and count sketch so I'm going to continue talking about dimensionality reduction that Ravi started I'm gonna focus on linear algebraic applications like regression low-rank approximation and variance of these problems so the outline of the talk is I'm first going to define the notion of a subspace embedding in the context of its usage for least squares regression give different examples of subspace embeddings they don't discuss different ways so subspace embeddings can be viewed as a kind of sketch of your input or sort of dimensionality reduction of your input and let's describe other ways of using dimensionality reduction in terms of high-precision regression low-rank approximation and I'll try to survey a number of other problems where this technique is useful so okay let me just first define regression I think Ravi already mentioned this but I'm gonna focus on layer linear regression which is just a statistical method to study linear dependencies between variables in the presence of noise okay so here's an example Ohm's law as the voltage is the resistance times the current and you have a bunch of current voltage points and you're trying to find this unknown line which fits them best fits them in some sense so here's just the unknown slope or the resistance and now there are many different notions of what it can mean to best fit something oftentimes there's no line that goes through all the points I mean you have noisy data etc okay so just to set up some notation in the standard setting we typically have one measured variable B this was the voltage on the previous slide and we have some number of predictor variables a 1 through ad okay so D was equal to 1 on the last slide and a 1 with just the current and the assumption is that these variables are linearly related up to some noise so we have the relationship that B is equal to some unknown X 0 plus a 1 X 1 plus a DX D plus some noise epsilon and so we're trying to learn these values X I which are sometimes called the model parameters and we don't want to make assumptions about this noise epsilon if we don't have to right if we can get more robust algorithms that work for you know arbitrary arbitrary noise settings this would be ideal via standard transformation you can assume that this X 0 term is equal to 0 so to do that what one thing you can do is you can replace D with D plus 1 and you can have imagined an a0 term here which is always equal to 1 so let's just forget about x0 and we don't just get one observation in regression we had all these points in the Ohm's law plot we get n observations so let's write our problem in matrix form so we can think of being given an N by D matrix a the rows correspond to examples or observations and D is the number of unknowns we're also given this n by 1 vector B of observations and the regression problem just asks to find this D dimensional vector X star so that a times X star a is n by D so this will be an N by 1 vector a times X star and B are closed under some notion of closeness okay we're going to consider the over constrained case when there are many more examples than unknowns ok and it's much larger than D a is a tall thin matrix so yeah what does it mean to you know find the line that best fits the data so let's first look at the least squares method so here what you want to do is just find this vector X so that the Euclidean distance between a X and B is minimized or equivalently the squared distance okay so this is the sum of over the observations of what you observe - you can think of as what you predict okay the inner product of the I throw of a and your chosen vector X squared okay so this has certain desirable statistical properties for example if this noise epsilon being added to each of your observations were iid Gaussian noise then actually finding the X that minimizes this objective function which would give you the maximum likelihood estimator okay it also has a nice geometric interpretation right so the least squares objective function we can write a times X as you know a linear combination of the columns of a right so as I range over all X I get all the Nira combinations of the D columns of a I can think of this as a d dimensional subspace of R to the N so DS I'm considering much smaller than n this is a low dimensional subspace if R to the N okay and and so really the problem here of these squares regression is just to find the point in the column span of a which is closest to B in Euclidean norm right so it's just the projection of B onto the column span of a another nice property about these squares is that there's a nice closed form solution so we can solve these squares via this so-called normal equations so how to find the solution well let's look at the least squares objective function or equivalently this square so let's write our vector B of observations as a times X prime for some X prime plus B prime where B prime is orthogonal to the span of the columns of a then the cost by the Pythagorean theorem is just the squared cost of ax minus ax prime Plus this the squared norm of B sorry of B Prime and so for any solution X that you might choose you know any X you choose you're going to pay this projection distance the norm of B Prime so really what you want to do is you know just make this first term equal to zero and so X is optimal if and only if it satisfies the so-called normal equations okay so what this means is so let's look at a transpose times ax minus B and I can substitute in this expression that I have for B and so I'll get ax minus a X prime minus B Prime and we know that B prime is chosen to be orthogonal to the columns of a so orthogonal to the rows of a transpose so this is just equal to a transpose times ax minus ax prime and we know that if we're optimal that this ax minus ax prime this norm should be zero okay so ax minus ax prime is zero and we get zero and conversely if it's not zero if ax minus ax prime is not 0 since it's inside the column span of a you know it has a non zero dot product with one of the rows of a transpose and this would be nonzero okay so the normal equation is just that a transpose ax equals a transpose B equipment Li you can think of this is looking at the gradient and sending it to be zero okay so that's the normal equation and the nice thing is if the columns of a are linearly independent then I can just write down a closed-form solution for X all right it's a transpose a inverse times a transpose B so the problem with this is you know that I'll get two more is with computation as so as Ravi was discussing you know computing a transpose a is expensive for large matrices okay so naively this takes n d squared time you can do a bit better with fast matrix multiplication but we want to do much faster ok just one thing to note here is if the columns of a are not linearly independent then there's more than one solution there's a null space but you can write down at least the solution with minimum norm okay and this is via the so-called more penrose pseudoinverse which I don't which I think Robbie mentioned in a response to a question but I don't think it was formally defined so let me just say it so we call the SVD that Robbie mentioned that every matrix a can be written as u Sigma V transpose U as orthonormal columns Sigma is you know now negative diagonal and I'm non increasing as I go down and V transpose has orthonormal rows then the more penrose pseudoinverse a superscript this minus is just V Sigma inverse u transpose where what is Sigma inverse was the diagonal matrix for the I ignore entry is you know 1 over Sigma III if Sigma I is better than 0 and otherwise it's just equal to 0 ok so yeah so if you if you write X to be the pseudo inverse of a times B then this is a solution with optimal costs with minimum North ok now computing the SVD also takes know nd squared time or you can use fast matrix multiplication for that as well to get a bit better but we'd like to do a lot faster than this ok any questions just feel free to stop me okay yeah so the time complexity just to put it on a slide is we need to compute this X equals a transpose a inverse a transpose B takes ND squared time or ND to the one point three seven six with fast matrix multiplication and we want a much faster running time so this will lead us into the technique that I mentioned of sketching or dimensionality reduction okay so sketching dissolves these squares let's first relax the problem to allow for outputting an approximate solution now what does that mean so we'll look at this from an objective function perspective so instead of outputting the optimal vector X let's output an X prime for which the Euclidean distance between a X Prime and B is at most one plus epsilon times the optimal cost that you would get say with the normal equations okay with high probability so here's the sketch in solve paradigm if you haven't seen it before so the idea is to choose a random matrix s from a certain family of random matrices I'll get into different families an important property about s is it's very wide in fat okay so it has a very small number K of rows typically much less than n and no often independent of n then the idea is to compute s times 8 which will squash down this tall n by d matrix into a very small K by D matrix you can think of this K as being roughly ordered D or D over epsilon squared all specified more precise than moment but roughly s times a is a small squarish looking matrix s times B instead of you know being n dimensional is now only K dimensional and now all you do is you have a black box reduction of the original least squares regression problem to a smaller instance of these squares regression where you've replaced a with s times a and B with s times B and now you small solve this small version of regression which can now be solved efficiently say with this closed form solution that I described independent of n you know if K does not depend on n so you can solve this in time which is like k times d squared if you think of K is being roughly D this is roughly D cube time okay so you know the goal is to choose families of matrices s for which this sketchin solve paradigm works any questions yes example the observations just picked up you know handful of those k of those and then solve the system this is going to be better what's yeah that's a good question so the quick the question is about random sampling can a uniformly sample a small set of or yes so uniform sampling will not give you the kind of relative error that that we want you can imagine say you know in general for regression you can imagine you know missing a very important example by just uniformly sample you could think about doing things like squared norm sampling like Ravi was discussing but that's also not going to give you relative error now you can imagine something where you have a matrix say this is your n by D n by 2 matrix and imagine all the rows have the same norm they all look the same but for one of the rows you have a 1 minus 1 okay and if you just sample by norms or sample uniform you will never find this one minus 1 but this one minus one might be very important for fitting your data I mean the the the the sample will have ranked 1 versus rank two in the two cases yeah so yeah set uniform sampling is not going to give us a relative error and neither will squared norm sampling any other questions okay okay so how to choose this matrix s yes a lots of families of random matrices work the first that might come to mind is a matrix of iid normal random variables so it turns out that if you choose s the entries in s to be iid and you choose each one to be a standard Gaussian there's a scalar factor I mean multiplied by a scalar then this works so the sketch in Sawle paradigm works if I choose the number of rows in this matrix s to be order D of R epsilon squared okay so I just replace a with s a B with SB for this matrix S I solve the small problem I'll get a 1 plus epsilon approximation relative error yeah okay let me say a little bit why and to do so let me introduce the notion of a subspace embedding okay so let K be order D over epsilon squared and let s be this matrix of iid normal random variables so as I said they're not standard normal the variance is 1 over K okay this is like this 1 over root K normalization factor that Robbie was mentioning ok so what we'd like is the following subspace embedding property which is for any fixed D dimensional subspace for example the column span of an n by D matrix a we have the following property for all vectors X and RD so with high probability simultaneously for all vectors X the Euclidean norm of s ax is about the same as the Euclidean norm of AX ok up to a 1 plus epsilon factor this is the definition of a subspace in many so notice the quantification that it's over all vectors simultaneously now what is the proof that a Gaussian matrix a matrix of iid normal random variables with this number of rows D over epsilon squared gives you a subspace embedding so this is via a net argument I'm going to you know I don't have any slides on this I'm just going to skip it but the rough idea is to take your D dimensional subspace the column span of a and to discretize it with a sufficiently fine net so to find a finite set of points roughly 2 to the D of these points in the subspace you find the set of points you show that this matrix S preserves the norm of all of those points and since those points are sufficiently fine for any other point in your subspace it's close enough to one of these points and so therefore this holds for all vectors in your subspace that's the rough idea so the size of the neck can actually be to the D you just need to choose the the distance between the points to be constant and then you can sort of if you argue that for all the net points you preserve them up to one plus Epsilon you can do like a chaining argument like I I preserve this I take my an arbitrary point I write it as this point and then I plus a difference and then I look at the difference and I approximate the difference with a net point as well and I look at the difference of that approximate with a net point and I can do this training kind of argument so it said so you save a log 1 over epsilon when you do that yeah yeah any other questions ok yeah so you know what does this have to do with regression you know what is the what why are we looking at subspace embeddings yeah so this was the definition of subspace embedding and two for the application to regression is the following so you know we call for regression that we want this 1 plus Epsilon approximate solution an objective function value so let's just it's a very you know direct application consider the subspace spanned by the D columns of a together with your vector B it's D plus 1 dimensional instead of D dimensional doesn't change any of the bounds asymptotically and then just apply this subspace embedding s to this D plus 1 dimensional space so we'll have that you know for all vectors why it's norm is preserved up to 1 plus or minus Epsilon so in particular if we look at you know a candidate solution to the regression problem it looks like a X minus B you know it's inside of this D plus 1 dimensional space that I'm calling L and it's norm is preserved up to 1 plus epsilon so all solutions have their value preserved up to 1 plus epsilon which means that I can just solve you know this smaller version of regression now where I've replaced a with Si and B with SB yeah and as I said so for normal random variable is the number of rows of s which I was calling K is D over epsilon squared so the time to solve this small problem is roughly d cubed over epsilon squared and you can accelerate that with fast matrix multiplication any other any questions on this you're saying so if for a specific as this whole and then what are the Union changed you know yeah so I won't address that we make no assumptions on a you give me an eight for any given eight I sampled this random matrix s and I have the property that I preserve you know the entire subspace the entire column span of a together with B so I preserve the entire regression problem with some good probability what is the probability for the case of gaussians this can be a two to the minus D yeah any other questions given SA and SB so where do we go from A and B to si anything usually okay so this is so we can take a we're given a and we're given B and we just sample random Gaussian matrix s we compute s times a and s times B we set up this regression problem we solve it the solution is d dimensional and we just plug it back in the original regression problem we save a lot between the computer it's a great question yeah so was that your question - yeah perfect great you guys are both saying the same thing yeah so this doesn't give us a savings at all right like I mean computing s times a you know s has even more rows than D we already said we could multiply by a transpose and solve it exactly in nd square time what are we doing now you know SSD over epsilon squared rows it's dense computing this product is taking even longer than nd squared time and we're just getting an approximate solution and with some probability right so we we haven't really gained anything but it's you know sort of a proof of concept and yeah so exactly this this this takes too long now the first improvement to this was given by Thomas Carlos who observed that you can choose s instead of being from n you know matrix with iid normal random variables you can choose it from a much more structured random family of matrices called subsampled randomized Hadamard transforms so what do these typically look like they typically can be factored into a product of three matrices pH and D right and the point about these factors is that they can all be applied very quickly to a vector okay so I just have one slide on this so Ken tomorrow will elaborate more on this and give a proof but let me just give you some intuition so what are these factors so D is just a random diagonal matrix so the entries on the diagonal are chosen independently each entry with probability 1/2 is chosen to be one and we probably have to chose them to be minus one H is just the fixed matrix they had a marred matrix so it's an orthonormal matrix where the entries are all bounded the nice thing about H is it's a dense matrix but even though it's dense there's a very fast matrix vector product associated with it okay which you can do recursively it's similar to like fast Fourier transform the entries of H they're not so important but you know they're all plus or minus 1 over root N and P well P is just a sampling matrix uniform sampling so when I apply to a vector I just uniformly sample a subset of coordinates in this context it's sampling a small subset of rows of H times D ok so D and H are both square matrices you know they preserve the length of an n-dimensional I mean so I have an N dimensional vector it's still N dimensional and P is going to the number of rows of P tells me how many rows I have in my sketch and the number of rows in P can be chosen to again be about V over epsilon squared like in the case for gaussians but there are additional log factors okay so importantly s times a can be computed in nd log n time using this fast Hadamard transform okay the other things these order n time P is you know even sub linear time and you can show that this works that if P has roughly Deever epsilon squared rows you get a subspace embedding and the same kind of argument goes through and now using the connection to regression that I described you get a much faster algorithm instead of nd squared you get you know roughly nd log n yeah this can be improved a bit using these techniques like nd log D or so but it's you know roughly n times D plus you know polynomial in this small parameters like d log n over epsilon so this is nearly optimal in the matrix dimensions you know it takes you sort of you have an N by D matrix if it's dense you need to spend nd time otherwise you'll miss say some very large entry for example but you know in many cases your matrix yeah so the proof see Ken's talk in many cases your matrix a is actually a sparse matrix ok so you might want instead of spending nd time to spare time proportional to the number of non-zeros of your input matrix a okay so so okay so this leads to yet another random family of sketchy matrices these are called count sketch matrices these make matrices are called count sketch yeah this is originally due to Moses Chen and frog Colton so these were these originated in sort of a data stream context this is why they're called count sketch I think because kind of finding frequent items correct me if I'm wrong and it's also been used for things like feature feature hashing so what are these so these are again white fat matrices with more rows now so instead of just D over epsilon squared rows i have roughly d squared over epsilon squared rows still independent of this large dimension n the nice thing about these matrices s if you haven't seen them is they're extremely sparse ok so what they look like is they just have a single nonzero entry per column okay so how do i generate a matrix S for each column I choose uniformly a position on that position I put a 1 with probability 1/2 and the minus 1 was probably 1/2 I can do that independently for each of the columns but you can actually show that bounded independence suffice is for this as well they don't have to be fully independent so the nice thing here is that s times a can be computed in N and Z of a time where N and Z is the number of nonzero entries of a ok so why is that I mean for each column of a for each nonzero entry I look up the corresponding column in s and there's only a single nonzero entry there so I can update my matrix matrix product in constant time per nonzero entry ok and the nice thing is that this this actually works with these parameters d squared over epsilon squared being your number of rows and yeah so using the connection to regression you get an overall algorithm which runs in time number of nonzero entries of a plus a polynomial in over epsilon so I'm going to focus on this one and give you a a proof of why this one works the other two families of matrices I didn't give proofs for any questions so far yes this works with uh I thought maybe and I would assume that would be um one - Victor right yeah so four count sketch the property is not very high like what we'll see is they'll be a linear dependence on Delta here yeah unlike for gaussians where we had sort of exponential okay okay so here's a simple proof so yeah due to in in a paper of everyone knew in and I so what we what we had is what we want to do is show the following we'd like to show that the the norm of si X is 1 plus or minus epsilon the norm of AX for all X we just have the subspace embedding property we like this show the first observation is you can assume that the the columns of a or orthonormal now why is that well you're going to be proving this statement for all vectors X right so as I range over all X you know I'm ranging over all ax I'm ranging over all vectors in my column space it doesn't really matter which basis I range over so I might as well range over a basis where a has orthonormal columns okay there's the first observation the second observation is that this is a linear map right so I might as well just prove this statement for unit vectors X if I prove it for all unit vectors x and therefore ax is also a unit vector since a has orthonormal columns then it holds for all X by linearity so really I just need to prove that the norm of s ax is 1 plus or minus epsilon for all unit X and again after we've shown a subspace embedding guaranteed then we immediately get the application to regression just by applying it to a comma B okay so here's the the argument so let me instantiate the number of rows of s to be 6 d squared over Delta epsilon squared as I said it's roughly D over ups a d squared over epsilon squared ok so here is the sort of a key observation let's look at the operator norm of a transpose s transpose si minus the identity okay this is a D by D matrix this is a D by D identity and suppose just ignore this middle norm suppose we show that this operator norm is at most Epsilon then we're all set right why is that because then if I apply si to any vector X do I apply it X transpose X and the unit vector then this will become the norm of SI x squared X transpose X that's just going to become one it's a unit vector and so I'm saying the norm of SI x squared minus one is at most Epsilon that's the same thing as saying that you know for all vectors X the norm of Si X is 1 plus or minus Epsilon okay so this operator norm captures you know sort of all vectors that clear okay so it's hard to bound and operate a norm directly so instead of bounding the opportun or I'm gonna upper bound the operator norm by the Frobenius norm and I'm said I'm just going to show that the Frobenius norm of a transpose s transpose si minus the identity is at most Epsilon remember Frobenius norm just sum of squares of entries to the one half for being a storm is something that I understand much better right just sum of squares of entries so you know we have a form for s we can imagine just trying to compute this directly and that's what we do okay so what we will show below is that the maker the following matrix product result which is that you give me any two fixed matrices C and D if I sample one of these random count sketch matrices s then I can approximate the matrix product C times D so suppose you know C the number of columns of C is the same as the number of rows of D so that these can be multiplied instead of computing C times D I'm going to compute C times s transpose times s times D and what I'd like is this matrix product result which is that the norm of the difference of my approximation in the actual product or the squared norm at most you know something that's you know roughly one over the number of rows of my sketch s times the squared for brainiest norm of c times the squared for being a storm of d with some good probability okay so it's sort of the you know my approximation is getting better as i increase my number of rows of my skeptic matrix and it's going down sort of the error is going down linearly on that yeah so any any questions about this so this is what we're going to show but i just want to say that once we have this property this approximate matrix product we immediately get a proof of this statement this subspace embedding which I'll just give on the next line okay so here's the proof so all we do is if we have this matrix product result set C to be a transpose and D to be a I remember a has orthonormal columns so what is C times D it's just the identity and you know what do we get we get a transpose s transpose s times a minus the identity Frobenius norm squared is it most something that's exactly you know of the form that we would like to show a subspace embedding and so how many the question is just how many rows of s do I need to choose well what is the squared for vinius norm of a well it's a matrix with orthonormal columns right it has d columns so the squared four be nice form of that matrix is d the squared four be no storm of d which is just a transpose is also d so I get a d squared here and I just need to set my number of rows to be roughly d squared to cancel this d squared okay so more precisely if I set my number of rows to be six d squared over Delta epsilon squared these D Squared's cancel this Delta cancels and I'm left with an epsilon squared here and after taking square roots I get back this epsilon that I that I asked for yeah question no so actually for the count sketch matrix d squared is actually tight one way to look at that is to look at the identity matrix so one way of viewing the count sketch matrix is hashing rows into buckets and if you have fewer than d squared rows in your sketch then you'll get a collision and so your rank will go down from D to something like D minus 1 so it can't be a subspace embedding so actually this turns out to be a tight analysis for count sketch but you can apply the same analysis to those earlier sketching matrices like gaussians and subsample randomized Hadamard and for those it's not tight as we saw yeah other questions yeah anything with more than a square also work thank your computation yeah you can use more than d squared here yeah it's yeah it's still like nuns n NZ of a number of nonzero entries of a time to reduce it to this problem and then now note if you use more rows you have to spend more time on the smaller problem and by the way one other thing I did want to mention is that going back to Fred's question is you can get better bounds so if you have some assumptions on a for example if the so called leverage scores which we'll see later in this workshop if they're all small inside of your matrix a then actually count sketch can this can be shown to work with only roughly DeRose yeah but under no assumptions this is tight yeah question so in general you're saying that this cup of this extra factor of T is necessary if you want to have a sparse matrix sort of bound guarantee of n n CLA like a what like for accounts cash for one sparse yeah so there are very nice trade-offs of Nelson and New Eden which showed a trade-off between the sparsity and like the number of nonzero entries in each column of s and the the number of rows you need in your sketchy matrix so if I allow for like a hundred nonzero entries per column of s then I can reduce you know the number of rows of s times a to something like D to the one point one in general like if I allow like poly log D I can reduce this down to d poly log D rows I think the latest result is due to Michael Cohen on this yet any questions okay okay so you know it remains to complete this proof to give this matrix product result you know I just sort of claim this so why is it true yeah we want to show that the squared norm of C s transpose s D minus C D is small okay so to prove this so there many ways of proving this we're going to do a very slick way do to Cain and Nelson which is the following let's say a distribution on matrices s has the JL property Johnson installs property more precisely has the epsilon Delta L JL moment property if for any given vector X with unit norm we have the following bound that the expectation over a random matrix s from this distribution of the elf moment of the squared norm of s X minus 1 is at most epsilon to the L times delta ok so this is giving us some control over the higher order moments of you know length preservation so you know similar to so Ravi was talking about some of this earlier so yeah so it has epsilon Delta L there moment property at the elf moment of s x squared minus 1 this is small and so this is just a statement about distributions of matrices applied to vectors X and we'd like to go from the statement to this approximate matrix product statement that I did that I said so the the transition is the following so let D be a distribution on matrices s with K rows and n columns that has the epsilon Delta L JL moment property for some L at least two then for any fixed matrices a and B with n rows so you can multiply them you get this approximate matrix product guarantee that namely the probability over s that the norm of your approximate matrix product a transpose s transpose s B minus a transpose B there probably that it's large is small okay so it's the same number of roads that you use in your Jo moment property and you just directly apply it to matrices and you get the statement about matrices yeah yeah these are the same statement I've just written it slightly differently here so if you rearrange things here you know put less than here put greater than 1 minus Delta etc you'll get back the same statement yeah I'll mention that more later yeah any other questions that you started with B is a so it's more structures could we do I mean oh can you do better if like a and B or equal yeah I don't think so I mean not asymptotically because you can kind of yeah I mean maybe look at a block matrix where a is inside and B is inside and so I'm not sure I don't think you can you can't do better asymptotically you might be able to do better in constant factors yeah any other questions okay will be another this treatment there we both inner products of uneconomic producing from vectors - yeah so this you know this product is really a collection of inner products you know between the rows of a transpose and and B and the columns of B and that's you know how we're going to prove this so yeah that's a different way of saying okay so let's try to prove you know first we're going to prove this transition statement that from we get this result from vectors and matrices and then we'll show later that the count sketch matrix has the JL moment property okay so how do we prove this okay so let's set up some notation for a random variable X a scalar random variable let's define the P norm I didn't I didn't say why it's a norm yet the P norm of X to be the expectation of the absolute value of x to the P raised to the 1 over P and you know how are we going to apply this well in our proof sometimes this scalar X will be the Frobenius norm of a random matrix T for example okay so right so and then you would just get that the P norm of the Frobenius norm of T is the expected value of the Frobenius norm of T to the P raised to the 1 over P okay now you can show that this P norm is actually a norm for any P at least one okay so it you know it's zero at zero the main thing to show here is the triangle inequality and the triangle inequality turns out just to be Minkowski inequality okay that the the P noam of X plus y is at most the peano of X plus the p-norm of why this is Minkowski inequality and it basically comes down to holders inequality in the proof but I won't I won't prove that here okay so all right so let's let's look at unit vectors x and y and let's bound the l norm of the difference in inner products between the sketch you know sx sy that inner product and the actual inner product XY okay so this will be a useful primitive to have okay so let's just write out an expression for this difference well one way of writing this is I claim is the following so it's 1/2 times the squared norm of s X minus 1 plus the squared norm of sy minus 1 - the squared norm of X minus y minus the squared norm of X minus y what's going on here all that's going on here is I'm taking you know one of these expressions and I'm just expanding it okay so the squared norm of s X of s times X minus y is the squared norm of SX plus the squared norm of s y minus twice the inner product of SX and sy all right that's all that's happening so the squared norm of SX here will cancel this squared number of SX the squared norm of sy will cancel this squared norm of sy and I also do the expansion for the squared norm of X minus y so I'll get squared norm of X plus square norm of Y minus twice the inner product of x and y these are unit vectors so the square norm of X the squared norm of Y those are both 1 those will cancel these ones and so I'll get twice this difference and I have a 1/2 in front okay so that that's all I've done okay now I'm looking at the El norm of this quantity and what do we know about the El norm well we know it's a norm so in particular I can apply the triangle inequality so this is at most 1/2 times the El norm of you know SX squared minus 1 plus the Ella norm of s Y squared minus 1 plus the yelled norm of X minus y squared minus X minus 1 is with me okay okay and what do we know about each of these things here remember this transition statement we're trying to prove if s has the je a moment property then it satisfies something about approximate tricks product so what is the jail moment property tell us I mean it tells us how to bound each one of these terms right X is a unit vector Y is the unit vector X minus y is on the unit vector but I can just you know factor out the norm of X minus y so just directly applying the day a moment property I get that this L norm is at most epsilon times Delta to the one over L this L norm is at most epsilon times Delta the 1 over L and the last thing I get its at most the norm of X minus y squared times epsilon to the Delta times epsilon times Delta to the 1 over L ok now the norm of X minus y squared you know is that most for x and y are unit vectors so this is at most say 3 times epsilon times Delta to the 1 over L ok so this is just some statement about differences of inner products so by linearity this was for unit vectors but for the by the narrative for arbitrary x and y we get the same statement right so I can divide out by the norm of X and the norm of Y and I get that for arbitrary x and y you know this inner product divided by the norms now is it most say three epsilon Delta to the 1 over L ok clear okay good so now what why are we doing this so ultimately we want to say something about matrix product so now let's look at a matrix product and as Dara I mentioned this basically boils down to a bunch of inner products so let the columns of a be a 1 through ad and that the columns of B be B 1 through B e say let's define X IJ to be the following quantity it's 1 over the norm of AI times the norm of B J times the difference of you know s AI inner part s PJ minus AI inner product BJ why do we do that well that's exactly the entry in the matrix product a transpose B ok so what do we have here we have that this squared norm of this approximately Twix product that we care about is equal to the sum over all entries I and J of X IJ squared times the squared norm of AI times a square norm of B J ok so that was the point of setting it up like this is that clear really ok ok so now we just want to bound this quantity let's do that okay good so so what have we shown so far we've just shown this statement for arbitrary vectors x and y you know we have some bound on the difference of inner products over the norms and we've also set up this notation for these X I gaze and we showed our matrix product can be written in terms of those X IJ's okay so let's look at the following quantity and it'll be clear why we're looking at this in a second so I want to look at a norm you know so I'm gonna look at the L over two norm of the Frobenius norm or rather squared Frobenius norm of this matrix a transpose s transpose SP minus a transpose B and we just said that we can write this matrix using this expression in terms of the X IJ s okay so that's all I've done there I just substituted in the definition okay so what can we do with this expression that's people with me what should the first thing and what what is one thing we can do with this this expression so it's the L of our 2 norm of the sum of a bunch of stuff try anybody call you great thank you so by triangle inequality this is at most the sum you know of the norms of all that stuff so the sum over all I and J squared norm of AI squared oma be J times the L over 2 norm of X IJ squared right that's the only random variable there these other things are just scalars ok ok ok so now this next line all I'm doing is I'm playing with the definition of the L over 2 norm remember the yell over to norm of some quantity is like that quantity to the L over 2 you know the expectation of that quantity the expectation of the absolute dog of that quantity to the L over 2 raised to the 2 over L so this is just a definitional statement right because when I raise X I'd a squared to the L over 2 I get X IJ to the L to the 2 over L that's the same thing that's going on here it's just a definitional statement okay now what do we know from you know the statement that we have up here well the Exide gays have this form and this form is exactly what we know we can apply this jail moment property to right I mean if I look at what the X IJ is they have the form of a of a difference of inner products and I'm normalizing by the norms that's exactly what we can apply this statement that we had for arbitrary vectors x and y so just plugging in that statement we upper bound this squared L norm as as this value squared okay and I just factored that out and inside I have you know the sum of squares of you know a I squared sum over all I J of norm of AI squared oh my be J squared and what is this quantity yeah it's just a product of Frobenius norm squared for being a storm's so this is just three epsilon Delta to the 1 over l squared times the product gets squared for being in stores okay we're almost done we'll finish on this slide okay so why did we look at this quantity well we're just going to apply Markov bound on a higher order moment just like Ravi did in his talk so let's look at the expected value of the you know elf power of the Frobenius norm of our approximate matrix product a transpose s transpose s B minus a transpose B okay the claim is that this is just equal to this expression that we had up here raised to the L over tooth power okay that's again just a definitional statement okay so like what is the L over two norm of this well it's the expected value of this raised to the L over two so I'll get the elf power here which is exactly what's over here all raised to the 2 over L and that will cancel this L over two okay it's just a definitional statement okay but we just bounded this quantity here so now we're gonna apply Markov's inequality and complete the proof so the probability that the Frobenius norm of our difference exceeds some amount well I can raise both sides to the elf power right so I raised this side to the elf power I raised this side to the elf power and then I just apply Markov bound so it's at most you know the expectation of this quantity divided by this quantity to the elf power so I get exactly this this this expression and finally you know I just plug in the bound that we just proved for this expectation so we just showed that this expectation here is this quantity which is the L over toothpowder of this quantity right so you know so what happens here I raise this quantity to the L over 2 let's this look at epsilon for example so I get epsilon squared to the L over 2 I get epsilon to the L that cancels this epsilon to the L you know I get 3 to the L that cancels this 3 to the L etc right I get for being a storm of a to the L this is being raised to the L over 2 cancels for penis no more of a to the L all that stuff cancels I'm just left with Delta that's the whole argument okay so we showed that if we have the Dale property for vectors then we get the approximate matrix product property that we wanted so yeah so just recall this is the approximate to expect property that we wanted we started assuming that our matrix s is drawn from a distribution which has this JL moment property and we said well if we take the same matrix s with the same number of rows then it satisfies this approximate matrix product property yeah question so okay so we're going to show that this holds for l equals 2 you have to show it for some L at these two we're going to show this holds for l equals 2 yeah yeah this this is all all we're going to use so I think Jeremy Epson you have some work on this recently with Cowan and Devon using other values of L for these kinds of proofs linear in 1 over 10 yeah even the territory right you don't need iron yeah I mean so for us we're just going to get like one of our Delta dependents this is all that's going to matter and we're not going to use this kind of proof for the other families and matrices okay yeah okay so we just need to show to wrap up the proof that count sketch works for regression that it satisfies the JL property and bound the number K of rows as it has oh yeah oh let's see do we actually need this to be the most half I don't think this is I think this is arbitrary yeah similar they might have been hiding some constants but yeah any other questions okay okay so counts get satisfies the jail property this is just a you know a statement about applying count sketch to a vector you know we want to bound we're gonna prove this for l equals two so we just want to bound the expected value you know of the square number of s X minus 1 squared and this really can't be that hard right I mean this is just so let's just go through this very quickly so let's first is considered the expectation of the square norm of SX is it unbiased so we want this to be one right so a four-count sketch matrix s is convenient to parameterize it in terms of two things one is a hash function H which specifies for each column where the nonzero entry is okay so it's a hash function from the end columns to the K rows tell me where each nonzero entry is and there's also a sine function so for each column the sine function Sigma tells me the value on that nonzero entry is it 1 or is it minus 1 ok and let's just let Delta e be 1 if some probably stick event a holds otherwise Delta V is equal to 0 and let's just compute this expectation okay it's very straightforward so the expected squared norm of SX what's the sum over the K coordinates of SX of the squared value on that on the coordinates so what is the squared value on the coordinate well it's the sum over all indices I between 1 and n first I need an event that actually the the I column had its nonzero entry in the J throw so that there's an indicator that that happened if it did happen then I multiplied by the sine associated with that nonzero entry Sigma I times the actual value X I this is I'm computing the square norm I'm taking the square of this everybody okay so let's just expand the square on the next line so all I did now is i expanded this and I have a sum over all pairs of indices i1 and i2 between 1 & M and now I have this expectation you know of the product you know I have 2 of these things now so indicator that the I want coordinate hashes to the date bucket and the eye tooth coordinate hashes to the date bucket times Sigma AI 1 Sigma 2 times x i1 x i2 where I'm calling the Rose buckets this is just expanding the squared and now there's some observation here so suppose that I 1 is not equal to I 2 then what is the expectation of this quantity here 0 right because these are random signs I 1 is not equal to I 2 this is at least pairwise independent the expectation is 0 so instead you just get a sum over all indices I of the expectation that this indicator is squared you know that the I entry gets hashed to the J's bucket times X I squared what is the expectation of this well what is this expectation it's just the probability that the ice coordinate you know the eye column has its nonzero entry in the Jayce row what is that probability just one over K the number of rows right so this is 1 over K and I'm summing up over K things each one of them I'm getting the sum of squares of all the entries so I get an unbiased estimator right the squared norm of SX is the squared norm of X ok so ok so so it's three o'clock now yeah why don't we take a break for half an hour and I'll you know when we come back we'll conclude with showing that count sketch has the data moment property and then we'll start looking at other problems [Applause]", "xVh6B7zhh88": "Welcome to the course on Scalable Data Science.\nMy name is Anirban. I am from IIT, Gandhinagar. Today\u2019s topic is going to be on random projections. So, before we start talking about random projections,\nlet us take a look at this a called the curse of dimensionality. Since you are taking the\ncourse ah, I do not need to sell the concept of large data to you or even the concept that\ndata is not only large most of the data that we collect has a very high dimensionality\nright. So, imagine for instance that you are creating\na classifier for individual web pages or for individual documents, right. So, the first\ntask at hand would be to represent a document or a webpage as a vector. And this vector\nthen is inherently very high dimensional. For instance, one trivial way this vector\nyou could obtain this vector is just by seeing, it is just by taking let us say the English\ndictionary, and then seeing which words are in the document or not.\nSo, the representation of a document is then this binary vector for instance, right whose\ndimension equals the number of words in let us say the English dictionary, right which\nitself is pretty large. And if it is a web page for instance, you might need to add in\nhtml tokens, you might need to add in by grams or trigrams which are basically tuples or\nmean pairs or tuples or tuples of words of consecutive words ok.\nSo, there is no question that most of the data that we face has dimensions that we do\nnot really I mean have much intuition about. For instance, the problem is that when data\nbecomes high dimensional a lot of the intuition that we bring from low dimensional sort of\npictures or images or intuitions completely breakdown right. And a let us see one simple\nexample of this right. So, imagine that you are generating data from a normal distribution\nok. So, if it was one dimensional, then you could\neasily draw a sort of a picture of this which is the standard bell curve right; so, which\nsays that; this would be the bell curve in one dimension ok. And if you look at this,\nyou can easily see that most of the data is really concentrated around the center mu ok.\nMost of the data lies very close to mu right, but now imagine that you were generating data\nnot in one dimension, but in some dimension d right.\nSo, what you are generating? So, what you start with is a center mu that is a point\nin R d. So, think of t as something pretty large right. But it will turn out that we\nwill see this the non-intuitive phenomenon that we talk about even for pretty small d\neven for d equal to 10, 20 etcetera ok. So now, we are generating a d dimensional vectors\nok. And we are generating them as per the following distribution.\nSo, each entry let us say that it is the ith entry x i is nothing but a normal variable\nwhose center is mu i which is the ith coordinate of the center mu, and whose variance is 1\nok. And x i's are independent of each other ok. So, one way to sort of write this distribution\nis to say that x comes from a multivariate normal distribution right. So, the center\nof this multivariate normal distribution is this vector at the point mu which is a d dimensional\nvector. And the covariance matrix of this happens to be I d right. Because the variance\nof each of the coordinates was 1 and the covariance of 2 different coordinates is 0. Therefore,\nthe entire covariance matrix turns out with identity matrix.\nSo now the question is, that if you look at this d dimensional space right, do most of\nthe data points lie very close to mu or not right. Because that that was what the intuition\nthat we took from the 1 dimensional case that most of the data points in the one dimensional\ncase was very close to mu right. And if you want to calculate this if you want to sort\nof see an intuition for this, right we need to let us try to calculate the distance x\nminus mu square right. So, for a so far a let us say let us say x is a sample from this\ndistribution, and I want to see how does this random variable behave this x minus mu square\nright. So, this you can imagine is then the random\nvariable that captures the distance between the data point generated according to this\ndistribution and the mu right. So, if so, if most data points are close to mu this particular\nrandom variable that we wrote the x minus mu square right, should be pretty small, and\nif the if x is more or less close to mu, but is it the case. So, you see this let us try\nto calculate the expectation of this random variable ok. So, this is a scalar quantity\nof course, because there is a distance. So now, because x i x has x has x is of multivariate\nnormal distribution whose center is mu, x minus mu is a multiple is also multivariate\nnormal distribution whose center is 0, and the covariance matrix remains the same all\nright. So, the center is 0 the covariance matrix is id; which means that if I write\ndown if I write this down. So, I have just used the definition of the other l 2 norm.\nSo, the l 2 norm between x and mu, the square of that is nothing but the sum over all I\nequal one to the x i minus mu i square right. And because x i minus mu i by the definition\nthat we have given out here, x i minus mu i again is a random variable, now with expectation\n0 and with variance 1. Therefore, this is nothing but the sum of the variances right\nwhich is d because each of them is variance one right. So, what that means is that, and\nit is not really hard to show that not only is the expectation d right. The variable the\nrandom variable x minus mu squared is actually pretty close to it is expectation; which means\nthat what we will see is x minus mu 2 norm will be more or less square root d plus lower\norder terms or the smaller square root d ok. So, here we got d because we were taking the\nsquare of the distance, and when we do away with the square we get a square root d right.\nSo, what does the statement mean? That the x minus that that which that for most of the\ndata points, which means that with high probability, over the choice of x right, x minus mu 2 norm\nof that will be more or less square root d plus small order of square root d. What does\nthat mean geometrically? Geometrically what it means is that, if you look at the ball\nof radius d around mu right, most of the data points will be on the surface of this ball\nright and on a very small annulus around it. And the width of the annulus is small of square\nroot d; it is actually something like d to the one 4th or something ok.\nSo, this is already pretty counterintuitive right, because if most of the ball is hollow,\nif you if you imagine that most of the ball is mu lies at the center of the ball, most\nof the ball is hollow and almost all the data points lie on something that is very close\nto the surface of the ball right. That is not what we were expecting if we when we look\nat the one dimensional plane right. So, since I mean and phenomena like this are\nfairly common right. So, the point that I am trying to make here is that, dealing with\nhigh dimension is a non-trivial thing. And it is not just a question of our intuitions\nbreaking down. There are a number of other issues. For instance,\nif you have, if you I mean if you are trying to build a model for instance right. And the\nnumber of features is something that is close to the number of examples that you have, then\nyour model is bound over fit unless you very strongly regularize right. So, and one method\nof regularization is essentially cutting down the number of dimensions. And this is what\nwe will try to do right. So, our basic intuition that we follow, that\nwe will follow is that if the number of dimensions is too large for me to handle. And we have\nto learn a model or solve an optimization problem; which depends on the number of dimensions.\nThen one of the very common tricks in machine learning is to reduce the number of dimensions\nto a manageable quantity, something like k we will we will typically call it k which\nis going to be much less than the number of original dimensions that were presented with.\nAnd then we will work with the data that is a dimension k ok. And this thing will come\ncomes over and over and over back again in machine learning right. For instance, this\nis also and this is known by various names known as finding out the intrinsic dimensionality\nof the data, I mean being able to do feature selection effectively and so on and so forth\nright. And depending on the particular variant that you are interested in there are different\nways of actually formalizing how to do this reduction.\nYou have seen singular value decomposition before or principal component analysis before\nthat is one way of doing this. You have also seen factor and you might have also seen factor\nanalysis before only or independent component analysis or feature selection. These are all\ndifferent formalizations of the same theme that we want to reduce the number of dimensions\nand d of the data. So, random projection is a unique one among\nthem. And it is unique in the sense that it is possibly the only one that is data oblivious.\nSo, we have 2 properties. Number 1, is that we will have a, we will make the dimension\nreduction data oblivious in some sense. That is how we do the dimension reduction is completely\noblivious to the data set represented with. And secondly, we will make sure that we have\nvery nice very tight interesting guarantees on what we can say are on the structure of\nthe original data versus a structure on the reduced data ok. So, what does this guarantee? This guarantee\nis going to be as follows, that suppose we have points x 1 x 2 x n in R d which are Euclidean\nspace. And suppose I want a representation of these points as x 1 prime x 2 prime x n\nprime where x i prime is the is a representation of the of the point x i. And they should lie\nin some dimensional space R k. Because I started with Euclidean distance I let us say we also\nwant to preserve we are we also want the distance function of this of this target space to be\nalso Euclidean, and we also want k to be much smaller than d ok.\nSo, the guarantee the kind of guarantee that we want is that the distance between x i and\nx I, x i and x j right. Should be more or less equal to the distance between x i prime\nand x j prime. And this should ideally hold for every pair ok, but it means is that imagine\nthat you started with a with a kind of with a kind of tetrahedron in space right. And\nyou want to reduce it to much smaller number of dimensions right.\nAnd you want to make sure that the distances between all the points are preserved. Right\nnow it is easy for it was it is not too hard for you to see that if you start with a tetrahedron\nin 3 dimension and you want to reduce it to 2-dimension right. And want to preserve the\nexact distances that are not possible right; because you have to squish something or the\nother right. So, this thing so, this preservation can only\nbe done approximately. So, what is this notion of approximation? And how does that help? So, very interestingly, this something like\nthis is fairly easy to do in a particular sense right. And this was found by and this\nconstruction originally came from 2 mathematicians William Johnson and Durham Lindenstrauss in\nthis lemma that they gave ah, it is called a lemma in about 1984 ok. What it says is\nthat such a construction not only does such a construction exist; you can actually find\na linear mapping that achieves. This linear mapping is the same as a matrix right.\nSo, what it says is that that suppose you are you fix 2 parameters, let us say you fix\nlet us say at this point, let us forget the delta. Let us fix a parameter epsilon to think\nof epsilon as my error tolerance; as in how much how approximately do I want to maintain\nthe original distances. Suppose epsilon is greater than say something greater than 0,\nand you start with the k that is c which is going to be remember k is going to be my size\nof the target my target dimension. So, k should at least be something like c by epsilon square\nlog n. We will decode slowly how that came about.\nThe result by Johnson and Lindenstrauss says is, that there exists a linear mapping A such\nthat for all pairs i j right, for all pairs ij the distance the l 2 distance between Ax\nin Ax j lies within A 1 plus minus epsilon guarantee.\nNow, 1 plus minus epsilon factor of the l 2 distance between x i an original x i and\nx j; so, supposing this was the original distance x i minus x j. This is the value of this is\nthe real line. So, the distance between Ax i and Ax j right and this is 1 minus epsilon\nx i minus x j and this is 1 plus epsilon x i minus x j right. So, what it says is that?\nThe distance between x i x j the projected points lies in this in this interval, and\nthis happens for all pairs ij ok. So, we can actually show that, if I choose\nthis linear mapping if this is possible to choose this mapping A in a random manner,\nsuch that this actually happens with a high probable probability. So, what that means,\nis that not only is it not only does it exist such a mapping, it is also easy to find such\na mapping right. Because once we give a random construction for it for specific types of\nrandom constructions that we will see right, that we will be able to guarantee that this\nparticular property holds with high probability ok. So, this will give a randomized algorithm\nfor this for this particular problem, for this particular matrix construction ok.\nSo, the constant c that you see up here that I had forgotten to write is in practice fairly\nsmall right. It is sometime, I mean in something like 6 or 7 is what you need to prove in theorems.\nAnd in practice it comes out to be smaller than that. So, just to sort of harp a little bit on the\nintuition of this problem. Why are we interested about pairwise distances right? So, this is\nthe picture right, that we start with let us say 1 2 3 4 5 or even more dimensions in\nline in my original space. So, this is my original space, and these are the points this\nblue are the points right. For any of these points x, we multiply x by A right, and then\nwe look at; so, this is the image of the point Ax right. And this target dimension this target\ndimension k is much smaller than the is potentially much smaller than the dimension of the of\nthe original space right. In fact, it only depends on the error epsilon\nand on the number of points that you are interested in right. It does not depend on the on the\noriginal dimension right. What it says is you look at all the pairwise distances here,\nall the pairwise distances here. They are more or less maintained in my target dimension\nup to this factor of 1 plus minus epsilon. Actually maybe it should have been this point\nok. So, why is this important? This is important for instance imagine that the original points\nare clustered in some way. Then preserving the pairwise distances also implies that we\nare preserving a cluster structure. Similarly, if the original points are separable, right\nfor instance we had we have class 1 and we have class 2.\nAnd there is and they happen to be nicely separated from each other by some by some\nhyperplane right. Then preserving the pairwise distances also means that the resulting problem\nis also separable; which means that I can actually learn a classifier on this on this\ntarget dimension very easily ok. So, the important point I want you to notice\nthat the input dimension does not come into the bound at all right. Only the number of\npairs that we want to preserve or the number of points whose distance we want to preserve\ncomes into the picture. And we will see why I am talking about pairs right. So, if there are n choose 2 pairs right, then\nit should be log of n choose 2 which is basically the same as to log n right. That should be\nthat should come into my target dimension. And of course, there is the 1 over epsilon\nsquare factor; where epsilon is the error parameter. So, why is it useful? As I have already hinted\nthat it is potentially one of it is use and we will see very concretely a sort of example\nlater in the course is that it will help us learn classifiers very efficiently right.\nThat instead of learning classifiers in my original dimension very high dimension we\nwill do a projection and then we will learn the classifiers in this resulting dimension.\nWe will also see a lot of examples in what we in what is known as now known as randomized\nnumerical linear algebra right. And here for instance, we want to we have\na huge data set in which for instance we undergo matrix factorization. Why? Because maybe people\nrecommender system. We want to solve a regression problem, and in all of this a symmetric would\nbe to reduce the target dimension, solve the problem there, aprox and thereby get an approximate\nsolution in the original space with high confidence. So now if you go back we have seen a lot of\nstreaming algorithms. We have seen count min sketch we have seen count sketch and so on\nand so forth. If you go back and think about this, you will see that a number of these\nare essentially random projections right. You have also seen a bunch of algorithms of\nlocality sensitive hashing right. And we have also discussed the variations of kd tree or\nrp trees and so on. And a lot of these as we will see in today also in today\u2019s class\nand in next class, a lot of these again the core idea behind that is this distance preservation\nnotion, and then we are doing things on top of it right.\nThere is a huge literature in communications as well as in computer science called compress\nsensing. The idea there is that that suppose we have a signal, and we can we cannot observe\nthe signal directly, but we can observe it through it is interaction through some through\nsome interaction matrix; that is, given the signal x we cannot observe x directly, but\nwe can make observations of the form p times x; where p is called observation matrix right.\nAnd we might have assumptions that say that x is sparse right. And now the problem is\nthat given A y. So, x potentially is very, very large dimension, but is sparse small\nnumber of non 0\u2019s. By making a small number of observations y\nwhich is putting which is ideally only proportional to the number of non 0\u2019s of x right; if\nwe want to reconstruct x right. So, the number of observations should be should be a function\nof more the number of non 0\u2019s of x and the dimensionality of x ok. This is the compress\nsense in literature and this very intimate connection between compress sensing and random\nprojections ok. So, we talked a lot about why this is potentially\nuseful. But how do we create such a matrix right. So, here is a very easy way of creating\nmatrix a bunch of such algorithms have been given by researchers, we look at a easy way\nabout which you can actually prove that it works. So, the way we will create the matrix\nis imagine that we start with the empty matrix which is of size k by t, and we and we are\ngoing to fill this up. So, how do we fill this up? It is very simple, you go to the\nentry ij, ijth entry and in for filling up the ijth entry, you sample a random variable\nfrom N 0 1 basically from the standard Gaussian. You take that value and you set that as the\nvalue R ij and you keep on doing this for all the entries.\nSo, independently you sample k times d entries and you fill use that to fill up the matrix,\nthat is it ok. So, this is my random projection matrix. And so, one other thing we need to\ndo is that we need to normalize it by this factor 1 over square root k. And we will see\nand it is easy to see why right. So, if you notice what the expected norm of a row is;\nso, take the row the first row right. Each of these are our samples from N 0 1.\nAnd therefore, each of them have a variance 1, which means that expectation of x i square\nwhere x is the ith entry of row is 1 and therefore, the expected norm of the first row norm is\nequal to k right. And I want it to be 1, right. Because if it is going to act as a as a projection\nmatrix, right all if it is going to add as a projection matrix I need it I basically\nif it is going to act as a norm preservation matrix. I need the norm of each column each\ncolumn to be one not the row maybe I was talking about a row before. So, I need the norm of\neach column to be 1 right. So, therefore, I am dividing by 1 over square\nroot k ok. So, why does this work? In order to see why this works, we need to look at\nwe need to prove a smaller result first. And this is, but this is really the core of the\nof the of the theorem right. And this is going to JL lemma. So, what does he say?\nWhat it says is that that suppose we set k to be some at least c by epsilon square log\nof 1 over delta; where epsilon and delta are 2 non negative quantities. Then for the previous\nmatrix that I created let me call that A, and let us take any vector x that has 2 norm\n1, then Ax the probability that x lies in the interval 1 minus epsilon and 1 plus epsilon.\nSo, the 2 norm x of Ax lies in the probability that the 2 norm of x lies in the interval\n1 minus epsilon 1 plus epsilon is at least 1 minus delta ok.\nSo, see notice a couple of things. First of all, I am stating the theorem only for the\n2 norm of x equal to 1. And this is enough because this is a linear mapping because for\nany other x i can always normalize it by the by that norm of x right. And once I multiply\nby A, I can again multiply back by the norm and so on. So, having such a stating a theorem\nfor the unit on vectors is enough right. And also notice that if I can actually show this,\nif I can actually show this particular lemma right. Then in order to show the previous\nJohnson Lindenstrauss theorem, we needed to look at all pairs x i minus x j right, and\nthere are n choose 2 pairs. So, I needed to take union bound over all this space and I\nwould be done ok. So, what we will do in this class in today\u2019s\nin this lecture is just see the proof of this and then finish ok. So, why does this work?\nLet us just do a proof sketch a brief proof sketch of this that. Suppose we look at the vector Ax right. And\nthe vector ax is nothing but 1 over square root k r x which is nothing but 1 over square\nroot k, let me call the entries this is a k dimensional vector Y 1 to Y k and Y i is\nsummation j R ij ok. So, this is the definition of Y I. So, what we are interested in; is\nthe 2 norm of ax square which is 1 by k summation i Y i square. And we are interested in the\ndistribution of this. So, the thing that comes to rescue is something\nknown as the 2 stability of the normal distribution, which is as follows. That if p and q are 2\nnormal random variables and p happens to be from n mu sigma right; that is that is the\nmean is mu and the standard deviation is sigma. Q happens to be from n alpha gamma, the mean\nis alpha and the standard deviation is gamma. Then p plus q has mean alpha plus mu plus\nalpha the sum of the means. And the variance is sigma square plus gamma square which means\nstandard deviation is square root of that and furthermore it is a normal distribution.\nIt is normally distributed random variable with this with this particular mean and variance\nok. So, this is what we will use. So now, it is easy to see that each Y i is\nreally the sum of normal random variables. Because R ij x j are normal random variables.\nTherefore, R ij times x j\u2019s normal random variable and therefore, y I is also itself\na normal random variable with expectation to be the sum of the expectation and the variance\nto be the sum of the variances of this. It takes only a little bit of calculation to\nsee that the variance of summation R ij x j; is really summation of A j x j square,\nwhich is equal to 1; because I started with x j to be norm 1 ok. So, therefore, each y\nI is really distributed as N 0 1 right. And now, therefore, the expectation of this\nquantity 1 by k summation Y j by i square which is which I was defining to be z is also\none. Furthermore, I can also say that k times z is something like a chi squared distribution\nwith k degrees of freedom right; because the square of a normal distribution is a chi square\ndistribution. And beyond this we just need to apply chain\nof bounds. We want to show that probability of z bigger than 1 plus epsilon is small we\napply we apply the standard steps that we did in sort of showing chain of bounds. And\nit is again really a chain of style bound, but it is a little more complicated. Because\nit involves the moment generating function of the chi square distribution and some algebra\nto show that the property that z is bigger than 1 plus epsilon is that most exp of minus\nk epsilon square c ok. So, that is it really I mean, and the proof\nfor the lower tail bound is also the same. In next in next lecture we will discuss some\nother properties of the of the random projections ok.\nThank you.", "281sDWz-Qm0": "Welcome to the course on Scalable Data Science.\nToday's lecture is on Introduction to Randomized Numerical Linear Algebra. I am Anirban, I\nam from IIT Gandhinagar. So, numerical linear algebra ok; if you are\nfamiliar with the machine learning, you probably know that a large fraction of machine learning\nproblems essentially boil down to solving a linear algebraic optimization problem right.\nAny kind of model that you are learning right, whether it be a very simple logistic regression\nmodel or it be a very complicated deep neural network model, it is you are essentially solving\na problem an optimization problem in some linear algebra space.\nSo, in fact there are specific types of linear algebraic optimizations that are not to be\nvery useful in specific branches of machine learning. The entire branch of recommendation\nsystems right is really all about matrix factorization ok. Under certain constraints, under assuming\ncertain priors, by sort of redefining the last function, it is all about matrix factorization\nin various forms right. Ranking computing page rank is really just\nan eigenvector calculation right. You redefine the transition matrix in various forms, and\nthen you calculate the corresponding eigenvectors. And this is also true for specific kinds of\nlet us say community detection or clustering right. These are these all boil down to certain\ntypes of eigenvector calculations eigenvalue calculations right. And as I mentioned that\na law I mean basically any supervised model is doing regression with an appropriate loss\nit is regression with an appropriate loss function.\nSo, such matrices the matrices that, we sort of solve the linear algebra problems in are\ntypically big right, because of the I mean, and this is what the big of the big data really\ncomes in right. If you imagine that, you are solving a recommendation system at the Amazon\nprime or at the Amazon prime level or at the Netflix level, we have like millions of movies,\nand we have millions of users, and we have hundreds and thousands of movies at least\nmillions of users ok. So, you have a huge matrix, which is also very sparse.\nAnd furthermore these methods are also often computation expensive, you have seen SVD the\nsingular validity decomposition right that takes time order n d times n plus d, which\nis at least quadratic right. And, so I mean n d times in n d order n d times mean of n\nd right something like this ok, which is fairly large. And it is not really realistic for\nany for any practical data set. There in hundreds and thousands, and these are the of the order\nof millions let us say ok. So, the idea behind randomized numerical linear\nalgebra is this essential sort of intuition that, the concepts in ran the certain concepts\nin randomization for instance sampling being able to sample from the data, being able to\nsketch from the data allows us to build algorithms that, can be efficient much more efficient,\nand can actually scale to massive data. But in order to do so you have to give in certain\nthings. Number 1, you cannot hope for exact solutions\nright. Because an and I mean, because once you sample right I mean, and you are trying\nto solve the problem on the resulting data, you cannot hope for a solution that is exactly\nI mean the solution, that is exactly optimal for the original data right. You have to be\nyou have to deal with approximate solutions right. But what will give you is a specific\nquality control parameters, for instance, we will be able to tell you that ok, this\nis not more than 1 percent far off from the actual solution.\nWe will also tell you right I mean a confidence parameter in the sense, that given the algorithm\nright. We will be able to tell you ok, that this algorithm is supposed to succeed with\nonly 90 percent chance, which means the 10 percent of the cases I might return you something\nbad, but if it is a fact if it is I mean mostly in reality it is not going to be really bad\nright. It is going to be but the theoretical guarantee is only hold with the high confidence.\nAnd in a lot of sense is fine, because once you are in machine learning applications,\nit is not really I mean we are not really looking for the actual optimum in any way\nright, because I mean if it is a classifier, and we were doing I mean fine it is we do\nnot really need to get to an actual optimal ok. So, what are the basic thing in randomized\nnumerical linear algebra? The basic theme is that is a following that, we think about\nrepresenting the data as a matrix, and now we are doing operations on this matrix. Maybe\nwe are doing a matrix factorization, maybe we are sort of solving a regression problem\nusing this matrix, whatever right. What will see is that ok, we will sort of to choose\nspecific rows or specific columns of this matrix or maybe a combination of both. And\nthis will give me a smaller matrix somehow it will give me a smaller matrix right. And\nthen, we will sort of then we will solve the problem on the smaller matrix right.\nSo, we have to be able to solve it in a way, so that we can say that there is a resulting\nsolution is a good solution with a original problem ok. So, sometimes we live in of course,\nstep and said that ok. We are not even choosing individual rows and columns of the matrix.\nWhat we are doing is that, maybe we are sort of maybe we are summarizing a bunch of rows,\nusing a single row or maybe by summarizing a bunch of columns using a single column right.\nRating a random combinations and then, we are creating a new matrix. So, we are creating\na shorter matrix batch is sampling one by doing random combinations right. This is essentially\nwhat random projections just does right; if you remember, if you sort of think about it,\nso and then, we solve the problem on the resulting matrix ok. So, in this particular lecture, we will see\na very simple, but very sort of clever example of such a algorithm right. So, we have all\nseen the we have all seen in our basic algorithms course that, the problem of matrix multiplication\nright. We even do it before we done the algorithm course. For instance, that suppose we that\nsuppose, we are we given 2 matrices A which is of size m by n right and B which is of\nsize n by p, and we have to find out the product of the of these 2 matrices A times B right.\nSo, the naive algorithm takes time order m n p right, because just two I mean for every\nif you take every row of A, you take every column of B and that takes and that takes\norder n right. And you have to consider m times p such combinations right, because every\nrow goes with every column right. So, and thus no cheating around this, unless the matrices\nhave specific structure right. So, here is a question can I find, if we are not interested\nin the exact solution, but in an approximate solution, can I find an approximate solution\nfaster ok. So, for this we have to define that what do we mean by an approximate solution\nfirst of all. And Secondly, how can you even think about doing this faster right. And this\nis where our first introduction into using randomization will be ok. So, here is a title different way of looking\nat the of looking at the matrix multiplication, and that will come to use right. Remember\nuntil, now that we have been looking at matrix multiplication as sort of taking every row\nof A and choosing a column of B right and multiplying it to get an entry right. So,\nbasically A B i J is summation over k A i k B k j right. This is a row of A this is\nthe under odd product of the ith row with the j th column. So, but this view is not\nvery useful for us. The view that is useful for us is a little unintuitive, but once you\nget used to it is surprisingly useful right. What we see is that right that, we can also\nview the product A B as a sum of rank one matrices.\nSo, what are these rank one matrices? So suppose, so think of the columns of A right and think\nof the rows of B right, the columns A and the rows of B. So, now, if you think about\nit, right I mean A is nothing really, but A collection of its columns and B is a collection\nof its rows right. So, the dot product A B can also be written as you take the outer\nproduct of the first column of A, let me write that at A star 1 and the first row of B write\nas a 1 star. So, I am using MATLAB type notation right.\nSo, A star i will be the ith column and A i star will be the ith row. This is what MATLAB\nuses and I find this clear enough. So, I can write I can also write A B as the product\nof the as a sum of as a sum of rank 1 matrices each of them is formed by the outer product\nof a column of A and that row of B. So, this is A star 1 with B 1 star A star\n2 with B 2 star, and A star 3 with B 3 star, so basically as a sum of n rank one matrices.\nEach of them is an outer product of 2 of a row and of a column of n row of B ok. So,\nnow, our sampling idea comes into picture. Now, what we say is that, if I were calculating\nthis exactly, I would have to calculate the sum of n terms ok. Suppose I am not interested\nin calculating suppose I am interested only in approximating. It not interested in calculating\nit exactly. How about we do this, suppose we assign a probability to every into each\nof these terms right. Suppose p 1 is the probability of the first\none, p 2 is the probability of the second one p p 2 is a let us I mean let us consider\nthese weights for now. Weights are probabilities that ripping having assign p 3 is the probability\nthere that is going to assign the third one and so on and so forth. So, summation p i\nequals to 1. We will see how to define this p i\u2019s, but suppose you had this pi\u2019s,\nthen here is what I want to do right. So, I am going to sample times ok. And each time\nI sample I am going to pick one of these rank 1 matrices ok.\nSo, the so for each of the c samples we pick the ith rank 1 matrix with probability p i\nright with replacement. And then, you normalize the matrix with the lower p i right. So, now,\nI have a sum of c terms right. And then we basically just average that by 1 by c average\nthat, I mean divide that by c and this is my estimator right. So, basically this is\nmy this is my this is the estimate of of A times B, that I will return ok. So, we will sort of see a little more I mean\nexample of this, that supposing let us kind of go over it in a little bit details that,\nsupposing a was a 1, a 2, a 3, a 4, right. These are the columns of a and b was let us\nsay b 1, b 2 b 3, b 4 right. So, now actually let me stick to the same notation. So, let\nme write b as b 1 transpose, b 2 transpose, b 4 transpose. So, when I write I mean a i\na i that is a column that is a column vector right, which is what which is like when I\nam writing down A row of B I am I am writing it as b i transpose, this is a row vector\nok. So, this is now equal to summation a i b i transpose ok.\nThis is this is what it is basically a i is A star i and b is bi bi transpose is B i star\nok,. So, what is happening here what is happening here is, now is now suppose we are choosing\nthree terms c equal to 3 ok. Suppose, suppose we define p 1, p 2, p 3, p 4 right. So, now,\nwhat we will do is that for c equal to 1 right. I mean I will take a 4 headed dice right.\nSo, the 4 headed dice it will turn out to be it will sort of evaluate to, so the random\nvariable can take value 1 with probability p 1 with value it can take value 2 with probability\np 2, value 3 with probability p 3, value 4 with probability p 4. So, I try toss this\ndice right. And let us say that the index that comes up is 2. So, for c equal to 1 I\ntake a 2 b 2 transpose right. And I divide that by 1 by p 2.\nSo, I sort of do this index choosing again for c equal to 3 for c equal to 2 right. So,\nlet us say it turns out to be, now the random variable says is that I mean based on the\nprobabilities p 1, p 2, p 3, p 4, I will give you the index 1. So, I take a 1, b 1 transpose\nI normalize that, by p 1 by p 1 right. And let us say I do it one more time, and it again\nturns out to b 1 c equal to 3. So, b 1 b 1 transpose 1 by p 1 right. So, this is my final\nsum. So, this is what I returned, and I normalize this by 1 by c ok. This is my estimator of\na times b. So, it happened with replay with replacement, and then, I sort of did it c\ntimes. So, an equivalent way of looking at the algorithm\nis as follows right that from the matrix A and the matrix B we create 2 matrices C in\nR. So, what is C in R 2, I mean a column of C right is really picked from a column of\nA ok. So, think of the choice that we were doing right that for the for t equal to 1\nto c right. We were choosing an index from 1 to n with probability p 1 to p n right.\nSo, now that I mean what you can also says that that choice of index let us say for t\nequal to 1. We choose column number 10, we choose we choose\nwe choose i equal to 10, which means that you take the column number 10 of A A and the\nrow number 10 of B right. And put it in position 1 of C and position 1 of R. Put it as a first\ncolumn of C and the first row of R right. Similarly, for t equal to 2 right. If the\nrandom variable say is that you should be choosing column number 100, then you take\nthe hundredth column 100 of A, then you take the 100 column number 100 of A, and the column\nnumber and the role number 100 of B right. And then, and then put it in the in the second\ncolumn I mean put the put the column of A in the column of C, and put the row of B in\nthe row of R ok. So, basically and then and then, it is not very hard to see that the\nI mean and of course, do appropriate normalizations right. And we will see how will do the normalizations\nin a little bit more, but, but once you define C in R and you add in the normalization add\nin you do the scaling correctly right. And then, you sort of do a 1 by c right, then\nthe product C R is what we will is what we will sort of is what we have been looking\nat the, so the summation that we are looking at is really nothing but this product of the\n2 matrices t times R. Now, let us going little bit details about how C is being normalized\nR is being normalized and so on right. So, the choice 1 is, so first let us sort\nof see is that how do we choose the probabilities right sort of so here is the first obvious\nchoice. Then do not try to do anything smart right. Make all the p i is 1 by n right. So,\nthat means, that at that at every point for every for every for every t equal 1 to C,\nI mean you choose you I mean uniformly one of the columns of A, and the corresponding\nrow of B. So, remember that the choice in A and the choice in B have to be locked right.\nIn the sense that if you if you choose the ith column of A, you have to choose the ith\nrow of B we make these two choices together, and then you put it and then, you put it in\nput them in C in R ok. So, we will sort of see how to do this in\nlittle more details, now. So, what we really want right. So, what we want is that the product\nA B should approximately equal the product C R. And we will again define what do we mean\nby approximate right. So, before we even before we even sort of a go to matrices, let us imagine\nthat we had n numbers right. And just to keep sort of I mean similarity with the with a\nmatrix setting that we have sort of written, I call the first number to be to be a 1 1.\nSo, let us say x 1 equal to a 1 b 1, x 2 equal to a 2 b 2, and x n equal to a n b n right,\nI mean a n, b n is a single number. And we want to estimate the sum of this summation\na i b i. So, summation x i is equal to summation a i b i this is what we on estimate. Now,\nwe want to say is that that suppose we want to do a uniform choice right that uniformly\nchoose I mean so choose C numbers out of out of 1 to n right. And for each number choose 1 from 1 of the\nnumbers I mean x i, which is a i, b i. And then and then just normalize it by n multiplied\nI mean normalize it I mean basically just choose summation x i and then divide by c\nmultiplied by n ok, where x belongs to the i belongs to the sample. x the j th sample\nlet me write it this way the j th sample is j i for j belonging to the sample ok. So,\nnow, this is something that we have seen. So, how can I mean does any choice work well\nhere? Well, you could see that not really right, because it could happen that one of\nthe excise is very big that supposing one of the exercise is 1 and everything else is\nbasically 0. So, then summation a i, b i is 1 right, but because we are doing uniformly\nrandom choice, because you are not even looking at the values, we could be ending up we could\nend up choosing the 0 value over and over again. And therefore, what we will get is\nis 0 right. I mean at the end, because all our sample is going to be 0 right. And so\nand so that is it is not it is we can take this intuition you can formalize it and say\nthat choosing uniformly essentially has very high variance, because some of the a i, b\ni could be very large. So, what is the better way of choosing it.\nWell, at least in this case a better way of choosing it is according to the value right,\nbut if you happen to choose the I mean the ith value with the ith I mean, if you happen\nto choose i with probability that is proportional to x i right, then we will actually get a\ngood estimate of the sum right, that is if the probability is proportional to a i b i,\nthen we will get a good estimate of the sum. Now, we are starting to get intuition about\nwhat we should do in the matrix case right. So, in the matrix case right the right sort\nof probabilities happen to be happen to be this right, that you take the k so, A star\ni is really the l 2 norm of the of the of the ith column of A and B star b i star is\nthe l 2 norm of the ith row of p right. So, we define p i to be proportional to the product\nof the of the of the l 2 norm of i of the l 2 norm of A star i and the l 2 norm of B\nstar b i star right. And then this denominator is really is essentially just normalization\nright, because we because we normalize we have to ensure that that summation p i equal\nto 1 ok. And here is what we do right that. For t equal to 1 to C, we choose the jth column\nof A and jth row of B with probability p j t.\nAnd if I happen to choose this right, then I normalize it by this quantity A star j t\nis what my job A is what I choose. And then I normalize it by square root c times p j\nt. And then I sort of so I include this normalized column in C and then, I include this normalized\ncolumn in R right. So, why did the square root come in, because if you remember that,\nC in R, I will going to be multiplied to each other right. And therefore, and I mean I essentially\nwanted to normalize the sum only by 1 over p j t and then outside by 1 over C right,\nbut now, because I am distributing the normalization and putting it some in C and some in R right.\nI am normalizing each of them by the square root of the final normalizer. Basically, it\nis that simple ok. So, then so this is what it is right and then\nhere I mean here is either way of looking at it from the matrix notation right. What\nif what we could also say is that that let us create a sampling matrix, sampling matrix\nof size n by c. So, this sampling matrix what it will have it is of size n by c. What it\nwill have is that for every column, it has a single nonzero entry, and that nonzero entry\nwill turn out to be j t at the position j t t right. And it will have 1 by square roots\nC times p j t. So, basically what you could say is that in order to decide where to put\nthe nonzero. We have the probabilities we have the probability\nis p 1 to p n, in order to decide the where to put the nonzero entry in the in the in\nthe tth in the tth column right. You toss this n headed dice right, so that you get\ni with probability p i and if you get or rather, so that you get the j t with probability pjt.\nAnd if you happen to get j t right, then you sort of put in a normalizer of 1 over square\nroot put in 1 over the value, 1 over square root C times p j t in the in the position\nj t comma t right. And then put in zeros everywhere else in the in this particular column.\nSo, this is my matrix S, now again it is not very hard for you to solve sort of C is that\nthe C that, we defined in the algorithm before is really the matrix A times S. And R that\nwe defined is really the matrix S transpose times p. And therefore, we what we can also\nlook at is that we create this matrix S and then, we calculate we essentially calculate\nour project the 2 matrices A and B using this matrix S. We only have to make sure that the\nmatrix S is really the sampling a projection matrix is the same for both A and B. So, now what are some guarantees ok. So, let\nus look at the ijth entry of C R right. So, the ijth entry of C R right is really a sum\nright. So, it is really A sum of C terms right. And remember that for the tth term we had\nchosen the jth the jth column of A and jth row of B. So, therefore the tth term right\nwill really be a i j t times b j t j divided p j t right.\nAnd this the entire thing 1 get divided by 1 over c ok. So, this is and here j t are\nthe random variables of course, ok. So, it is easy to see that the expectation of C R\ni j is really a b i j right, because once we start taking I mean once we, because if\nI take the expectation of  expectation of C R i j right, because each\nof the t choices is i i d right I mean all these have the same expectation. And therefore,\nI could as well calculate the expectation of A i t A I j t and b j t j divided by p\nj t right. And then there is a 1 by c and then, there is a c times this and this c and\nthis c cancels out. So, this the second c comes, because the summation every term in\nthe summation has the same expectation ok. So, now what is this quantity let me just\ndo a sort of quick calculation of this A i j t B j t j divided by p j t right. So, we\nknow that this value can take one of n possible. This random variable can take n of impossible\nvalues right. And the n possible values are i equal to 1 to n. It can take value let me\nactually call it k equal to 1 to n A i k B k j divided by p k. So, this value happens\nwhen the column k of A and the and the corresponding row of B is chosen, and that is chosen with\nprobability p k right. So, this is the value chose taken by there\nis the value that the random variable assumes is the probability of that right. And this\nand this cancels out which is the exact, which is exactly the point of this normalization\nand equal to summation k equal to 1 to n A i k times B k j right, which is equal to A\nB i j right. So, then what we then see is that coming back here, is that the C R I j\nhas exactly the right expectation right. Again you can do this calculation not very\nhard to say not only hard to do is that, the variance of C R is this quantity which is\none over C times the summation of the of the product of the squares of A i k and B k j\ndivided by p k minus the minus this quantity this is one by C B i j square. So, one thing\nto notice that I mean if nothing else what you see is that if c increases by the variance\ndecreases so, just to be expected. So, then we have been talking about AB being\napproximately equal to C R right. What we really want is that one way of measuring that\nis Frobenius norm. There are other ways of measuring it like the spectral norm of the\nof A B minus C R, but for, now let us concentrate on the Frobenius norm, and we want to bound\nA B minus C R Frobenius norm ok. So, again I mean A B minus C R Frobenius norm squared\nthe expectation of that is the same as writing the expectation of A B minus I replaced C\nby A I replace R by S transpose B Frobenius norm square right.\nSo, again I mean coming back from here, because Frobenius norm Frobenius norm of a matrix\nis really the summation of x i j square right. So, therefore, the expectation of this is\nreally the expectation of this and, the each x i j, because x i j is a random variable,\nand the expectation of the I mean in this case x equal to A B minus C R in this case\nthe expectation of I mean x i j squared is really just the variance of C R C R i j the\nvariance of the of the ijth entry of C R, which is what we have calculated right. And\njust plugging that that back in if once you finish the calculation what you can say is\nthat expected value of A B minus C R Frobenius square is at most 1 by C Frobenius square\nof a times Frobenius square of B. And we have a bound on the expectation of the expected\nFrobenian the expected error. Now, now we can do multiple, now what we can\ndo is that we can use the I mean we know that this is nonnegative variable. Therefore, we\ncan use Markov\u2019s inequality to say that the expectation is small. And if the expectation\nis small, the actual random variable is not going to be very much larger than its expectation\nwith at least some probability with constant probability. Let us, say it is not going to\ncross 10 times expectation with at most 1 I mean with probability more than one-tenth\nok. We can also prove better bounds using Chernoff style inequalities right.\nIn fact, what we can also say is that, expected value of a we can take away the square this\nsquare is not that important Frobenius is less than equal to using Jensen's inequality\nexpectation of A B minus C R Frobenius square right. This is Jensen and then, you plug in\nthis quality right, the quantity that you have been here ok. So, a special I will just before ending I\njust mentioned a special case that going to be useful for us. So, one special case is,\nwhen B equal to A transpose, in this case the sampling probabilities turned out to be\nvery simple, it is just an it is just the squared norm of the of the ith column right.\nAnd the denominator turns out to be just the Frobenius norm square of A right.\nAnd the and the bound becomes A transpose minus C C transpose Frobenius is less than\n1 by square root C Frobenius norm of A square right, because the Frobenius norm of A and\nFrobenius norm of B are the same right. And in fact, it turns out that in this case this\nFrobenius instead of the Frobenius norm bound, we can actually get a spectral bound right,\nwhich is which is a better bound. Another side variant that, we will see is\nthat that, we were talking about this the sampling matrix S right. We were designing\nthis sampling matrix using the I mean using the we will designed this matrix S using this\nsampling idea. We could also design this matrix S using a sketching idea right. In the sense\nthat, we I mean instead of saying that S is this represents a sampling notion that every\ncolumn has only one only nonnegative entry. We could say that S represents A j A j l matrix,\nwhich is data oblivious actually. Now, right, and it could be either the dense\nJL, it could be the a it could have N 0 1 an so on and so on and so. And in that case\nalso I mean it is easy to get a bound on a transpose minus CC transpose Frobenius ok. So, let us just look at the running time using\na sampling matrix right. Being a I mean doing the sampling takes time m n plus n p right,\nbecause we have to do a linear scan for each of the for each of this for each of the and\ntime C actually. Well, m n plus I mean just to calculate the probabilities you have to\ndo a linear scan over the two matrices, then we have to I mean to choose C samples. And\nthen, so maybe that takes time n c and plus m c p is the time for doing the smaller matrix\nmultiplications. And you get a similar boundary using the FJLT, please think about it yourself.\nYou should note that this is much less than the order m n p that we were saying initially\nright which is for line matrix multiplication. So, just to summarize what we will see in\nthe next couple of lectures is that including this is a randomization approximation is a\nvery powerful tool in numerical linear algebra. We have seen two applications of this already.\nIf you remember, we have seen the this question of approximating the principal component analysis\nusing random projections. We have also in this lecture, we also saw approximating matrix\nmultiplication, and this will be the basis of many results. You also pointed out the\ninterchangeable role of sampling and sketching. So, the reference for this is this lecture\nnotes by Michael Mahoney and Petros Drineas on that is an archive freely available, please\nlook at this. Thank you.", "GQimdDZncIo": "in modern data and computer science the sheer volume of information that we can collect is becoming problematic our ability to collect data far outstrips our abilities to sort and analyze it as datasets grow increasingly massive and unwieldy there's a need for new algorithms that can process the data faster and more accurately at UC Berkeley professor Micheal Mahoney has been working with computer scientist Petrus Rheneas on a potential solution randomized numerical linear algebra is an interdisciplinary approach that uses randomization as a computational or algorithmic resource to solve massive linear algebra problems by efficiently sampling to create new smaller matrices that can serve as a proxy for the original data sets but are more manageable in size their paper Randall a randomized numerical linear algebra published in the June 2016 issue of the sea ACM explores the current state of randomized numerical linear algebra and discusses how it can be applied to topics as varied as genetics astronomy and climate science linear algebra is the mathematics of vector spaces and linear transformations between vector spaces and it's really been at the center of developments in large-scale data analysis randomness is an interesting thing most people think is sort of randomness in the data and oftentimes when statisticians model data they think of it as randomness you have some sort of signal plus a little bit of noise or randomness from a computer science perspective randomness can also be sort of a computational resource an algorithmic resource and there are problems for which you cannot solve them deterministically in a reasonable amount of time but you can't use randomness and solve them to very high quality approximation so randomized linear algebra is a very good model for truly interdisciplinary research in this area of large data analysis these albums have been applied in a bunch of different application areas it's also been applied to atmospheric data and to climate data into ocean data this example we consider genetics data in the form of single-nucleotide polymorphisms geneticists are interested in this because it can get at population genetics among more clinically those applications in precision medicine and personalized medicine we took the genetic state of the snip data and encoded it in a matrix and we considered what does this matrix look like when we projected on to the top three principal components you have a particular structure which provides visual support for the so-called Out of Africa hypothesis so this is nice and it provides the basis for a range of work that heterogeneous and I and others have done in genetics but it also provides an example more generally for how randomized numerical linear algebra can be used in this area these analytical methods are powerful but they can be challenging to use first off manipulating the data for this kind of analysis requires creating low-rank approximation which can take a lot of computational power especially when they're used in an exploratory manner second these results are based on the mathematical properties of principal components and eigen vectors which are notoriously difficult to interpret in terms of the original data in units these can be essential to machine learning and data science but they are linear combinations it may include all the data elements together which makes them of limited usefulness geneticists it's safe to say that most scientists want the output of their algorithms to be something they can interpret easily this provides two motivations for a lot of the work in randomized linear algebra one is to speed up matrix albums very very large data sets and the other is to come up with novel matrix decompositions that are interpretable and more useful in downstream data analysis applications to find out more read professor Mahoney and petros genesis article randomized numerical linear algebra in the June 2016 issue of the sea ACM", "KtLLeifUJR0": "this is topic 3b in computational methods and electrical engineering the name of this topic is numerical linear algebra the purpose of this lecture is not to present a comprehensive treatise on numerical linear algebra but instead just to give you a small taste of what is underneath the hood of all of these different algorithms that MATLAB is running or other types of numerical packages primarily we'll be talking about solving matrix problems like ax equals B or calculating matrix inverses because those tend to be the fundamental building blocks of more advanced algorithms roughly we can divide these methods into two different parts the direct solution methods and the iterative solution methods in the direct solution methods we set up a loop and we ramp through that loop and in exactly let's say a hundred steps we have our answer where as an iterative algorithm we probably make a guess at the answer and then we have another loop but we successively improve on that until the changes in that answer fall below a certain threshold and then we say we've converged on the answer and there's always the chance that the iterative methods do not converge a lot of people tend to prefer those four very large matrices and say that they're more accurate and can get to the answer faster however if you have poorly conditioned matrices the direct solution methods are much more robust albeit they can be slower when you have very very large matrices and last we'll generalize all these algorithms for calculating matrix inverses our first method is called naive Gauss elimination and it is naive in the sense that we are about to implement the simplest and most brute-force way of solving a matrix equation and there are some problems that can arise we won't worry about them we'll just pretend like they're not even going to be problems they're easily fixable but when we don't fix them we just call naive Gauss elimination our goal with naive Gauss elimination is to solve the problem ax equals B and as I mentioned there are some problems that can arise it's fixable with a technique called pivoting I'll mention it later on when when I can point out that that happens but we won't worry about it in this algorithm we're just implementing the naive version of Gauss elimination so we have to start off with a matrix problem here I'm just showing three equations with three unknowns put into matrix form but you can generalize what we're going to do to any matrix size so for illustration purposes we're starting here with an ax equals B problem three unknowns our next step what we would like to do is eliminate references to the unknown x1 from equations two and three which will be in rows two and three so we want to somehow eliminate this a21 and a31 we will do that by subtracting Row one from rows two and three however we have to scale it in a way that will give us a zero in the positions for a 2 1 and a 3 1 so I have here for Row two we will calculate a new Row two and that will be the old Row 2 minus Row 1 scaled by a to 1 divided by a 1 1 when we do this scaling and then the subtraction we're guaranteed to get a 0 in this a 2 1 position then for Row 3 we do a very similar thing we take the old Row 3 and subtract Row 1 now however scaled by a 3 1 over a 1 1 and that's what guarantees that we get a 0 in the a 3 1 position so if we were to be very brief in our language we would just say we eliminate X 1 from rows 2 & 3 the next thing we would like to do is now eliminate any reference of x2 from Row three in other words we would like to get a zero in the a3 to position and we'll do this using the same technique we did on the previous slide we will take the old Row 3 and now subtract Row two scaled by a 3-2 / a2 2 and the prime superscript on a is indicating that those are our new values of a 3-2 and a2 2 that we arrived on the previous slide at this point we will have a 0 in the a3 2 position notice now our matrix where was the a matrix is now an upper triangular matrix we've mentioned this before that when we have a triangular matrix we're looking at a matrix that is almost solved and in fact what you'll see when we calculate X 3 X 2 X 1 from this from this point it's very very fast and simple so onto the next step with this upper triangular matrix if you look at the equation on the very bottom row that is easily solved for X 3 X 3 is now just b3 over a 3 3 and the double prime superscript is to indicate those are our most recent values of those that were calculated on the previous slide so that's trivial to do then once we have a 3 now we can go to the second equation and easily calculate x2 after that we now have values for X 3 and X 2 we can then go back to the first equation and calculate X 1 this process is called back substitution and it's very easy once you have an upper triangular matrix in this slide this is dedicated to our observation that triangular matrices are almost solved the majority of the work and solving ax B is in the forward substitution process and that was gives us these this triangular matrix but once we have that it's very fast and almost trivial to solve and this is why triangular matrices upper and lower triangular matrices find their way into so many linear algebra algorithms because they're so rapid to solve and you'll see that in this lecture I also wanted to point out while we were doing the forward substitution part we had to scale the rows before we did subtractions and if you noticed those scales were highlighted in blue or made of blue color and they're repeated here I just want to point these out so that you can remember where they came from because they'll come up again when we talk about Lu decomposition our next method is the Gauss Jordan method this is an excellent method for solving ax equals B by hand or calculating matrix inverses by hand so those are really the two goals of this method solving ax equals B and calculating matrix inverses you'll find it very similar to Gauss elimination we just talked about with one difference in Gauss elimination we were forming an upper triangular matrix we'll do the same types of row manipulations and subtracting one row from the next but our goal will be to construct an identity matrix instead of a triangular matrix and you'll see that as we step through the algorithm so step one we start with our matrix equation again I'll only be doing a three by three matrix here but this can be generalized to matrices of any size so this is our starting point once we enter the algorithm proper we have our matrix equation ax equals B what we want to do is form what is called an Augmented matrix and that's what's being shown at the bottom of this slide and it is a combination of the a matrix on the left and the B column vector on the right it's we can think of it as a block matrix and our goal here now is to do row manipulations until where we see the a matrix presently we do our row operations until we get an identity matrix on the left and when that happens this last column will actually contain our answer the X 1 X 2 and X 3 but for right now we just have this Augmented matrix and we go on to the next step this is a methodical approach so we set up a loop that will go Row 1 Row 2 Row 3 through all of the rows that you might have and once we're inside this main loop the first step we want to do whichever row we're in we want to normalize that row by dividing by the diagonal element since we're in the first row that diagonal element is a 1 1 which means we divide this entire row by a 1 1 and that gives us a 1 in this position and of course everything else is just divided by a 1 1 and it ends up being whatever numbers we end up with there so rather than keep writing all of those parameters divided by a 1 1 we're just going to calculate a new a 1 to a new a 1 3 and a new B 1 and put the prime superscript on those to remind us that are those are not the original a 1 2 A 1 3 and B ones but they have information absorbed into in this case they've been divided by a 1 this step mirror is very much what we did in Gauss elimination our goal here is to eliminate reference to X 1 from all of the other rows so we're going to set up a loop that iterates through all of the other rows and in this case it's rows 2 and threes and our goal is to subtract Row one scaled by something such that we will get zeros in the a21 and a31 position when we do that we get a new augmented matrix and you'll notice that I'm using prime superscripts to indicate that we now have new values there after doing this operation and so what you'll see for the new Row two we will take the old Row 2 and subtract Row 1 scaled by an a21 and that's what guarantees we have a 0 in the a 2 position after that subtraction same thing for Row 3 it's the old Row 3 minus Row 1 however the Row 1 is now scaled by an 8-3 1 such that after we subtract we get a 0 in that a 3 1 position the next step the main loop in our program that is going Row 1 2 & 3 now we're in the second row and so we normalize that row by divided by the diagonal element in this case our diagonal element is a 2 2 Prime and the prime again indicates that that is not the original a 2 2 there's been some numbers absorbed into that so we divide the entire row by a 2 2 and that gives us a 1 in the a 2 2 position now we're ready to go on to the next step now we enter our second loop which is inside the first loop and the second loop iterates through all of the other rows and in this case that is rows 1 & 3 the goal is to eliminate reference to X 2 from equations 1 & 3 which means we want to get a 0 in the a 1/2 position in the a 3 2 position we will do that by subtracting Row two from rows 1 & 3 but we have to scale it by the right constant to ensure that we get zeros into a 1 2 and a 3 2 position so Row 1 is the old Row 1 minus Rho 2 scaled by the a 1/2 constant that ensures we'll get a 0 in the a1 to position Row 3 is the old Row 3 also minus Row 2 now however scaled by a 3 2 and that scaling is what guarantees we get a 0 in the a 3 2 position now we have our new augmented matrix and you can start to see that the left side of this is starting to look like the identity matrix and we're always doing this in two steps we normalize the row we subtract that parameter from all the other rows and we keep doing this now we're on to the third row and so we normalize the third row by dividing by the diagonal element in this case it's a three three double Prime when we divide all terms by a 3 3 double Prime we get a 1 in the diagonal position now our goal is to iterate through all the other rows in this case that is rows 1 & 2 to eliminate reference 2x3 from equations 1 & 2 which is in rows 1 & 2 so we do this for Row 1 we will take the old Row 1 and subtract Row 3 however scaled by this constant a 1 3 and that's what guarantees we'll have a 0 in the a 1 3 position our new Row 2 will be the old Row 2 minus Row 3 scaled by the a 2 3 that's what guarantees we'll have a 0 in the a 2 3 position now you'll notice we have the identity matrix in the left part of the Augmented matrix so the last part of this algorithm is to extract that very last column of the Augmented matrix and this is our answer it's very important to do this extraction don't do the Gauss Jordan method with your Augmented matrix at the very end circle the Augmented matrix because that is not the answer the answer is hiding in that Augmented matrix and in this case it's in that last column so when you pull it out now you have your answer let's look at an example with actual numbers so we're trying to solve ax equals B we want to find X and so we start our algorithm with an A and a B so remember we have two nested loops this first loop will go rows 1 2 3 the second loop inside of that iterates through all of the other rows so here what we do we form the Augmented matrix you'll notice a is that the left B is on the right and now we can start these loops and it starts by normalizing Row 1 by dividing by the diagonal element in this case by coincidence the diagonal element was a 1 so in fact the first row doesn't change at all we then iterate through rows 2 & 3 and subtract Row 1 from them however we're doing the scaling in a way that gives us the zeros in these two positions and that's where X 1 will be referenced by rows 2 & 3 that we wanted to get rid of now we move on to the next row we normalize Row 2 by dividing by the diagonal element that gives us a 1 in the diagonal position then we start our next loop that goes through the other rows and we subtract Row two from rows 1 & 3 in a way that gives us zeros in these two positions now we move on to Row 3 again dividing by the diagonal element and we enter our other loop which removes reference to X 3 from rows 1 & 2 and now we see that we have the identity matrix on the left so that must be our answer hiding on the right of the Augmented matrix and remember we're not done here we can't circle the Augmented matrix as our answer we must extract the answer out and then we have our answer Oh 2 4 in this case is the answer to so X 1 will be 0 X 2 would be 2 and X 3 would be 4 here's a written version of the algorithm so step 1 we defined a and B step 2 we constructed the Augmented matrix at this point we entered into our two nested loops the very first one will iterate through the rows and in our matrix only had 3 rows so step 3 would be a for M equals 1 2 3 the first step in that was normalizing the Dagda row by divided by the diagonal element then our next loop started which iterated through all the other rows and this is where we've subtracted to put zeros above and below where we had a 1 in the diagonal position and we iterate through all the rows when that is done the last column in our Augmented matrix contains our answer we have to subtract that out and that's the gauss-jordan method we can generalize this method instead of solving ax equals B let's use it to find the inverse of a matrix and we'll talk a little bit more about this at the end of this lecture but now we only have to define matrix a we don't need B anymore because we're not finding an answer X we just want to invert a so we form an Augmented matrix and on the left is a just like it always was but on the right we're going to insert an identity matrix that this that is the same size as a then we do our Gauss Jordan stuff we iterate through all the rows we have our nested loops everything looks the same and we do that until where the a matrix was originally inserted will look like the identity matrix and now what's on the right-hand side of the matrix is the inverse of a and we're not done we have to extract that out of the Augmented matrix and then we'll have our matrix inverse so it's probably the best method I know of for calculating matrix inverses by hand our next method is called Lu decomposition and recognizing that triangular matrices are very rapid to solve the ability to decompose a matrix into lower and upper triangular matrix is a fundamental building block of many many numerical algorithms the first step in Lu decomposition is calculating the upper triangular matrix so we're going to start with our standard 3x3 matrix that we've been working with and if you remember back to when we were doing Gauss elimination we started with this we did a forward substitution procedure and we ended up with a matrix on the left that was upper triangular that is what we do to come up with this upper triangular matrix so our a matrix has changed it's now are u matrix are upper triangular matrix and what was our B column vector there's also new numbers there that we will call D our next step is to determine the lower triangular matrix L and I think this is a pretty interesting thing it turns out there is a lower triangular matrix we can pre multiply this upper triangular matrix and get our a matrix back and even more interesting is what's inside this L matrix it has zeros in the entire upper triangular part it has ones going down the diagonal and this lower triangular part are those L terms that I highlighted when we are talking about Gauss elimination so in fact there really isn't even any extra work we have to do to construct our L may tricks those L terms fall out of the process when we're constructing the upper triangular matrix so in a sense it comes out of the process for free at this point we want to start with our original ax equals B equation I'm then going to take the B column vector and move it over to the left side of the equal sign so we end up here at this point I'm replacing a with L times U the next step is to factor this l term out from both terms in this equation so i factor out an L and what that means is I need an L inverse pre multiplying B at this point I can recognize that this L inverse B is actually our d column vector from the previous slide so we put our d column vector here so here is the Lu decomposition algorithm the first thing we do in step one is decompose a into L and U and we use Gauss elimination to construct that upper triangular matrix during those calculations we're storing these L terms and in fact from there we can build directly the lower triangular matrix just from those L terms now a neat thing is done to make this a little bit more efficient remember in the L matrix it had ones going down the diagonal and zeros in the upper triangular part of that so in a sense we already know those numbers we don't have to store them so instead we can just put in the numbers for the upper triangular matrix and the upper triangular matrix had all zeros in the lower triangular so we might as well just store the L terms there so in fact it's a very efficient thing to do is just store L&U all in the same matrix but recognize that they are actually separate entities now if we want to solve ax equals B given Ln U it's a two-step process that are very very fast and efficient the first is we're solving the LD equals B and since we have a triangular matrix that's very very fast once we have D we can then use it to solve UX equals D 4x which is our original unknown and that is also very very fast that completes our discussion of direct solvers I now want to talk about iterative solvers and the first one we'll talk about is called the Jacobi method so as I mentioned before we tend to be interested in iterative solvers when the matrix size is very very large or perhaps when we have a pretty good idea of what the answer is and we can make a very good guess in which case if we have a good guess it will take very few iterations to refine that answer and get it down to whatever accuracy that we want but this is an iterative algorithm and in general an all iterative algorithms for solving ax equals B we'll start off somehow with an initial guess we'll calculate some sort of adjustment that we have to make to X then of course we make the adjustment to X and we check if that adjustment is sufficiently small that's 10 to the minus 10 or something like that we can say you know what we're finished the details and that certainly changes from algorithm to algorithm but all iterative algorithms have this basic flow to them one thing we need to be careful of with the Jacobi method is that the the matrix we're solving the a matrix needs to be what is called diagonally dominant and the algorithm is very very picky about this so when you write the Jacobi method highly reccommend and the start of your code write in a little script to check and make sure it's diagonally dominant and if not return an error message that will be much more meaningful than when your code errors out because it was not dominant and then it's harder to find so what do we mean by diagonally dominant we need to make sure that each row is diagonally dominant and the way we do that we ignore signs we just take the absolute value of all the numbers and we look at the diagonal element and then we look at the sum of all the other numbers in that row except the diagonal element we need to make sure the diagonal is greater than the sum of all the other numbers in the row and we have to make sure that's satisfied for all rows in the matrix when that's the case we call it diagonally dominant so let's go through a bunch of examples we have a little two-by-two matrix is this diagonally dominant it turns out this is not diagonally dominant if we look in that first row the diagonal element is a 1 if we sum all of the other elements in that row in this case there's only one other one it's a 2 well that too happens to be greater than 1 so the diagonal here is not dominant the second row that satisfies being diagonally dominant but all rows have to satisfy this so that matrix is not diagonally dominant here's another matrix and the question is is this matrix diagonally dominant this is also not diagonally dominant the first row and the third rows are diagonally dominant however look at the second row the diagonal element is a 1 and if we sum all of the other elements in that row we get a 1 in which case that sum is equal to the diagonal element and that's not sufficient to call your matrix diagonally dominant so this matrix is not diagonally dominant next example is this matrix diagonally dominant it turns out yes we have some negative signs in here but since we're taking the absolute value of all of the arguments or elements in the matrix we ignore that we can pretend that they're all just positive numbers and the first row we have a for the sum of the other elements is 3 4 is greater than 3 in the second row we have a 3 in the diagonal position if we sum all the other elements we get 2 and 3 is certainly greater than 2 and in the last row we have an 8 in the diagonal position and the sum of all the other numbers is 5 8 is certainly bigger than 5 so we have another diagonally dominant matrix last example is this matrix diagonally dominant and the answer is no and in this case we have two rows that violate our rule for being diagonally dominant but it only takes one row to not satisfy our rule and the matrix is not diagonally dominant so the Jacobi method absolutely must have a diagonally dominant matrix in order for it to be guaranteed to converge so before we discuss how to implement the method I want to derive the equations used in the method so of course we're starting off with our typical matrix equation ax equals B just three by three in this case and again the algorithm is extendable or generalizable to any any size matrix the first thing we'll do here is take that 3 by 3 matrix and expand it into the three component equations that the matrix equation represents the next thing we'll do we'll take the first equation and we'll solve it for X 1 we get this answer then we take our second equation solve it for X 2 we'll take our third equation and solve it for X 3 it turns out these are the equations that we will iterate we'll have some initial guess for X 1 X 2 X 3 and then we'll use these three equations to calculate our next version or hopefully improved version of X 1 X 2 X 3 and we keep doing this over and over and over and what we'll see is that the changes from iteration to iteration will get smaller and smaller and we iterate until that is sufficiently small now let's talk about the implementation so at the start of this we have to feed into our algorithm a and B because that defines the matrix equation we want to solve next we have to come up with an initial guess for X ideally you know something about the physics or the science or whatever it was that created a and B to begin with that lets you come up with a good guess if not you're forced to just try different things maybe you set X to all zeros maybe you define random numbers and X maybe you can make good guesses as to just some of the elements in X and not others and you use random numbers you may have to try different things if you don't have a good initial guess otherwise we now have our initial guests and we have our main loop that's just going to iterate until every time we calculate new values of X they're not changing very much so we simply just iterate through our three equations for calculating our new values of X now there is a trick here the first equation would work just fine we're calculating a new value of x one from old values of x2 and x3 so that would work just fine now if we just type this into MATLAB blindly we could accidentally be using the new value of x1 instead of the old value this should be the old value here not the new value so in fact when you're calculating these normally in MATLAB you would calculate an x1 new an x2 new an x3 new and that way you can still use the old values and then just copy it over when you're done so once you have the new values of X you move on to step B here and you calculate how much they've changed and we look at the difference between the new and the old values and maybe we add up all those differences and we're constantly monitoring that and we wait until that difference between the old the new values is sufficiently small and we'll probably pick a number like 10 to the minus 10 and then we're guaranteed to have four or five digits of accuracy and our final answer so it really is that simple now we really would much prefer to implement this using matrices instead of algebra because it's more easily extendable to large matrix sizes so let's think about this we started off with our three equations that we use to iterate what I can do now is just write these in a slightly different form now you may have to think about this for a bit and stare at it for for quite awhile but it turns out by observation we can take this and write it directly in matrix form so what I like to do is first look at these terms in parentheses those really are our original equation just without the diagonal terms this first equation which used to have an a.1 one times x one over here on the left is no longer there X two is no longer in the second equation X 3 is no longer in the third equations so it's our original equations - the diagonal terms so we can write this in matrix form as our original matrix a - the diagonals and a so now we'll have a again with just zeros running down the diagonal and that multiplying our X column vector so this product of terms here will give us these three equations and - those diagonal terms then we're subtracting that from our B values so we need to subtract that from our B column vector and then notice on the outside we're dividing by the elements of a which are along its diagonal so we've pre divided by the diagonal of a so this is the matrix equivalent of these three equations and here's just to remind us what the diagonal of a is so it's all zeros except just the diagonal elements of a running down its diagonal so for simplicity let's let D equal the diagonal of a so D for diagonal that lets us write our previous equation from the previous slide this way we had an A - diag a here and we had a die I gave her here so now we just write that as a D and an A - D we then do a bunch of matrix algebra and we end up in a form that gives this some pretty good physical meaning and we have the new value of x equals the old value of x or the previous value of x plus some kind of improvement to X and I like formulating my equations this way because it we calculate the improvement on X which I can then do two things with the first thing I'll do is use that to improve our value of x but the other thing is I can use that improvement to see how big it is to see if we're finished so here's the entire algorithm written out step one we're going to define our matrices a and B although maybe you could say that's not even part of the algorithm those are inputs to the algorithm and maybe even step two is outside of the algorithm somehow we have to make an intelligent guess and maybe we're just all random numbers maybe we know something about the physics of our problem that makes us or lets us make a good guess really step three is where we enter the algorithm proper and the first thing we'll do is we'll extract the diagonal of the a matrix and in MATLAB it's just simply D equals diag of a and it knows to pull the diagonal out of a then we enter our main loop and the first step inside the main loop is to calculate this adjustment of X this will tell us how much we need to change our X column vector to get to our next better solution and so a MATLAB that's just DX equals D backward divided B minus a times X so that's pretty much as typing in what we see here now remember once we have that Delta X this is the thing that will do two things with will use it to improve X we'll also use it to calculate our error so that's the next step we update our values of X based on this adjustment and then we calculate the error and we iterate until that error is sufficiently small and that's it that is Jacobi iteration I have a block diagram with this method which may let us visualize a little bit better so here I'm showing the input arguments as a B and X X may not be an input argument it might be in the algorithm where we just fill it with random numbers so step one we extract the diagonal of a and we call that D and we enter our main loop and are we done what we just started so we're not done yet we calculate the adjustment we adjust and we use the adjustment to figure out how much we've adjusted and we keep doing this until that adjustment is sufficiently small and then we're done our algorithm the last topic in this lecture is calculating matrix inverses and we only have two things to discuss here one is using the Gauss Jordan method as I mentioned before what we want to do now is construct an Augmented matrix with a on the left and I the identity matrix on the right a is the matrix that we want to invert so our augmented matrix will have a on the left and identity matrix on the right now we do the Gauss Jordan method we do our row operations subtracting rows from other rows scaling and we do that in a way that gives us the identity matrix on the left and what we're and what we end up with on the right is the inverse of a so we implemented exactly like the Gauss Jordan method for solving ax equals B we just have a few more columns on the right and when we're done we extract that out and that is the inverse of a this is my favorite method of calculating matrix inverses by hand of course I don't like calculating matrix inverses by hand especially when they're large I much prefer doing that on a computer we can also calculate matrix inverses with Lu decomposition although what I'm showing here really can be generalized to any technique you have for solving ax equals B we can use that to solve for matrix inverses so what we'll do is we'll solve for ax equals and instead of a arbitrary b vector here we're gonna put a 1 in the first row and a zeros and the rest of the positions but a 1 in the first row zeros in the other when we solve this what we actually is the first column in our a inverse matrix to get the second column in our inverse matrix we now put a 1 in the second row and a zeros in the first and third when we solve this matrix equation the answer is the second row in our matrix inverse and you can probably guess what's going to happen now to get the last row we put a 1 in the last position zeros and all the other positions when we solve this little ax equals B problem we get the third column in the matrix inverse so really any method we come up with for solving ax equals B we can generalize it using this method to calculate matrix inverses and that is it for this lecture thank you very much", "2hAiJe8ITsE": "- This video is about Object Detection using an algorithm called YOLO. First, I'll talk about\nwhat Object Detection is and explain a naive\nsolution to solve that. And I'll explain the steps\nof the YOLO algorithm, and then I'll go into how to implement YOLO solution for your own application. So, to begin YOLO Object Detection is a problem that's distinct from classification and Object Detection. Our goal is to identify and locate objects within an image using YOLO. So here in classification\nas you might remember, it's matching one label to one image. So as you can see this images is a cat, so we want to label it as a cat. In Object Detection of course\nit's a bit more complicated because now we want to\nlocate multiple objects within an image and we want to find the location. So now this image has a dog and a cat. We wanna identify both these objects here. We wanna draw a box around that. And then in this object, it's you're finding that there is a car, but we wanna know where\nis the car in the image. So we call these boxes we're\ngonna draw bounding boxes. And we'll explain how you can put them and work with them a little bit later on. On to our naive approach, holds a lot of the same\nlogic from classification. The idea is that we have a sliding window that we pass through the entire image and scan through here, as it scans through it picks\nthis to the window here and passes that into\nyour Classifier Model. Much like we saw in the past. So as it scans, all these little\nsnippets and these windows are passing through our Classifier Model which might be like a\nConvolutional Neural Net. And then from there, it will predict, is this a dog, a person or is it nothing? it's doing that every single window that we pass through. So, most of these is gonna predict absolutely nothing, right? Because there's nothing to predict over. But sometimes when this\nwindow lands on like an image of a person or a dog and then a predict something. So there's, that's how we can use the logic to predict where\nobjects are in an image. The thing is, this is slow because they're passing\nit through a bunch of, you're passing it through\na bunch of windows that don't contain anything. So it can't be used for\nlike realtime cases. And then when improvement is something called the Region-based\nConvolutional Neural Net. And this, what we're doing is we're looking for areas of interest. So instead of passing, you\nknow, each window into it, we're looking for areas\nthat might have an object. So we would use something\ncalled image segmentation then you get where an object IP since this is our IP we have a piece of interesting shape here. Then we would pass like this\nobject or this window in here into the Classifier Models, wouldn't pass, you know\nsomething like over here where there's clearly nothing and it would appear that\nthere's really not much in the shape over here, right? So that's idea between our Region-based\nConvolutional Neural Net. There's an improvement\nover this naive approach. But the model we're gonna\ntalk about today called YOLO, is a much much better, much\nfaster Object Detection model. So next of the steps\nof the YOLO algorithm. The YOLO algorithm is called, it's called You Only Look Once, although for short, because it can pass in a whole image into a\nConvolutional Neural Net and then predict the output in one pass. And it's much faster than\npassing in windows altogether. The idea here is that\nthe Convolutional Net is predicting some type of, of predicting on an image,\nlabels, finding boxes, and confidence probabilities so it's bringing all\nthese different values out in one pass through the\nConvolutional Neural Net. And here's a pretty quick overview. You passed in an image, we pass it into a Convolutional Neural Net and then it outputs a nice label, and a confidence probability here, and then a bounding box on the object. So then the steps of the\nalgorithm are as follows. Well first I'm taking image, we're gonna divide it\ninto an S by S grid , so for example, we're S is equal to three so we're dividing it into\nnine equal cells here. And the cell is one little box and then each cell is responsible for predicting the\nnumber of bounding boxes. So in this case, B is equal to two so each cell here is\nfilled with two boxes. And what we mean by\neach cell is responsible for keeping two bonding boxes is that the center of the bounding box, also within the cell, and that cell should\npredict that bounding box. So as you can see, here is\na new box around this car, the center is right here, and it falls within this cell border here. So this cell should\npredict this bounding box. And as you can see here sometimes these bounding boxes\nwill extend beyond the cell and that's perfectly fine as long as the center\nfalls in the bonding box that bounding box of the center, the bounding box falls in the cell, that cell can predict that bounding box. And what happens is you see there's a lot of different\nbounding boxes here that don't contain anything. So we would set some confidence threshold, anything below the confidence interval is just gets removed and we return when the boxes\nof the confidence interval. All these boxes are gone and then this box, which has a higher confidence interval should go there, would be returned and the rest of these\nwill go somewhere else. And you can have, of course\nthere's confidence threshold, but in this case it\nwas something like 90%. And that's a little side note here. The idea here is that, you\nknow, S was a small number three but in practice we use a larger number. Yours should be a larger number as well. You can predict way more\nobjects in this image. So how do we take these boundary boxes which are these squares\nof interesting objects and code them into sort of a matrix and then take them through\nthe Convolutional Neural Net? That's where it's explained\non this side here. We're gonna start with the simple example where there is gonna be a three by three, by three grid, and there's\ngonna be a total of nine cells. And then each cell will\npredict only one bounding box. Last time it was two, but this time we're gonna predict exactly one bounding box. The object is either a dog or a human. Okay so here's the image, we've cut it into nine cells. And we're gonna be focusing\non this particular cell here which is interesting because\nit contains the human. And for each cell, we're\ngonna predict a vector y. So we're gonna predict the vector y through this cell and I'll show you what object is vector y. So this bounding box is denoted by a center b x and b y, has a height and a\nwidth, b w and b h here, is actually brought into\nthis y matrix right factor. So these values over here these four values in the middle there, we also have two, three more values, Pc is a probability that the\nboundary box is an object. So it's probable that this\nblue box contains an object. And then c one and c two, this is a conditional probability. It's the probability that\nthe cell contains an object that belongs to class one or is it probable that\nthis cell contains the dog given that this bounding\nbox contains an object. So these are conditional probabilities but these allow us to say given that this, this bounding box contains an object what is the probability that this cell contains something that\nis a dog or a human? And then, so we're gonna\nwalk through an example here. So you saw here that, that for this particular cell the probability that it\ncontains an object is high because there's obviously a human here, so we should get a value of one and it has, I use some type of values here for the bounding box's center, and within the height, these are actually written as a percent of the cells dimension. So it's b height is actually\na percent of the cell. So this our height and then b width, is the width of the bounding box as a percent of the cells width. So these are basically how\nwe move right to data here. And then for C one and C two, well, C one is obviously, should be zero because this is, obviously\nnot a human, it's a dog. So this is obviously a\nhuman, it's not a dog. So the probability that it's a dog or object class one should be low, which is why we put zero. And then the probability\nthat this particular cell contains a human is high because we would actually use a human here to be closer to one. That's, now we can basically\ntake this bounding box and break it down into a probability. And then some values variables that show where there's bounding boxes and what size it is. Okay. So that was a particular case there was exactly one bounding box, right? Each cell was predicting one bounding box. What if we want to pick more than one bounding box per cell? The idea here is that we can, we can pick now two bounding boxes. So we have a blue and a red bounding box. The blue box has the same\nb x, b y, b h and b w. The red bounding box has\nthe same type of a profile. Very good to specify. What we do now is we can\njust extend our augment y. So now we have these five variables for this bounding box, and then these five variables for this bounding box. And as you, as we mentioned earlier, this Pc it's a probability\nthat this bounding box contains an object. So it's unique to the bounding box and Pc here is now our red bounding box. This bounding box contains an object so it's unique to this bounding box. So, what we have here is\nthat these five different, these five variables here, denote each unique bounding box. And then as you recognize\nthat c one c two, there's only two of them here, right? Because there's two classes. And the reason why is this is the probability that\nthe cell contains an object that belongs to class one, right? So it doesn't depend\non bounding box as much because it's just the probability that the cell contains an object. So that's why there's only\ngonna be the two classes there's gonna be two values here. So, as you might've noticed here that y has the dimension five B plus C, there is, by variables\nfor each bounding box and then there is the bounding boxes. And then there's a new b and c, c classes here, So there'll be c values here. So this is the dimension of y. And remember we have one y\nin each of the cells here. So that allows us to see that we are predicting a y each cell, and then each y should be\nable to form five B plus C. That means we have S by S, cells here, and each cell has five B plus C output tensor. You can see this mentioned here. So the results then what was, what we're gonna want to predict from our Convolutional Neural Net is a bunch of y vectors. Each y vector, we can think of this, the y vectors together as\nsomething called a tensor which is just a fancy way to say a bunch of multidimensional matrix. We're gonna store data in it. And it should be of this shape, S by S by five B plus C. So it's B, S by S, which is\nthe dimensions of the cells. And then we have the five B plus C which is the length of y, right? This illustration here. So, the overview, the YOLO algorithm must\nalways have an input. We pass through a\nConvolutional Neural Net, and then we can have some output which we label here. On what that actually looks like, is that the image is broken down into, within height and pixels,\nand then there's RGBs, there's three color shells there, so we have three, these are three like three sheets. And these are each of their colors. And remember, RGB is just the value for the color rating between zero and 255. So let's take an image and break it down into basically a tensor or some multi-dimensional matrix here. They pass this into a\nConvolutional Neural Net, which is essentially a series of convolutional and pooling layers. If you're familiar with that from the Convolutional Neural Net, round you up, and then from there, you've, is they're predicting this S by S by five B plus C tensor. So this is normally, we're not\ngoing to talk about the CNN, but the idea is here, is we know the outputs are, and we know what inputs are, we know what outputs are. And then the goal is we want, we've just trained. (indistinct) So one of the important things for any model is to measure performance. You wanna see how well your\nmodel is actually predicting. And in this case we're using something called\nUnion over Intersection which measures the overlap\nbetween two bounding boxes. And if you're to measure this, you are gonna be using a training record to be upgrading the UoI\npredicted bounding box and the ground truth. The ground truth is\npre-labeled bounding box that we're aiming to match. So this is the image here. And then we want to label the ground truth which is that box. This is the correct human\nmade label for the box then predicted bounding box is the box that our machinery model or Convolutional Neural Net predicts. And the Union over Intersection is the area of the intersection, which is this area here, over the union of, the area of the union which is the whole, the whole green and the blue box here. That's the savior. And this fraction is what we call the\nUnion over Intersection. And just to quantify this there's gonna be a core you\nknow, a sectional core score. That's very low overlap and it would be, you could\nhave been it's higher. And then it's very, according\nto this, almost excellent. So one of the issues with these types of\nObject Detection models is that you might have the same object being detected multiple times, and we can solve this with a technique called Non-Max Suppression. The idea here is that you are going to want to obviously remove, remove bounding boxes in a\ncase where the same object has multiple bounding boxes with the same label on each other. I'll walk you through those steps here. So, first I want to identify the box with the highest confidence. So that's this box here, the\nblue box at the back here. And then you are going to now calculate the Union over Intersection between the blue box\nand all the other boxes with the same label. So the blue box and an orange box has a Union\nof Intersection of 0.62. And then we'll also calculate\nthe Union over Intersection of the blue and the purple box. We're gonna select some threshold so we're gonna set the threshold to be something like 0.3. Any of these boxes with\nthe Union over Intersection and then point will be\nsuppressed or removed. So I was gonna see here the orange, and the orange with the blue, and the purple with the blue, have a Union over\nIntersection greater than 0.3 so we're gonna suppress them\nand remove those tiny boxes. And now we're now left with\njust the blue bounding box. So next, I'm talking about how to implement YOLO for your own particular use case. It's harder to use a pre-trained model. One of the big problems is that there are so many, or to basically get your\nimages ready for a YOLO model it takes a while. You have to draw the bounding box, on all your objects, and you need many many images to actually train the model throughout. So what tends to be used is something called is pre-labeled images. You can find them from\na variety of PC data. This is online. And you can use those to train your model or a lot of people have already went ahead and filled virtual models\nusing those images already. So you can just take their models, download them and use them\nin your own applications. One of these places for those, a lot of pre-trained labels, for pre training images is\nsomething called the COCO, or the Common Objects in Context. This particular data set is a variety of different images that are labeled used for Object Detection in image recognition. One of the challenges is\nthat if you don't have, if the object is not in COCO, then that data set you\nwon't be able to use it. So, here is a clear image of some food, we're gonna pass it into\nYOLO implementation, which was of course\npre-trained with a COCO model. And then we're gonna output this image with these tiny boxes labeled. Now you might notice that the cantaloupe and then the pineapple they're not in your (indistinct), I mean, in the COCO data sets they're not gonna be correctly outfitted or, or evened up by the model. So it doesn't do anything. And that's why, you know it's predicting the orange, the apple, and the banana but in the bowl in the back here, but it's not, pineapple and cantaloupe because it doesn't exist in any place that it was trained. So the COCO data set is pretty large it has a variety of\ndifferent objects in it, on here, just a quick list of some, there's a person, a car, a motorbike, traffic light, and so on. And then you can approach\nthe documentation to read about all the\ndifferent types of objects that these models have been built with. And so, that was using\na pre-trained model. What tends to be the cases you might want to build them out of that predicts objects that are\nnot in the COCO, right? So if the object is not in COCO, or if it isn't COCO it's okay because you can go ahead and download a pre-trained model and just go ahead and\nbuild your application using that directly. And here's some, I just think some imageAI which is implementation that has a pre-trained\nYOLO model using COCO, as well as Darknet. That's another, another\npre-trained models. These are very easy to use. You can just look up the API and use them pretty readily. The other hand, if objects are not in COCO then you'll need to\nbuild the custom model. And in this case, you'll\nneed to go into find images of the particular object\nyou want to predict on you'll need to label the images, you'll need to go through and implement your own model. And you can do this in a variety of ways. One of them is to build your own model using openCV, Tensorflow, Keras. to build the whole model. One can also use some libraries such as imageAI has a nice, I believe we can actually custom, the custom training model using images that you've made for yourself. Then real quick here's just an image of some COCO pre-trained labels if you're interested in (indistinct) you can check them. What you wanna predict is\nin this inner set here. And then there's some\nfurther tips for this meeting and that's it for the YOLO model. And next I'll be talking about the code in the next step.", "k60GUovpsLw": "our next speaker is Rasmus who was a postdoctoral scholar at Harvard and he will be telling us about matrix marketing nails and their applications randomized linear algebra yeah thanks for the introduction Joel and thank you to the organizers for inviting me and I guess I should plug this in yeah so my hope is to look at a particular example of using matrix martingales in numerical linear algebra coming up and try to convince you that you know you should probably do that too it seems to be a really good tool okay so let's start with just concentration of scalar random variables not matrices we've got random X that's the sum of little X I random variables that are each real value they're independent and let's say they have mean zero and then we ask is this approximately zero with high probability right so Bernstein's inequality will give us some concrete conditions under which this is true so this is that okay if it's also always true that our random variable is absolutely bounded so absolute value bounded by R and the variance of all these values all these little random variables sum together is that most Sigma squared then we get a probability that a bound saying that the the sum of these guys X is probably not larger than epsilon in absolute value with some good probability for a large enough Epsilon so for example I just picked some parameter values if epsilon is 1/2 and R and Sigma squared are both like 1 over log tau we'll get a probability bound of town it's not important that you parse that right now it's just to give you a big sense of you know you choose some parameters you get some problem ok now I said I was gonna talk about martingales I'll talk about matrix martingales particularly but first let's just remember what's a martingale maybe it's been a while since you used one so we've got a random X this is the set up from before the important things were it was the summer things the things were independent and they had 0 me this is not a very flexible set of conditions like if I'm doing algorithms usually my things are quickly not independent ok so meeting matrix mine Gail sorry give you a situation where you can deal with proving concentration even when things are not independent you need the second condition to be changed so you ask for something a bit stronger here namely that not only are your individual variables 0 mean would you put some order on them from X 1 through X ok and you say that X I is mean 0 conditional on any set of outcomes for the previous variables so it still means 0 no matter what happened before so I'm gonna write that summarize it as X I conditional on the previous steps this means it ok so let's remember Bernstein's inequality and see if we can get the same kind of thing for Martine deals you can that it's them called Friedman's inequality this is at least one way to get it a martingale concentration bound so again we switch from independence to asking for a zero mean condition now we want to prove a probability bound like this okay so I think many of us in the audience here have used problems down so it just yet another one but you probably haven't seen this one so we'll be a little bit careful about understanding it it gets a little bit more complicated in the conditions now because we don't get to just say that the variance measured by summing up the individual X I and squaring them is small we get a different variance object that we need to control so we want to say that the sum over X I squared conditional on previous steps this sum is small okay this is harder to do because this is now a random variable this variance object for X I that depends on what the previous outcomes were it's not independent from it and we need to bound the sum anyway okay but that's maybe that's a lot to ask for so Friedman gives us a little bit of extra flexibility it says you don't always need to bound this you just need to bound it with a good probability okay so if it's usually the case that the variance object the in the natural definition for martingales which was this thing if it's usually the case that this thing is small you'll get a probability okay so you get a different random variable that acts like a variance and you have to say this is usually well-behaved and then things will go through so if this has a low probability of being a greater than some threshold Sigma squared my like my variance they now get the Bernstein probability bound except for a little fudge of an extra Delta probability okay so this is what martingales do for you they say you can basically pretend that you're doing Bernstein if you can control this funny variance okay so that was Friedman's inequality but to make it exciting we need matrices so there's a matrix Bernstein inequality which is just it's due to trop sitting right here Joel and Oliveira sorry yes there are different versions so this is just Bernstein the the concentration inequality that we saw but sort of adapted to a matrix case so now you are random variable X is a D PI D symmetric matrix in this setting that I'm considering sum over X I so it's a sum of little x i's that are also d by d symmetric matrices the x i are independent they're mean 0 their spectral norm is always small and this variance object here has small spectral norm also and then we get with like the same symbols down here exactly the same kind of probability statement saying that the thing is probably concentrated like it's unlikely that the sum has spectral norm greater than zero there's some important changes we got a D here which is the dimension of the matrices so the probabilities get a bit worse this is basically because we need to Union bound over D different eigen values that can go wrong something like that the other thing that you should be careful with is that this is a funny way of measuring variance okay each x I squared is a matrix we're taking expectations of some funny matrices we're adding them all up together and then we're taking a norm this is going to be extremely important that we have a funny notion of variance for applications so can we get a matrix Freedman the answers yes and we're just making all the same changes as we did in the scalar case so we get zero mean conditional on previous steps we get this variance conditional on previous steps has to usually be small here are some parameter examples the only thing that changed from doing scalar Bernstein way back in the day when we started on an earlier slide is that you need a D in this lock to get rid of this guy if you want small probabilities okay so what's the takeaway it's that you can do concentration on martingales but you have to worry about some extra object and for us it's this funny notion of variance which is called the predictable quadratic variation and which is a random variable so we're gonna be a little have to be a little bit careful for thinking about it I I think you can come up with other random objects to control instead of this one and that could give you a maybe a turn-off like version of this too okay now we want to think about an application so we're going to talk about laplacian matrices which are maybe not familiar to everyone here so we'll just do a quick definition so the plastic matrices are matrices that we use to talk about graphs undirected graphs so we have graph G with vertices V and edges eat some positive edge weights let's say that there are n vertices and M edges here I've got a graph on three vertices I'm gonna build the laplacian of this graph so it's a matrix that I associated with an undirected graph to help me talk about the graph okay so I'm gonna build it it's gonna be an M by n matrix same so a column and a row for each vertex and I'm gonna build it by summing together little baby Laplace Ian's one for each edge so now I just need to tell you how to make the baby laplacian of an edge so let's look at the edge between a and B and build a baby laplacian for it so again it's still an N by n matrix it only has entries in the rows and columns for a and B and it's just a 1 on the diagonal and a minus 1 on the a B off the animals and then we scale it by whatever is the weight of the edge in the graph which is a positive number then we add them together we get the plane of the graph I have more definitions but I don't have time for them so there are many equivalent perspectives on the laplacian but we'll just keep with this one so let's build an example it's a graph on three vertices there's an edge of weight one between vertex one and vertex two it puts out one on the diagonal for one and two and a minus one on the one two off diagonals okay an edge great way to between vertex 1 and vertex 3 it puts a 2 on the diagonal for a 1 and 3 and a minus 2 on the 1/3 off by another edge between vertex 2 and vertex 3 and adding them all together gives us the laplacian of the graph okay so theoretical computer scientists we just love solving the plasti linear equations because it lets us do a lot of fun algorithms so there's a very basic building block so it was got very excited people got very excited when Spielman and tang in 2004 showed that you can solve these linear equations in time that's nearly linear in the number of nonzero entries of the matrix which is just the number of edges in the graph so this is a very useful tool but let's not talk about why we just want to learn about martingales to me then there was a lot of other work making this much much more reasonable and I'm just gonna focus on one particular paper which gives a very simple algorithm first these linear equations and it uses matrix models okay so we want to solve a laplacian linear equation so we've got L and B and we want to find X such that L X equals B okay the classical way to do this is Gaussian elimination which says let's find an upper triangular matrix u such that u transpose u equals L and then we'll set X equal to u inverse u transpose inverse times B okay but the last step is easy because we can solve linear equations in upper and lower triangular matrices very straightforwardly okay first you look at the row that only has one entry you solve in that row then you look at the row where that only has two entries but you know the answer for one of them you solve for that row and so on okay if you didn't follow that you can think about it offline but it's not a terribly hard exercise but how do we get the U transpose u well we'll talk about that in a second first let's see what's our algorithm going to be it's it's Gaussian elimination is not fast in general it's an N cubed time algorithm but we'll make it fast by doing approximate Gaussian elimination so we're going to find a you that's upper triangular such that u transpose U is approximately equal to the matrix L so this is weaker than what we had before so what are we winning well we'll gain that you will be sparse and we'll be able to compute it very quickly and then the fact that it's only approximately a decomposition of the matrix we want it is not going to be much of an issue we'll do log 1 over epsilon iterations of solving linear equations in the wrong matrix fixing the left over error and then we when we've repeated this a few times we get an epsilon approximate solution in some motion of approximate that I haven't told you about yet but we'll get there so it's not a big problem that it's only approximate okay so Sushant Satya and I showed that when L is a laplacian matrix with M non-zeros you can find an M log cubed and time an upper triangular matrix with M log cubed and non-zeros that's a pretty good approximation in the sense that it gives a decomposition that's approximately equal to our starting matrix in a way that's sufficient for solving linear equations questions about that statement it's a constant factor approximation and spectral noise it's a very very strong statement okay so let's remember how Gaussian elimination works there's a slightly unusual perspective going on here but if you don't see you know there if you don't remember the difference it's not important but we take an additive perspective on it so let's take a matrix M that's PSD and let's do Gaussian elimination to it here's an example okay so first we find the rank one matrix that agrees with M on the first row and call so that's the first role in com here's the rank one matrix that agrees with it let's subtract the rank one matrix now it's zero in the first one column what's left and we say we have eliminated the first variable what's left is a PSD matrix and we can repeat so we're going to find a rank one matrix that agrees with our matrix on the next row and column here it is and we can subtract it there's zero in the first and second row and column we've eliminated the secondary and we can keep repeating this until we've written all parts of the matrix as Rank 1 terms and notice a very special structure the second term has nonzero entries only from the second entry onwards the third term has nonzero entries only from the third entry onwards and so on so we can rearrange this as the product of the lower triangular matrix and an upper triangular matrix this is just this column goes with this column is my first rank one term this goes sorry for this row this column goes with this row is my second rank one term and so on now if we transpose that lower triangular matrix we see that we found our u transpose u so that was Gaussian elimination now what's special about doing Gaussian elimination on Laplace it's the remaining matrix after we pull out some rank one parts is another laplacian okay so let's see an example of this here we have a laplacian matrix a i'm going to write it as a rank one term that agrees with the first row and column and then what's left see here's the rank one term you can see that it's rank one because i now write it as an outer product of the vector with itself and then look at what's left it's another laplacian you're not used yet to check in quickly whether something is a laplacian so maybe you know don't try but it is another passing okay so we saw something special about laplacian so we saw gaussian elimination let's talk about why it's slow so solving alex equals beat by Gaussian elimination can take order n cube time even when the matrix is the laplacian and even when the input matrix is sparse okay and the main issue is what's called fill so we've got a apply C&L and we want to eliminate a row and convey which in the graph we can think about the corresponding graph it's a vertex V and there's a bunch of you know it has nonzero entries this corresponds to edges to other vertices in the graph so let's look at the graph picture for what Gaussian elimination does so it says you know this variable would disappear and its edges to other vertices will disappear but then in the remaining matrix when we look at that as a graph we no longer have this vertex but we have a clique on its neighbors so this is what Gaussian elimination does when we pull out this rank one term we're going to create a clique on the neighbors so the nation plates a clique on the neighbors of B okay these clicks can very quickly make the graph grow dense and this is what makes Gaussian elimination slow so this happens for example in an expander graph that doesn't work at all there's no ordering for an expander graph it always gives in cute okay and that's not just expanders that's not that hard to construct okay so but there's hope because Laplace Ian's are particularly susceptible to sketchy right I would say sparse vacation but we're in a room where you say sketching it's a particular kind of sketching right so we can specify this click OK and we can hope that the new matrix is a good approximation of the old right but we're going to have to do this a lot of times right I probably can't apply a matrix Chernoff bound every single time I eliminate a variable I would go wrong but this is exactly what we're going to do we're going to eliminate variables and just immediately without even in fact we don't have to write down the clique ever we can sample from it without writing it down and this is going to be our algorithm for doing approximate Gaussian elimination now we won't have time to go into the same playing exactly but it's three lines it's very straightforward three lines of code so Gaussian elimination said pick a vertex be to eliminate add the clique created by eliminating V and repeat until you're done approximate Gaussian elimination says pick a random vertex V to eliminate sample the clique created by eliminating V and repeat until you're done and that's the whole so is this something done again by effective resistance like no but you don't need to calculate anything yeah and for the numerical analysis people in the audience of whom there are many this very much resembles doing randomized incomplete Chomsky factorization so in a way you can think of this is us saying well the algorithm actually works and we can prove it on any laplacian matrix but we have to randomize it and neither of these randomizations is an artifact so picking a random vertex is not the only way to do well I think but there are orderings that will give you trouble and random orderings work ok so remember the goal was to get that you know that u transpose u in the end is going to be approximately equal to L so how are we going to prove it we're going to show that in expectation u transpose u is indeed equal to L and then we're going to show that it's concentrated around its expectation and then we'll apply a concentration bound it will be matrix Freedman to show that it's approximately equal to L with high probability in in a useful spectral sense so first let's look at the expectation so consider eliminating the first variable and now I'm going to call the original laplacian l0 and then I write it as a Rank 1 term I'm not doing any sampling yet a Rank 1 term that's the row and column I pulled out and then a remaining graph plus a clique created by elimination right now I sample the clique all right this gives me my approximate matrix after one elimination let's think and let's call del one let's think about the expectation of L 1 ok well the expectation of well 1 we're not sampling the rank 1 term so we don't need to worry about that we're not sampling the remaining graph so we don't need to worry about that we can push the expectation past all those and just say well suppose that we get the clique right an expectation then the whole thing adds up to the original matrix okay and then the expectation of l1 is l0 so let L IP our approximation after I eliminations okay if we insure at every step that the specified cliquing expectation is orig is equal to the clique that Gaussian elimination would create then we'll get that the expectation of Li is Li minus one okay and now with the way that I've defined martingales this says that the expectation of the difference is zero and this will be true conditional on the previous steps right it just has to be true locally that we're preserving the expectation of the clique and we got a martingale and I think also that I like you to maybe consider whether this shouldn't be a fairly generic phenomenon right what I'm saying is that algorithms like to be locally sensible right and locally sensible is like preserving the expectation is in this particular step right and that's exactly what gives you a martingale property so martingales are a very natural fit I think for algorithmic analysis provided that you can find the right operator where some expectation stays the same although if you go read about Dube martingales maybe you don't even really need to keep the expectation right although of course you'll need some control over the expectation okay so we're running you know slowly out of time but you know let's talk about the approximation a little bit is it this one right is that is that what I'm going for no no if I had this kind of spectral approximation it wouldn't help me solve linear equations at all like for linear equations small eigenvalues are important because when I invert the matrix they blow up to be large eigenvalues and this spectral norm is just telling me that I got some large eigenvalues right so we need something completely different we need to measure the spectrum in a kind of relative sense which we do by dividing everything through by L before we do approximations okay but for matrices dividing through by L means multiplying by L minus half on each side so we transform our approximate decomposition apply L minus half on each side it's just for analysis of course now L becomes identity and so we want this thing to be close to identity and that's enough for solving linear equations okay it's hard work to keep writing L minus half everywhere so let's give it a name and call it it's a tropic position I'll put things in isotropic position by calling them blue and putting a bar on them just you know for those of you who might have trouble seeing the blue the goal is now to make u transpose U and isotropic position be approximately equal to L okay that's the most important thing together with the PSD order or the lova order which I'll just remind you says that if I have two symmetric matrices a is less than B it's a partial order on matrices if for all X X transpose ax is less than X transpose BX okay let's have some random variables okay so we're just sampling edges the random variables look like either you scale up the edge by something to counter the fact that you only keep it by probability P or you set it to zero okay and this clearly has expectation L e okay it's not exactly what's happening in the algorithm but let's just think about flipping a coin for every edge the real analysis is not much different then we have some zero mean variables just subtract the meat and then we call them blue to put them in isotropic position and now we remember the predictable quadratic variation right this is the hard thing to control some funny object like this in order to apply martingale concentrate so we wanted to show that this thing is usually well behaved okay now we can get a promise it turns out if we do our sampling sensibly and this is subtle but let's not worry about it right now and it's often not the hardest part that every sample that we make has low variance okay and now I think we're almost at the message the last message I want to get across which is that the variance was a matrix right and I told you it was a funny notion of variance it looks like the laplacian of the edge in isotropic position okay that's a time bomb because this seems simple but it turns out that we're very close to something very powerful now it's somehow relating the variance of each sample back to the structure of the graph is a very powerful statement but it's not enough because we're constantly changing the structure of the graph and that's I think one of the hard things that you know we need to get used to form our tamils so remember that when we're doing elimination we eliminate a vertex we create a clique on its neighbors right then we sample the clique and I just told you that the variance that we create looks like little baby Laplace Ian's on these edges but an isotropic position which was this weird normalization now it turns out that in the lava order this matrix is less than the one the edges so just the variance created here these baby laplacian are less than the corresponding edges of the vertex that we eliminated and this is actually not hard to see because you get this matrix by pulling out a PSD rank one term from this matrix okay but now I solved the problem of the fact that we're changing the structure of the graph because we went from having variance in the new structure to getting variance in the old structure now comes the other important part I picked random vertices but it's easy to think about what does a random vertex and then getting bearings here look like because I'm getting variants of edges of a vertex equal to edges of a vertex but when I sum that over random vertices I just get the whole graph because I can pick either end of an edge right there are two ways that I can get the edge pick one and pick the other and I'm averaging over all these different ways of getting the edges so I get each edge twice and I divide through by the number of vertices okay so the whole variance adds up to the whole previous graph in isotropic position divided by the number of vertices there are some fudging constant but in isotropic position the laplacian is just identity so the trick is that we can really understand what the predictable quadratic variation is doing so we can control the variance okay and now we just sum up the variance let's skip this we sum up the van's over different rounds and we're dividing by number of vertices in each round and getting a small matrix otherwise the first elimination we have n vertices so we're dividing by n the second we have n minus 1 vertices and so on so this one over number vertices just gives us a login because it adds up to a harmonic sum well right there's not a lot of variance it's the whole story so the whole thing works now summary martingales are a natural fit for algorithmic analysis understanding the predictable quadratic variation or some other similar object may be is key and I really think you should be able to find other cases of this we have some cases so there was an unlined row sampling by Cohen Moscow and pecho key we have this approximate Gaussian elimination that I talked about today we've also been using it for semi streaming graph sparse efficient for solving directed Laplace ian's for proving tration bounds for a negatively dependent matrix random variables so you know go home and take out your martingale s and do stuff with them thank you questions so bad elimination ordering is like suppose even this is a bit silly but suppose the graph is just a ring or like the global structure looks like a ring then eliminating in order around the ring is a bad idea because you're sort of going to be accumulating variance in a in a bad way yeah so we have some cases of this but not a whole lot so we can generalize it to something called connection Laplace Ian's we can also generalize it to directed laplacian which basically give you ways of like basically finding any information about a random walk in a directed graph in nearly linear time so I think that's a very cool primitive but whether you can do it for you know everything we don't know and maybe don't think so but that's a very big question yeah so all the other cases that I mentioned connection laplacian and directed laplacian also have this property that they're class that are closed under Gaussian elimination so you do some Gaussian elimination you get back another one of that class if you find another class like that you can probably do the same thing I mean it's also true for positive definite matrices but something else then goes wrong so and the fact that these matrices you know always have a decomposition into small parts more questions all right lunch time let's think [Applause]", "3y3E2sMVKk0": "Hello and welcome to the course on Scalable\nData Science. My name is Anirban I am from IIT, Gandhinagar and today we will study Leverage\nScore and its Applications. So, until now we have looked at different\napplications of the random projection techniques and various linear algebraic problems right.\nAnd some of these problems some examples of the things that we have studied have been\nnumber one application of random projection to approximate PCA to approximate matrix multiplication,\nwe have looked at a particular decomposition low rank decomposition called a QB decomposition.\nWe have also looked at how to apply random projection in order to do l two regression\nefficiently. So, the pros of these of these techniques\nhave been that that lot of them are numerically stable, and hence practical, they are also\ncomputationally efficient theoretically as well as practically, but the cons is that\nis that when you are computing random projection right the result might not always be very\ninterpretable right. For instance imagine that we started with the matrix of documents\nversus words right and then we sort of took a projection of it, some random projection\nof it on the column space and we have some documents versus projected directions right.\nNow these projected directions are linear combinations of words right including both\npositive and negative weights, because the random number is going to have; I mean, the\nrandom variable is going to take both positive and negative values. And therefore, you are\ngoing to combine words the word counts by using coefficients that are both positive\nand negative. And this does not have any direct interpretable\nmeaning right because what does what does it really mean to take combinations of words;\nthat is point number 1. Point number 2 is that I mean see each of the documents itself\nwas fairly sparse right, but once we take linear combinations of the document I mean\nit is possible that each document had 10 words or an average. But once we take linear combinations\nbecause the linear combinations are dense, then a is essential at dense random projection,\nthe matrix that we have at the end is fairly dense right. And this is the same issue that\nwe had seen in the case of the of the dense random projection. And so, it is possible\nthat the space taken by the by this by this dense projection although its low dimensional\nis actually much more than the space taken by the original sparse or document matrix.\nRight and the and there is there is nothing special about what documents this could happen\nfor any sparse data. That once you convert that is that once you use random projection\nright or once you or once we get a singular value decomposition of it, right the space\ntaken even if we do a low rank decomposition of it the space taken could be much more than\nthe space needed to represent original data, because a either the random projection or\nthe low rank decomposition for instance the QB or the SVD. They are all dense I mean,\nthey are essentially dense there is no sparsity constraint on them. So, let us keep these\ntwo issues in mind as we as we go through this lecture. So, in order to handle these two issues, let\nus look at let us think a little bit about the relation between projection and sampling\nright. So, until now we have really been talking about projections of vectors right.\nSo, what is projection? Projection is essentially nothing, but a linear combination. So, so\nwhat if we wanted to choose actual data points instead of taking linear combinations of data\npoints, can we do that right? And we again had faced this issue when we were trying to\nspeed up random projections, and then we resort it to something like a sampling matrix and\nthen we saw that we could not always do this right we could not always do this while retaining\nthe properties of the random projection. So, here again will welcome back to that question\nin a slightly different setting, but related setting. And this actually is part of a very\nbroad I mean a much broader question in algorithms right and the broader question being is that\ngiven an optimization problem right. Can we create a smaller data set? And we want and\nwe are solving the optimization problem over a data set x right that suppose we have we\nhave a problem we have some function f, and we want to get let us say the max of summation\nf i right. Where the max over f and the summation is\nover all xi belongs to x correct and my intent is to get the f that maximizes this quantity\nok. So, the question is that instead of instead of taking the sum if the data set x is very\nlarge, instead of taking the sum over all points in the data set that can I find a much\nsmaller data set let me call that c right; such that I take this sum over that smaller\ndata set and maybe I sort of adding a weight to I sort of multiply each of these by a weight\nright because maybe I mean; for full generality let us just I mean put in a weight for each\npoint and instead of taking this sum over the entire data set can I just do the sum\ncompute the sum over the set c. So, this is typically known as a core set\nand there is a lot of literature on this from the computational geometry perspective ok.\nThe techniques are mostly orthogonal to the ones that we study have studied in this randomized\nnumerically linear algebra techniques, but in a bunch of recent papers these directions\nare starting to converge. So, we would not look at the problem of corsets from the. So,\nwe will look at the problem of corsets again from this rand and from this randomized numerical\nlinear algebra setting and specifically we will look at two settings linear regression\nand matrix factorization ok. And we look at a one specific way of creating these corsets\nby using a sampling technique known as leverage core sampling ok. So, let us sort of recall what linear regression\nthe setting of linear regression that we had, that we have we are mostly talking about over\nconstrained setting that is n is bigger than d. So, we have a set n your matrix A of that\nis of size n by d and were looking to find a vector x right; such that the error minimize\nfor all possible x A x minus b 2 norm this is what we looking to minimize.\nSo, at this point we are not putting any constraint or any regularization of them on x right all\nthat is all that has been done, but we are not covering that in this course and what\nwell be doing in order to solve this problem more efficiently. We had our projection matrix\nomega right. And we were sort of we were shortening the matrix A converting it into a smaller\nmatrix by multiply pre multiplying it with omega, and then we also find out a smaller\nversion of b by pre multiplying it with omega and then we solve the smaller problem this\nis what we were doing right. So, now, the question is that, that instead\nof projection can I replace omega by a sampling matrix right? So, what is replacing omega\nby A sampling matrix mean? What it means is that that the effect the omega A right should\nreally be a matrix where each row in omega A right really comes from one of the rows\nin A and this should happen for all the rows. Each when omega I should really come from\none of the rows from A right and typically we we tend to do sampling with replacement,\nbecause that is much easier to analyse ah, but there is nothing sacrosanct about it.\nAnd some and that is the same and that the same for each element in b ok.\nSo and why would this be helpful? Ok one of the reasons is; obviously, clear that if each\nrow in A is really just a scaled version. So, this potential is scaling if each row\nin A is really just a scale if its omega A is really just a scale version of the of one\nof the rows in A, then the size taken to store omega A cannot be much more than the size\ntaken to store a right. So, the question is that does such omega really\nexist ok? Because remember what what the property that we what was the property that we needed\nout of omega? We needed the fact that the rank of omega A right; we need we needed the\nfact that the rank of omega A. It is really equal to the rank of A right. In fact, we\nneed it a little bit stronger than this what we needed was that with mega preserves the\nomega preserves the singular value structure of U of A right which is the left singular\nvector of which are the left singular vectors of a ok, but let us even forget that let us\neven look at the smaller constraint that we want rank of omega A to equal the rank of\nA and then it is not entirely clear right that we can actually find such an omega.\nSo, of course, here is an easy solution right that if we were allowed to look at A and if\nwe were allowed to sort of find out d linearly independent rows of A right, then we might\nbe able to do this well easily. But doing that is really again to just solving this\nproblem this regression problem by itself right. So, the question is that can I quickly\nfind out an omega in a manner that is computationally much less efficient, that is computationally\nmuch more efficient than actually solving the regression problem and can I still guarantee\nthis ok. So, the next question and one that we just\nmentioned before is that of finding a CX decomposition. So, what is the CX decomposition? Here we\nare saying that suppose we have users versus movies and what I really want is a low rank\ndecomposition of this matrix? Because, we believe that a low rank decomposition\nwill expose the different genres or the topics that the users. And the move and the movies\nthat the users are interested in and that movies belong to right. So, the low rank decomposition\nis going to put users and movies in the same in the same plane right and then and then\nwe can use this representation this joint representation in order to do recommendation\nand so, on and so, forth right. So, but now here interestingly what we want\nis that, we want this low rank decomposition in a manner that the I mean instead of instead\nof a general QB decomposition, we want it to be a form such that we first get to choose\na set of columns C that are C I mean each of the columns needs to belong to one of the\neach of the columns of C has to come from one of the columns of A modulo some scaling\nright. So, C is really a subset of the columns of A and I still want to guarantee right that\nI can find such an C and such an X and the corresponding x.\nSuch that the error A minus CX frobenius is within a small bound a small factor bound\nof A minus A k frobenius which is the optimal error ok. And this approximation really I\nmean that I mean theoretically it should be within some small 1 plus minus epsilon and\nof course, epsilon is going to control the size of the size of c.\nSo, does such a C really exist even that is not entirely clear. However, if we could do\nthis then we have convinced ourselves that this will be useful in a lot of machine learning\napplications right. Because now the columns of C are really interpretable in addition\nto not being in addition to having a smaller foot that of a the columns of C are also interpretable\nbecause they are actually in this case for instance they are actually examples of movies\nthemselves ok. So, here is one possible way right. So, we\nhave remember we have looked at the problem of matrix multiplication, that we have that\nwe are trying to I I mean in order to multiply a times b two matrixes A times B, we created\ntwo samples a sample A from sample C from A and A sample R from B using some sort of\na length square sampling right. In particular one of the one of the special cases that we\nlooked at is as follows when B equal to A transpose right. In the case that B equal\nto A transpose, then the length square sampling that people interested in looked turned out\nto be the following right; that I choose C that I create the matrix C. So, the matrix\nR in a uv equal to n transpose then the matrix R is a is again equal to c transpose right.\nSo, I need to create only the matrix C and one of the ways we discussed in order to create\na matrix C was to choose the ith column of Awith probability that is proportional to\nthe length square of the ith column right. So, basically every column of c right is a\nIID random variable right and that IID random variable takes the value of the of the ith\ncolumn of a with probability proportional with probability pi. So, it is a and it takes\na for instance it takes value A 1 it takes value A 1 with probability A star 1 with probability\nb 1 it takes value a star two with probability p 2 and so, on and so, forth ok. And we are\nwhat we discussed was that you know I mean that once we choose this values of pi we get\nthis particular approximation right. That if the happen to choose C columns right im\noverloading the the notation C im using it both as a set and as and as a matrix. But\nif I if I happen to choose C columns in small c columns in C, then I get the guarantee that\nA transpose minus CC transverse frobenius is not very big.\nIn the sense that it is frobenius sum of a square by square root c and this particular\nsampling itself and this particular decomposition itself can then be used to obtain low rank\napproximation and CX decompositions with additive error with additive error. And the additive\nerror that we get is really the one that comes out here and in each of this cases basically\nthe idea is that you choose a set of columns C using this particular sampling technique\nright and then you do a low rank decomposition of that set of columns right or you get a\nCX decomposition using that set of columns. And the bound on the error that you get in\nthe data t bounded error that you get in these two cases is comes really from the bound that\nwe have calculated here and we are not going to go into the details of this ok. Because what we are interested in is today\nis known as the leverage scores. So, let me define the leverage scores first ok.\nSo, let us take a matrix a let us consider the setting when n is bigger than d right\nthat the number of rows is bigger than the number of columns, and let us take an orthonormal\ndecomposition of the set set of columns right. So, let u be an orthogonal basis of the set\nof columns and let me write A as u times x right. So, use of size n by d without loss\nof generality assume that A has rank d. So, in that case I defined the I defined the\nith leverage score the leverage score after of the ith row right, to be the rho norm the\nnorm of the ith row of U square right. So, I take the so, if I take ith row of U and\nI take the l two square norm of that right and in order to make it a probability I divide\nit by the sum of these norms right, which is essentially the frobenius norm of U ok.\nSo, this is a is a quantity between 0 and 1 and the sum of these equals o1 exactly ok.\nAnd and this is known as the leverage score of the ith row or we will call it the row\naverage scores. So, so one of the things that we will do is that what we will typically\nonce we have defined this leverage scores, we will typically create a sample of rows\nusing the leverage scores li with replacement ok. So, just like we were using the probabilities\npi in the previous in the previous slide, we I mean here instead of the probabilities\npi will use this leverage score probabilities li right and will create and we will do sampling\nwith replacement with I mean by taking these probabilities ok. So, before we show its use let us look at\na few of its properties see. So, one of the things should be fairly obvious the row leverage\nscores only makes sense that the number of rows is equal is greater than the number of\ncolumns. Because else the form of u looks as follows\nU looks like I mean U looks like because if number of a if then, if a looks like this\nnumber of if the number of columns is bigger than the number of rows right then U is a\nsquare matrix right U is a d by d orthonormal matrix and in that case all the row leverage\nscores all the row norms of U are all 1 right. So, in that case it does not make sense to\ntalk about it and it sort of is intuitive because if the number of columns is bigger,\nthen you should not really be talking about sampling rows you should really be talking\nabout sampling columns ok. So, and also an important quantity and an\nimportant thing to realize is that, while we define the leverage scores in terms of\nin terms of a specific ortho[normal]- I mean an arbitrary orthonormal basis the actual\ndefinition is independent of the orthonormal basis that you choose right. The in order to say this let us just look\nat this calculation, that suppose Q and u are two different orthonormal basis of the\nof the columns of the columns of A ok. And because there are two different or thermal\nbases of the columns of A, then there exists some rotation matrix R right which is I mean\nrotation which means an orthonormal matrix d by the orthonormal matrix R such that Q\nequal to U times R right what; that means, is that, the ith row of Q is really equal\nto the I at row of u times R. So, the norm of the of the two norm of the\nith row Q equals the two norm of the ith row of U right and this is true for any two basis\nany two orthonormal basis Q and u and therefore, and because this is what we define as the\nleverage score as the and this divided by the normalization, and remember that the normalization\nthe frobenius norm of u is always d the frobenius norm squared always d and that is independent\nof the basis that you choose. Because both th enumerator and the denominator are independent\nof the basis that you choose an leverage score itself is independent of the basis.\n. So,. So, here is I mean, once you define the\nleverage score here is going to be my algorithm for linear regression. What we will do is\nthat we will create the matrix will create the matrix omega as follows. So, omega is\ngoing to be s by n matrix right and in order to create the in order to create that the\ntth row we will choose we will choose I with probability li with replacement. So, therefore,.\nSo,. So, and and suppose for the tth row, we happen to choose the we happen to choose\nthe I specific I right. So, therefore, for tth row of omega will place zero everywhere\nelse,except for the except for the tth position and in that position we will place we will\nplace one over c square root of li ok. Basically we are normalizing by the by the o[ne]- by\nthe square root of the probability that we chose it, with and then we solve this.. And\nonce we have defined this particular omega we use it as we were using it before basically\nwe solve omega Ax minus omega b and then we return the result as my approximation. In order to do this CX decomposition here\nis what we will do right? We will first we will first decompose A as U sigma V transpose\nU sigma V transpose right and then we will take the top rank a U and V. So, so it will\nbe a thin SVD and now what well do? Well define a lis because we because now we want to sample\ncolumns of a we will look at V transpose instead of instead of U ok. And we will and we will\ntake the rows squared length of V k or the of the column square length of the of the\nVk transpose and that and that will give me the corresponding leverage scores right.\nAnd we will do exactly the same; that will pick column I with probability in order to\ndefine C we will pick column I with probability li normalize it by by 1 over square root of\nli and add it to the to the tth column of as the th column of c and this will keep on\ndoing with replacement which means that the same column of a can be pick multiple times\nand we put in C and then we will define X to be the pseudo inverse of C times A that\nis it ok. And what and what will be able to show is\nthat both in the I mean both in the linear regression case that, we saw before and in\nthe in the in the CX decomposition case the that were seeing now is that well get 1 plus\nepsilon approximation right. So, of course, this I mean the epsilon that you get you have\nto be in order to show such a result the epsilon the that you get has to figure in the number\nof samples that you choose right. So, so only when the number of samples is\nright I mean when number of samples depends on on some on this on this epsilon and delta,\nyou will be able to show that with probability 1 minus delta you get a you get a guarantee\nthat a minus x is is not more than 1 plus epsilon of times the optimal error, and will\nbe able to get a similar guarantee for linear regression. So, just to give some intuition as to why\nleverage scores work; so, all proofs that show leverage scores have basically of the\nfollowing form right then consider U which is which is an orthonormal basis of a right\nand therefore, and because of that U transpose U is actually the d by d identity matrix.\nIt is not very hard to see and these are the leverage scores right and these are the leverage\nscores that come out. So, using this leverage scores wego from U to U tilde right. So, so\nusing the leverage the sampling using the leverage scores like I described in the previous\ntwo slides as well as using normalization we go from U to U tilde right. And suppose we choose suppose the number of\nsamples that we choose that is the that is the number of rows of U tilde is some r where\nr is at least d log d by delta by epsilon square. So, this epsilon is the error is my\nerror parameter again and delta is my confidence. Suppose we have chosen so, many rows so, many\nsamples from u right. In that case well be able to show that with probability 1 minus\ndelta right the u transpose u remember u transpose u is really the identity matrix t. So, u transpose\nU minus the U tilde transpose U tilde the two norm the spectral norm of this matrix\nis less than epsilon ok. So, let me interpret what; that means,. So,\nwhat; that means, first of all is that see the I mean the singular values of I are all\n1 right and therefore, what this means is that the singular values of U tilde transpose\nU tilde lie between. In fact, the singular values of U tilde transpose U tilde lie between\none 1 minus epsilon and 1 plus epsilon, which means the singular values of u lie between\nsquare root of 1 minus epsilon. And square root of 1 plus epsilon is specifically\nwhat it means is that U tilde is full rank because all the singular values line this\nline in this interval right. And what this will allow us to do later is in the proofs\ninstead it allow it allows us to bound the norm]the pseudo in the norm of the pseudo\ninverse of a right and this and this pushes helps us push the proof through ok. .\nSo,. So, how do we estimate leverage scores right? Because if you were sort of paying\nattention, in order to I mean sample using the leverage scores we I in every case we\nneeded to get we needed actually needed to get the singular value decomposition right\nand as we saw that singular value decomposition is fairly is fairly expensive.\nIn fact, it is at least as expensive as, as the linear regression is doing solving the\nlinear regression and also I mean getting a CX approximation. So, the CX approximation\nis not so, obvious because it is not clear that such a without the without the proof\nwithout the without our proof using the similar using the without our proof using the leverage\nscores. Leverage scores it was not even clear that\nthe CX decomposition existed right with the small case. So, therefore, for the CX decomposition\nwe can still say that the leverage score has its use, but for the linear regression it\nis not entirely clear; however, luckily it is easy to estimate leverage scores although,\nI mean while we cannot get an exact value of it we can estimate it to some bound. And\nit turns out that the entire analysis can work if we can approximate it to some particular\nbound and. In fact, here is a here is an algorithm for approximating it and. So, first it is\ngoing to take time I mean which is proportional to n nb by epsilon times log factors instead\nof the instead of the nd square and its going to use the randomized Hadamard transformations\nthat we have seen right basically has the idea that we have a right. So, so first we\ntransform a using using the using randomizedha Hadamard transformation matrix right and then\nwe get a much smaller a much smaller a right ok.\nSo, we get a which is s by; so, a is n by d and using using a PHD matrix omega A we\nget omega which is s by n. So, this is omega A right and then we do a QR decomposition\nof this tI am calling it p how right till and therefore, I will rename it to say that\nthis is p ok. So, so PA is of size s by n and then, we do a QR decomposition of PA which\nis actually cheap now because PA is of size only s by n right and then we get see the\npoint is that the r that. We get out here is a very good approximation of the r in that\nwe could have gotten if we had done the QR decomposition of the original a.\nSo, therefore, once we look at the matrix AR inverse right that is a very good approximation\nto that is a very good approximation to an orthogonal basis of A right. So, AR inverse\nis a very good approximation to the u which is a orthogonal basis of A right. And therefore,\nby using the matrix AR inverse in fact, we can do another I mean in fact, because we\nonly need the length of the rows of AR inverse we can we can do another trick with the random\nprojection just for speed, but basically using the matrix AR inverse we can get a very good\napproximation of that leverage scores. So, just to summarize that in this in this\ntechnique,I mean in this lecture; we discussed the role of sampling versus projection and\nwe discussed that sampling is more preferable to projection if you are interested in preserving\nthings like the sparsity of the matrix, the total memory footprint the interpretability\nas well as its often useful in downstream machine learning applications. We did see\nat least one interesting I mean basically two interesting ways one is the length square\nsampling and the other is the leverage score sampling, in which we are doing length square\nsampling with respect to the an orthogonal basis right. And we saw that at this particular\nleverage scored has very interesting applications in solving linear regression in solving linear\nregression as well as getting a CX decomposition approximately. There is an extension of leverage\nscores to other norms ah; however, the there are while that theoretical results the I mean\nthe extension of leverage scores to other norms is not really very practical yet right. Just for the just to give you some references\nlot of the slides as I have been gotten from Michael Mahoney and petros stock from their\nboot camp that you can Google, that you can obtain by Goggling this. And it is really\na very nicely set of lectures and we have been following this lecture notes by both\nof them and. Thank you.", "JngdaWe3-gg": "welcome to remember something a trace of the drinking coffee four hours of my study imagine anyway the American house was at the end of saris three and that was right next door to the race and anywhere with numerical analysts and more graduating from Stanford professor April in advance of these Corral institutions hey yo MIT and Cornell authority master Oxford and is now head of the America analysis gravely and and something called the mathematics mr. wonderful new building just like some of the best engineering anyway- distinguished for having invented something called Sweden spectra for normal nature states the present with more than no and symmetric seven painted something profound of amazing set of MATLAB routines for dealing with not just with matrices and vectors and they mainly just approximate in ten years functions and operators and tell me oh when the first Leslie talks price of numerical analysis this is an engineer in the unit is a power of the Royal Society and [Applause] and in fact this is where we have seminars amazing people old friends of the to be here everything in the engineering quad area is completely new as to Noah's kind of astonishing how Stanford it feels as if this amazing University got to magically double the reason I'm here besides visiting San Fran is that my children who are pretty much English I've decided that the world is based in the Bay Area they live the square root of two blocks apart from each other in the Mission District and also apparently the cube root square root of 3 or 5 Estella Pyfrom Zuckerberg we will never get them to either this is the center of the world which is partly horrible I find the energy here is kind of frightening the sense of responsibility to go off and change the world is it's always vibrating here I'm sure you will all go to change the world right so this is a unusual talk in that it really does have 10 pieces they're all in the area of quadrature of course not everybody will follow but you can start again when we reach an 8 in quadrature means algorithms for integrating functions and this is not officially my corner of numerical analysis but I found that in the last 10 or 15 years I've been doing a number of things in this area and sort of by accident I've discovered that it is one of my interests so I wanted to put together a bunch of things most of them have connected with my research over the years though only the lab - on the let's play enough things up to now there is a handout here which present a beauty you might ask them back cuz I'm sure their recent arrivals so I'll just spend a few minutes on each of these topics to begin with these are the sender settings for a quadrature formula so in one dimension you imagine an interval over by default the unit interval we approximate that by a linear combination of function values with appropriate weight and the Gauss quadrature Newton Cotes quadrature all these familiar Simpsons rule now I want to tell you some things that have real mathematical substance and of course I'm doing it so quickly that a lot of it will go by quickly and that's why there's a handout to remind you that each of these kind of ways really does have some potential not but also I think them because I think they're interesting to do not the obvious subjects focus on okay so let's begin the first of the ten points is that Gauss quadrature which is the formula everybody's heard of Gauss quadrature converges geometrically if you have an integrand f the element I assume you all know what an analytic function is it's got converging Taylor series geometrically needed synonym for that is linearly it just means well here's what it means like that so as you increase the degree of the formula on a large scale the errors go down like straight line or FAFSA so that's a theorem and it applies to every analytic function there's an example of an analytic function in this theorem has a remarkable status I believe it should be in every numerical analysis book because every numerical analysis book does mention accounts positive in fact it seems to be in no numerical what so that at least I haven't found one why well partly because maybe some people don't know this result but probably mostly because people assume that students are scared of the concept of an analytic function that's regarded as a constant well that's not as it should this is the most basic idea in mathematics students should be expected to the length of this theorem and from now on let me spend one minute running through a proof so one way you can explain the result is to note that if you have an analytic function on the interval then it has to be analytic in the neighborhood of an integral of the interval and in particular has to be analytic and some bit lips with focus at minus 1 and 1 and those ellipses turn out to determine everything in this business the theorem is that the error in Gauss quadrature decreases geometrically and the Factor involve the geometric factor is a parameter that tells you how big the elliptic row so the bigger the ellipse of analyticity the faster the convergence and you can prove this by looking at chebyshev series which i know without meaning for some of you but others you can show that timoshev coefficients decrease geometrically if your function is bounded of X and then five simple estimates the resulting law so it's a pretty simple theorem and of course is important because it's a starting point a big function of geometric unity so we're already descended way through the next observation is actually strangely maybe more familiar because people tend to rediscover it for themselves and that is the trapezoidal rule where you just add up equally spaced examples of the function is spectacularly efficient accurate if your function happens to be periodic and that limit so if so does means you also get geometric convergence of the equally spaced trapezoidal rule for a periodic analytic function so that's a beautiful useful fact people use this all the time whenever you have a periodic function the chances are adding up equally spaced samples is the best way so here's a classic example of a periodic integral which is non-trivial mathematically the perimeter of ellipse is of course you can write that as an integral that's periodic thanks for the symmetry you only need to sample it at a quarter of the points but that's basically a periodic interval and wasallam figured out that if you sample the integral of equally spaced boy if you get spectacular convergence that's the picture with nine points so you see with nine points we've been about but even with two points just layered up there you already get to so it's a beautiful analytic function you notice you're also losing friends I'm sure and he doesn't have to make sure you through all those big tits and you'll notice this is the world's first Easter Egg he puts a misprint in there knowing that around 200 years later somebody would give the calculation oh I worried about where this comes from for Gauss quadrature on an interval the thing is this varies on the lips but for analytic functions in a periodic the thing to do is look at a strip in the complex plane around the other so those points are the sample points on the real line and that shaded things in a strip of analyticity in the complex plan if you assume that the spacing between the points of a and the half width of the strip of a logistic is alpha then what you find is that the ever again exponential depending on the ratio of the width of a logistic to the grid cell and thanks for that 2pi especially that really is a spectacular rate of convergence this theorem was not written down by Poisson because actually I think I like to take this oh she within the office next door inventing complex analysis well what son was doing this stuff so Boston really didn't know about but nowadays it's an easy thing to prove and the first person who wrote it down was still Vegas you're still around at Providence Rhode Island they look more than 59 um there's an analogous well that result on the infinite real mind as opposed to periodic same thing essentially the error if now we use the minus two pi times the password for the script element is to be divided by the good size we're not formally speaking of an infinite sum it's very unclear who first noticed this touring certainly used it but it can more or less used it all so there's all this wonderful statistics appearing so Henry said this for example raise your hand if you ever met Hendrix it was in this room at seminars he was one of those old men so yes the Riemann hypothesis were all there's a pretty good chance that Torian would have figured that out because he used this trapezoidal rule in order to locate zeros on the critical line regrettably he was unable to find anything work by example here's the wrong a function we imagine an infinite trapezoidal rule very good convergence here is a Gaussian even more spectacular thankfully very rapid decay so with a grid size of PI over five which is like point six you know I'm not sure what they did but it's something like that you get that's why I hate Kenan others were you because it's the Mystics they're constantly measuring moments of distributions that have a Gaussian flavor okay now this one is just for fun so everybody's heard of Simpsons rule and you know that well Simpsons rule is like it's a little higher order than the trapezoidal rule on a finite interval but if you put them together you get this 2 4 2 4 2 4 type of effect now that I hope when you learned about that puzzle so let me remind you all heard of the Euler Maclaurin formula is about what to do with the trapezoidal rule if your function is not pivot and it says add a function sampled trapezoidal style but then do some endpoint correction is involved in derivatives so the other Maclaurin formula tells you how to do endpoint reductions to improve the convergence of rule or mom now the Gregory formula is the same except instead of derivatives at the end it uses finite differences and the reason for that is just the way across someone didn't know complex analysis Gregory didn't know come to us because it hasn't quite been invented this was 1670 more or less when Newton is under the formula so imagine a long string of equal sentence if you're doing Simpsons rule the weights should be a third and then 4/3 concerns for third conservators deserves well sure you get to the other man well the Gregory formula has a much more natural behavior that all the weights are one except for a few that so this has no importance in practice but it's reassuring I think to realize that you don't have to be so fancy as that citizen stuff it's perfectly okay to do things in a more obvious fashion so why is it the Gregory formula famous what is it that we both are sensitive I don't fully know the answer but it seems to be a ratio 4.75 so here we have the Simpsons rule in the analogous orbit regular formula but there's verging on a log-log scale with the nation the fourth era but there's a gap between the two curves ninety number four it's strange how these fashions go so Simpsons rule is in every Americans there's always a chapter in quadrature it always means in the world Gregory is as it used to be in a few books two generations ago but it's sort of faded out whatever book you've got on your shoulders probably doesn't mention I'm not saying it should be definite like without it but I'm always interested in intellectual foundations as how odd to tell this story and about algorithms for computing Gauss quadrature inevitably so let me remind you that Gauss quadrature is the optimum quadrature if you have n nodes and then weights and what's special here is that the points are distributed optimum so that it integrates as high as possible degree polynomials so it's the gold standard of quadrature and it was invented by Gauss it really wasn't bad he was 37 years old in 1814 and those of us in the American occupation learn about it from a guy called Colin published a paper when he was 37 galavan welsh an amazing and beautiful result they showed that you could compute these nodes and weights by setting up a try to handle matrix and doing an eigenvalue problem this is 1969 the eigenvalue algorithms that we know and love we're just being invented pretty much so it was a beautiful early application right so the amount of work for their algorithm is o of N squared it would be uncute for a dense matrix it's N squared because of the tri diagonal structure the eigenvalues are the nose of the Gauss quadrature formula and the first component of the eigenvectors squared and doubled or something gives you the top weights it's N squared in principle it's actually n cubed in practice because everybody writes it did not mind about that it doesn't notice the Tri diagonal structure so for the last 40 years in practice this beautiful algorithm has the third algorithm for motels now gallop died in 2007 that's the year that the O of M not really started appearing thank way through you and Rockland was the first many of us didn't quite hear that they didn't make a fuss about it they published it in a more general context but then in more recent years a bunch of people have been getting faster and faster all the papers are published in the very Journal of the team founded when I was a graduate student so Jean died believing that Gauss quadrature would never be due a holter and bigger than a thousand now of course there aren't very many applications where you need but he would have thought that in principle it wasn't even something you should effectively with you but in fact much bigger numbers are easy so I'm only going to do one content it's not easy for me to give a talk and only do one so it's fun we have a command leg for needs they extend from the Sun and what it does is compute roots of a Legendre polynomial so those are the three zeroes of the third order Legendre polynomial so the nodes of the three-point Gauss quadrature if you also want the weights you could say something like s comma W equal to leg points three so there you have the nodes and weights of the Gauss quadrature so if I wanted to integrate you know e to the X thing else make a punch I could now say W times F of s that is the Gauss quadrature approximation to the integral of e to the X on the unit intervals based on three points I'm sure most of you immediately know whether that's accurate or not so let's see the exact result should be even the one - and that's only three points well let me show you the speed moving further down so what I'll do is I'll say tick and then I'll say s W equals Lake points of three talk now you can never measure so this already is something that you all have never saw anything like that and he would have thought it was not feasible so three million points in how many such people so they open these books all talk about Ross quadrature and a vendor they say well it is the printed formula unfortunately computing these nodes and waves of the challenge know about you cookie probably because of my early experiments okay serious word it's rare that you have an application that needs the California up something new but of course in our business we build tools that are as powerful and reliable as possible just the way you don't normally need 60 digits of accuracy when you're something an engineering problem but I Triple E wonderful so now there are important ok the fifth one it's getting more serious about the mathematics this is very important and beautiful leads in many directions and have been an inspiration for my research this comes back to council so to remind you we have an integral and then we have a quadrature formula approximation to beautiful now it turns out it both of those things can be represented by a contour and some of you are comfortable with this but here's the idea let's look first at the quadrature approximation which is this finite sum now whenever you have a linear combination of function of elements assuming the function is analytic well a function value if the resident of an analytic function if it has a simple pole so a linear combination of function values is a sum of weighted residues how do you get a summer waiting residue into a country over a comp you are going around your nose if you multiply your function by a rational function R whose residues are W sub K and homes are XM and then compute the contour integrals around all those nodes you get the quadrature across maybe so F has to be analytic for this to work but if it is this is a contrary tuple representation well the true indigo also can be represented by a pea country same contour again we imagine we've got the unit interval and our Titans analytic so we draw a contour around the interval the exact integral is equal to the contour integral of f times this function and this function is now belong to C so this is what we want to calculate the true integral that's what we do calculate and the amazing thing is mathematically they're both contour intervals which means that to find out how accurate our approximation is all we need to know is how close is 5/2 power and if Phi is very close to our in a region of the Z plane where am i living then your approximate integral so this is fancy mathematics that most of you probably haven't seen and yet it actually is the basis of surprisingly much surprisingly many things and it really does go back to gaps even expressive in terms of a contour intervals but he did derive his formulas by finding rational functions through continued fractions that were optimal approximation to this function so a communication idea and then a couple of remarkable Japanese mathematicians use this in the modern era so I want to show you first of all the simplest example the simplest quadrature formula one of them would be the trapezoidal rule on the unit circle where you add up equally spaced function centers and multiply that's the rational function associated with that profitable RFC is 1 over 1 minus Z to the N and just by looking at that function we can get a sense of how these rules work so if Z is inside the unit disc then Z to the N is about 0 so R is about what if Z is outside unit this tendency to the end is about infinity so R is about 0 so that function has this bug it's absolute value you see essentially 1 inside it is inside the circle dependent 0 outside that kind of filtering effect is related to the geometric convergence temperature so that's a case where as you can see the picture more generally you don't see the picture if you're interested in things involving roots of unity so I mentioned more in akashi um they used this connection between quadrature formulas and integrals to analyze the accuracy of quadrature formulas which is sort of the lake house with you can also go the other direction suppose you have a quadrature formula you could use it to back out an approximation of rational approximation so in fact this is a good way to find rational approximation to the function do quadrature figure out what rational function is there these are the two most famous problems of rational approximation one is absolute x or square root of x on a finite interval the other is e to the X on the negative real axis in both cases using the quadrature formula you get the awesome product so here is a picture of the best approximation to e to the X on the - real axis these are error occurs and the difference between e to the X and it's approximation it's a degree I think 1414 or 6 1616 Blue means 10 to the minus 15 red means 10 to the 0 so you see this remarkable accuracy of best approximation these are the poles of the best approximation to assist any poles on the other hand rather than compute the best approximation you would publish your problem on using essentially the trapezoidal rule or the suitable curve you do all that I'm leaving out the beach north of course notice the difference in the pictures is there I just had dots because it's a best approximation that happens to have supposed here I'm doing quadrature on the curve and these are the nodes of the quadrature for me but that corresponds to poles also so by doing quadrature between at nearly now let's go back to something more elementary I've met you yells quadrature which is the perfect formula highest possible ordered accuracy using roots of Legendre polynomial hasn't known Fletch all Kurtis is the more elementary idea where instead of interpolating your function at village number points you interpolate showmanship is in fact the extrema of a chatty Chicago so it's a more elementary form of now none of the books mentioned clincial Kurtis quadrature they all mention cows virtually non management the reason it is two for one reason is that Gauss would appear to have twice the order of accuracy of code operators because when you do endpoint Gauss quadrature you exactly integrate polynomials of degree 2 n not dead whereas clench our curtain is atonia the other is that Gauss quadrature was invented in 1814 and so the books all say that Gauss quadrature has a big advantage over other formulas because it twice the rate of convergence is what actually the blue dots are Gauss quadrature and the red dots are clean chakra so blue dots based on interpolating your name in English on graph points red dots chebyshev there's no difference if your function is very analytic there's a difference but if it's not valid if there's no difference so here we have a C infinity but not a function same rate of convergence the standard wisdom would lead us to expect a clencher Curtis would converge blankets upper curve but in fact at convergence like gaps and this observation was actually negative xvi and got lost and then the beautiful paper about this I figured this out by looking at beautiful pictures of rational approximations this is errors in Gauss quadrature rational approximation compared with errors and claims Walker is rational approximation I drew these pictures and you immediately see that near the unit interval they're the same it's only far out the pair difference and if your mind is in the right place you immediately conclude that unless your function is analytic in a big region Gausman clincher Curtis will have the same but you don't need to think of that beautiful theorem basically the state of the art is nobody knows a theorem for non analytic functions that suggests that Gauss quadrature is any faster courteous and moreover in practices I know about functions that FK derivatives it's the same for various details so it remains the fact that there's a beauty of how the council quadrature but the two main thing is that the books will tell you about it are both wrong so the few things the book to tell you are first of all and it's more accurate than other formulas well that's more and secondly that unfortunately it's hard to compute and well that's monkey ok well this is different but of all the things I want to mention maybe this is that conceptually most fundamental the whole field of interpretation so virtually any formula is derived by imagining that we interpolate the data one way or another by a polynomial and then integrate that so I want to point out that that whole way of thinking is doomed and it loses you a factor of - not an important factor in practice but an important one conception itself well I said this I guess all the standard formulas are based on interpolate to the polynomial and integrate the eternity the thing about polynomials is it sounds odd but they're really highly non uniform so if you have the unit internet say and you want to approximate a function the public O'Neill's are much more powerful near the end than a minute now there's one of these ellipses that comes up when you analyze father now let's this is a region in which we're assuming a function is out of it and if you want to know how accurate are the nominal proximation that you have to use these regions they really are inevitable for polynomial approximation let's think what it means to require a function to be analytic in that ellipse at 0 the function has to have a Tanger series that converges in this disk the radius of convergence at minus 1 it has type of Taylor series that converges in distance microscopic rate so analyticity in an ellipse is a completely non uniform requirement that demands much more smoothness of the function in the middle than near the ends anything you do with polynomials is implicitly making this kind of assumption of your function so when people say that Gauss is optimal that means it integrates exactly polynomials of the high possible degree well that may have little to do with what an engineer would care about them the natural engineering point of view would be I have a function it's sort of equally smooth everywhere what's the optimal formula for a function that's equally smooth everywhere polynomials will not engine so there are many ways of getting on this the one I happen to like is using a formal math term your lips into it infinite strip that tourney's Gauss or sentence I've heard his points into much more evenly spaced points and by doing these scripts you can get convergence at a rate up to PI over 2 of course I see you largely eliminate the clustering of cows or potential purchase points in one dimension it doesn't matter in practice but in higher dimensions maybe it does so to Pope's age isn't it in between you know eight is kind of between one and and 58 is the kind of a dimension where people might really use these things in each direction in order to compute well if you use these more uniform points rather than gauss points you're beginning to get an improvement that's pretty significant you might need 37 times fewer samples to get the same I still don't play this is a very important practice maybe it's a little important in practice but it's fundamental pictures from my book meet one of the things check fun does you can draw words in it in it Oh theorem for example here's a good engineer's assumption about our reasonable expenses suppose at the diner with a confounded on the unit interval and in an epsilon neighborhood of very uniform assumption and at for thus they are the Holy One at fun to be recently small well this is the rate of convergence you get for Gauss quadrature and this is the rate you get for transplanted that's water to actually slightly better than so there really is a provable 50% improvement um when you dispense with polynomials and Transplant approach the reason for 3 over 2 is the the theoretical optimum is PI over 2 which of the little flavors in three okay so now this one is for the linear algebra people of the audience if you've read my essay complex analysis as president of Siam I wrote the essay because I knew one of them was about the side the climate complex and the question is what replaced it and I think what replaced it is linear home because not useless Edo but sadly nobody really study this complex variables or nothing my mama complex variables do give you some of the best methods in linear approach so I wanted to mention that here if you have a function of a matrix or an operator F of a then the basic definition of it to a mathematician what does it mean a function of a matrix well basically it means that come to our integral around the spectrum so see if a contour in the complex plane that imposes the eigen value matrix this is the cleanest mathematical definition of F of F so numerically it's an outstanding way sometimes to compute because contours in the complex plane are of course periodic almost by definition and they can be taken back who so the trapezoidal rule and variations and in a typical application 10 or 20 points is enough to get you six or ten digits of vectors so what does that mean from a linear algebra point of view well if you want to sample that function that means solving a system of equations involving Z minus a so you're solving a system of equations but as a is sparse or has other structure you maybe have fast ways to do so the idea here is to reduce the well understood problem of the new linear songs sorry to reduce the difficult problem of ever thing do the Wellman your song that's the wrath of a very analogous methods have come into play for finding eigenvalues which are simply poles of this simple resultant and the vegetable mane of that business is fixed that's the main that Eric policia UMass gives through his methods of this labor similar work by sufferance to you or any other pen the idea is to find eigenvalues of matrix by regarding them as poles of a function and you find those poles by doing a country ok so that's those are the eight topics that are in my past sometimes one before and then the final two are things that were working on so this has to do with perturb you points and I guess I'd like to tell you the story of where I got it so oh it's been a funny theme of my last three years but I'm grave a Daman and I were writing this paper about the trapezoidal rule and it's fast convergence and I got interested in the analogy between the trapezoidal rule exponentially converging and the Faraday cage as you know the Faraday cage phenomenon is that if you have um a bunch of wires forming a cage then there's hardly any field inside and that seemed like the same mathematics but you know how could it not be traditions oh that periodic points there's a field out there something exponential is going on so that must be how Faraday cages shield electrostatic and electromagnetic fields but the more I thought about it the more directions this went and one of them was um the trapezoidal rule dependent the super convergence of it depends having equally spaced points does the Faraday cage depend on having perfectly spaced wires well I'll cope not of course they're not going to be perfectly safe in any actually stage and of course obviously physically open it possible depend on if you have reasonably well spaced things of course a Faraday cage will be fine so that got me thinking about quadrature analyzers does the trapezoidal rule depend on having the points well the answer is no if you view it right so here we have the trapezoidal rule for a periodic function we have one do twelve points or something and the error in the quadrature is like 10-15 there's an underlying approximation there which the trapezoidal rule is equivalent to interpolating your periodic function by sines and cosines trigonometric interpolation and then integrate the interval it turns out the accuracy of that encirclement is sort of the square root so it's about 60 that's quite reasonable suppose we perturb the points so we can think of this as our Faraday cage that hasn't been manufactured will we lose the geometric convergence or mark well not if we think of the trapezoidal rule properly forget trapezoids think of the trapezoidal rule in the deeper sense FB trigonometric interpolation followed by integration of the integral so here the blue curve is the trigonometric interpolation to be prepared and once we integrate that and we find sure enough that the quality which is now square root we no longer have that double accuracy but it's still exponentially convergences comfortable to the approximation models so sure enough as you would expect for turn points are no problem so let me start looking for the literature and their images let me remind you of two facts I'd mention if you have an analytic function you get geometric convergence I guess I haven't quite mentioned that if you have a continuous might can you get convergence so what happens if you perturb the points and the natural setting is to imagine that each point can be returned a certain fraction of the way to the next point and that fraction is outside so we have our points we're going to allow each one to be preserved up to a fraction also of the distance for the next one if alpha is less than 1/2 and the points are staying separate and here's what you find we don't know a single paper in the quadrature that return a considers displacement in the approximation Theory literature you find that you get they claim that you get convergence even as the points coalesce and that's true you know one of these theoretically non-us citizens if you have coalescing points together spectacularly on statement methods but it's true mathematically with no rounding so let me give you a 30-second summary of what's happening theory is in the fundamental question of approximation theory is if you have a function with a certain smoothness how fast the certain approximation converge the approximate a smooth function by polynomials a faster than unity approximate a periodic function by sine the cosine how faster they convert the fundamental theorem a question of Sakhalin theory is what class of functions can be recovered exactly so for example if you have a function that's an entire function band-limited to how many set the points we need the nyquist minute so the nyquist minute the prototypical pheromone centerfield it's virtually the same mathematics is almost the same and yet the focus of people is very different and I don't know I'm sure some people do in sampling Theory has a thing called codex theory which tells you that alpha equals 1 is a critical month so you perturb in your points and it's not enough to keep them from hitting each other you have to keep them they are only allowed to go a quarter of the way to each other so they have to stay very simple in order to have a read spaces now one of the reasons it sort of means energy conservation in l2 as far as I can tell one quarter has no relevance whatsoever to approximation or quadrature so I think what we are in the process of showing is that if you perturb the points the same old non robust result that the approximation serious I've mentioned is still true and the quadrature convergence in practice continues funding the right answer holds so long as the points remain separate so alpha less than 1/2 if anyone knows any literature it's only pretty because every species he does afraid it doesn't matter so you get more points then this was up on the body temperature more yeah so I think that's the first thing yeah yeah very nice I think so so then that would be like changing be square multiplayer measurements are exactly still interpolate that step Oh so that is a whole other framework yeah that's it that's why it doesn't matter because let me argue that my channel is this you know even though I still America I agree with everything you said it's true however it's that's going off in another direction there's still the question of whether these formulas that I'm talking about break down that a quarter your argument is that that's the wrong question we should be using other you can't make that sufficiently two points yeah you will at some point is I think police traces then it probably does apply but if they're from all points in that nothing the said in every space is intersecting with the brain which loose pieces as a special case but then it doesn't define who do more of us is so this one on which I've been asking everybody I've seen in the last few months if they've seen this performance the observation I want to make it so elementary but nobody seen those made it so this happens to me often and it's a little embarrassing because the first make a few remarks about a very big subject these days that touches many people the curse of dimensionality is every and world is responding in in all sorts of ways and I've listed some now in some sense if you're trying to represent an arbitrary function and really deal with it in high dimensions if arbitrary functions really can have arbitrary structure I don't think you can really however these methods I mentioned are very effective in many cases and it would seem that the reason for that is many problems that arise that involve functions that are not arbitrary in particular very often the different variables that define a function are somewhat decoupled you have a function maybe you're doing an integral in 100 dimensions there's a pretty good chance that your X Y Z and so on variables are naturally distinct which one could describe in some kind of alignment unity if you look at the literature both anchors don't the typical 7 years somebody has a low-rank compression algorithm that goes great things for their application they say we faced the curse of dimensionality to beat it we do this the implication being somehow the need to that's not true so we all owe American people know a lot about iterative methods for conjugate we all know that conjugate gradients doesn't work for the Irishman symmetric positive definite matrix you need a good condition number or good distribution of eigenvalues that's the first thing we all say about interactions they need preconditions we would never think of saying that conjugate gradients is a good method for solving problems in general that would be considered a mistake you lose points on an exam for presenting so I think this field of matrix innovation has a more solid foundation than what people are saying we're about half the papers imply even though they don't say it maybe everything so I don't have a solution that is problem I'm just doing all of us in this business to use matrix iterations as a model and try to get a little more responsible in the things we say about that's not the point this is a mantra make sure about approximation squared or cubed so we have fun we're constantly drawing pictures like this we have a function and we want to approximate the machine precision on an interval so here the function is e to the minus 100 x squared what degree polynomial do you need to approximate that to 15 degrees so whichever fine it's doing that all the time you need the 320 so indeed two people on our team that function essentially is a polynomial of degree n natori you can't do it to 15 digits with Rihanna so now let's ask what about a multi-dimensional version so let's take our function e to the minus 100 x squared and make it regularly symmetric so that's the same function except now if it's in the square instead of the English x squared plus twice write it exactly the same behavior in that direction and of course in all the other examples so let's ask what degree by variable you need to get that - 15 minutes now here they obviously the function is isotropic moreover according to the standard definition of degree multivariate polynomials they're also a surcharge what I mean that is the degree of a polynomial and multivariate is the sum of the degrees of it's different constants so for example this has degree 2 this has degree 2 and x squared times y squared would have degree 4 so the reason for that definition of degree in the decisive prophet s is isentropic we conclude that we would expect to lead agreed 120 for the library so here's a picture of that expectation so I'm going to plot the chebyshev no divisions in x and y for that first two and the expectation is that to resolve it 250 digits we should need the coefficients below the red line in this triangle that's the triangle corresponding to various okay yeah so here's what you actually see that these are contour lines of the sizes of the chebyshev expansion coefficients when check finally resolve this function to 15 digits what we find is essentially the things within the purple curve are the chebyshev coefficients that are nonzero along the x-direction 120 along the y-direction under 20 but out there 120 square root of 2 degrees so the standard mention of isotropy for multivariate polynomial now once you see that it's easy to see why because approximating this function along the diagonal of a square well that's an interval of length 2 square roots of 2 instead of 10 it's a longer interval and on a longer interval you need more tension once you look at it that way and yet we can't find any discussion in this matter now a square root of 2 is hardly exciting but in higher dimensions everything accumulates exponentially you get square roots of D where D is a number of dimensions but you get D raised to the square root P so in fact equals a B and my favorite moment as far as we can tell to resolve a function in 8 dimensions check on agency-wide will require 639 time many coefficients as the standard wisdom would suggest because the standard wisdom is based on a simplex rather than a spherical ball of coefficients so we're totally puzzled about this trying to find the literature so one thing that I learned was if I have a function that's difficult to integrate but I can split it into a positive function that's highly variable in one that's pretty smooth and I use that positive function so if you use nothing again Oh Kathy pre-computing I mean baby [Applause] where when I did this example I had well adaptive methods and not to be based on global interprets the whole power of activities to do something more local so I think that's a different context most there's an exception to that once you start doing adaptive rational improvements you can begin to get the best think you can do it very uneven grids pay for matching the presidents that are spectrally converted but that's heavenly especially since mother for the Messiah support began if I hide recruitment most of the volume in a hypercube is concentrated in the corners shouldn't tell me that something go wrong I think so yeah somebody must have pointed this out so it's a it's a funny thing that of course if you're doing multi-dimensional intervals of course God does not say you have to work in a rectilinear but in principle you can work in the sphere so obviously you're aware but maybe some people are a bit high dimensional hyper cubes have amazing geometry the fraction of the volume that's in the inspired sphere goes to zero and in fact goes to zero pretty quickly as the dimension increases so the inscribed fear is nowhere they had to do everything in the corner [Applause]", "HwBZUpBo6l8": "okay so our next talk I'll be given by Michael Mahoney here at UC Berkeley on randomize America linear algebra sampling for linear algebra statistics and optimization all right thank you David so what's counts as foundations of data so we've heard about sampling and sketching and the last day or two from one perspective and a little bit of optimization today and tomorrow and then some statistics tomorrow and these are the things and so I thought I've described a little bit at least in a sense how to see how some of the pieces fit together I think if you've seen some of these things you see how the pieces fit together but if you haven't it might be a little bit dizzying all the different things that are going on and sampling arises in a lot of different places you know you can imagine there's a world out there and you get data from the world and you put it here and you want to make inferences about the world I mean that's one place sampling rises so that's sort of the sampling in statistics a lot of what we've been talking about so far is sort of very different than that right there's here there's 100 machines and what you want to do is too expensive and so I want to down sample and get a sketch and so what if anything is the connection between those and I think if your backgrounds sort of been computer science or theoretical computer science you tend to formulate problems and think about them one way and that's rather different than if your backgrounds in scientific computing and numerical analysis how you formulate and parameterize problems and that's rather different than if you're sort of background the machine learning or statistics and yet all these sort of ideas permeate through each of those areas and so I thought I'd describe a little bit about what I think sort of makes these things work so they're bridging some of the theory practice gap connection so I'll talk about that and that's sort of in the context of this area sort of randomized linear algebra so a little bit of background an overview if you thought about this is what the intent of boring you so the first few slides will just sort of be a little bit of background but the set stage on the first hour I'll talk a little bit about some sampling things how they fit together and then the second hour I'll get a little bit more detail about a particular a couple particular use cases and so take take a step back so I we were we wrote as an article I authority article turn addison CA CM a couple years ago and so we did due diligence to sort of think about how this fit in the broader landscape and I think that so the matrices are very natural way to model data and a data model something means natural and computer science like you know databases but less so l so but it's a way to model data and it's it's perfect or imperfect but it's a way to model data so in particular you have an M by n matrix and the M objects and n features positive definite matrix and codes correlations and that sort of stuff and so as hopefully by now if you've been here the last day or two there's a lot of sort of exciting developments motivated by a lot of large-scale data processing using randomization and so historically it's sort of been assumed to be a property of the data but you know meaning you measure it from the world but you've seen it use sort of as an algorithmic resource or a computational resource so what's what's the going on or what's the connection so this randomized linear algebra which is a theme of what we've been talking about arises because in a lot of cases you're modeling day as matrices but you want to make inferential claims about the world but you also want to compute on stuff so this is an area that's interdisciplinary as you may have gathered and it uses randomization as a resource to extract insight from data which means you need to understand these sort of two complementary aspects so there's a very foundational perspective right the roots are in theoretical computer science a lot of the techniques have connections with convex analysis and probability and metrics embeddings and so on and also sort of applied math and computational math and scientific computing from implementations you know it's not obvious but well engineered algorithms in this space you know be tell a pack in terms of wall clock time that was the claim of been London pick result it has been a range of results that are following up on that using these things as preconditioners and in terms of data analysis strong connections with machine learning and statistics so I think there's a couple ways to view about this and sort of today and this week and the semester is viewing it sort of from the context of foundations of data so how broad or how narrow is this in scope so take so linear algebra sort of as you know so there's a long history and at least by the standards of the day large-scale data analysis so Gauss had this algorithm in roughly oh one for doing QR type decompositions beginning of the next century Hotelling and others developed PCA and this was before computer science existed right so people think there's noise in the data and you compute something if there's noise in the data then then least-squares residuals are good in certain senses and keeping lead principle complements might be good in other senses and so that's sort of the motivation and what people have in the back of their mind when they using this and sort of having been said of doing randomized linear algebra others it seem to be sort of an interesting sort of disconnect or change once you have the development of the computer because it wasn't obvious that you could invert a matrix on a machine stabili right I mean back in 1950 and so it's a big deal to get that done and so I think culturally a lot of scientific computing and random and linear algebra sort of push randomness out it entered into other areas and so so this in a sense is obvious but I wanted to emphasize this because a lot of the particular questions people ask have remnants from this and so what you're seeing today it's meeting and more generally is that people sort of are inventing and reinventing stuff that is grappling with some of these issues and so it's the early computer science and early applications of linear algebra focused on scientific computing problems and there there's this notion that you only want to solve well posed problems and so even for well posed problems certain algorithms might perform poorly so that appears in numerical analysis but it appears in statistics right you don't wanna be solving poorly posed problems they might not care about machine precision they care about robustness and posionous and different senses in early work turing may be known for the term machine but if your backgrounds in numerical I also you know that he and others had early work in numerical analysis where they addressed these very questions in this led to sort of problem specific complexity measure sort of a condition number so one notion of a condition number is that top to bottom eigenvalue you saw a different notion of conditioning this morning for an optimization problem so the point of a condition number is it's a problem specific or an input specific complexity measure that says how hard is this input for a specific class of algorithms we'll get back to that and the traditional condition number is how hard is this input for a class of iterative algorithms like conjugate gradient or something there was for a range of reasons sort of a split and a lot of the continuous work moved on into applied math and scientific computing in a lot of the discrete work went into computer science you know it's easy to formulate computation per se in discrete settings you know there's a range of sort of reasons for that and so Rob you mentioned yesterday you know if you were to ask a computer scientist 20 or 30 years ago which of these algorithms would perform well and unscaled the data flow algorithms would or something and that's exactly not what happened and so with respect to what he was talking about yesterday do you want to run an algorithm that gets all pairwise distances right is that better or worse than an algorithm that sort of gets some all right on average and so this was the difference between random projections and SVD he talked about yesterday and so one is obviously better if you view the world one way and the other is obviously better if you view the world the other way and so under the hood and a lot of we're talking about is is trying to tease out which ones obviously better and depending on the application because if you using method a versus B versus C you're implicitly using one or the other and so let's sort of characterize that so so this goes into it a little bit more details more information in these slides and I'm going to talk about so for archival reasons but this will go and put a little bit more detail about that and so recently as you may have gathered this sort of a convergence of these perspectives and so I think this has been motivated by large-scale data so scientific internet a range of sort of applications computation per se is necessary you need to do it but it's not sufficient most downstream users of these things don't care about computations per se and so if you formalize stuff in a particular model and you ask is this tight or is that so the lower bound those are oftentimes very brittle to the detailed models of data access and so they break if you consider something slightly different and so most people want to gain insight in terms of downstream predictions and so central to a lot of these developments is random sampling and linear algebra randomize aspects of linear algebra well you have randomness is it in the data or is it in the algorithm or is there a tension between the tool maybe you can speed something up and do better because of some implicit statistical regularization continuous versus discrete doer about numerical issues and scientific versus business type into your sort of data motivations lead to very different types of questions you want to ask so sort of a good hydrogen atom in a couple senses I mean one a lot of the questions you might want to answer you can ask much more simply in a linear algebraic setting right how do you talk about implicit regularization and a57 way or neural network if you don't understand that for least squares so a lot of the themes are eyes here but also these these sort of basic linear algebra things appear but pulling them out and talking about them sort of in isolation oftentimes doesn't lead to the best insight about how they might be useful in in a pipeline and that pipeline could be use it as a precondition or to solve some other linear algebra to high precision but it could be feed into an iterative algorithm or feed into all sorts of other stuff to use it for a statistical inference so let's understand sampling in a range of these ways and view it sort of as a hydrogen atom to understand these are the foundations more generally so from that perspective with that background so what have you been talking about for the last couple days so Basics in a randomized linear algebra technique does the following you have a data set it's either in the world or on the machine most of what we've been talking about it's on the machine and you want to construct some sort of sketch and that sketch is typically done by sampling something it could be elements could be rows and columns but um you sample something and you use that sketches of proxy so you compute something on that sketching you say it's good in some sense for the original problem I had that's the high level and so if you take a step back there's a million moving parts and a lot of projections and this and that so I think the way to tax on amides are to think about it is in terms of basic design principles right if you wanted to have good coding practice or good theory practice the idea is randomly sample so some matrices whatever matrices about today about their elements or Bay they about their rows and columns some people say that they're the rows and columns right if you have the row space and the column space you're good well this might say you know if you have a term document matrix I'm interested in this term in this document if you have users and preference as I mister than this user in this preference of their elements so either want a sample elements or you want to sample rows and columns and in general you want to do it in in a data dependent way and in a smarter careful way and then you use that as a sketch to chew things but doing that sort of naively you're doing that uniformly typically does sub-optimally so use it a but typically a naive way to compute the data dependent structures you want that's going to be expensive and so you can approximate them and you heard about approximating leverage scores or you can pre-process a precondition so preconditioning is a notion that doesn't appear so much in computer science but it's very common in numerical analysis and the idea of preconditioning is I have a problem and I transform it to a different problem I solved this problem the two solutions are the same or easily related to each other all right and so one answer is I transform it by doing an identity transformation in which case this is no easier than this right another is I transform it by computing the exact solution in which case you know you've done a lot of work to transformation so the question is is there a trade-off space can I transform it in a way that's quick and also such that I can solve this more quickly and so that is what random projections are doing in in this case they rotate you to a random basis where good things happen so I'll give you some examples that but hey that's thought there's that's the way to think about a lot of these random projections and how they're being used so if you do element wise sampling you know you're thinking of the matrix is a bunch of numbers and you know your sample uniformly or not over the numbers and you know what does that mean that could mean the data is here and I pull out a few things it couldn't mean I don't know if it was arrived or someone yesterday I mentioned two techniques one method it could mean there's oh there's a world out there and I'm a company I want to measure do you have a preference for this or that so I'll show you something and if you click or not I sort of assume that you have a preference for this and so it could mean that I sort of interrogate the hypothesized matrix uniform sampling tends to lead to poor results one thing that's been used a lot is non-uniform sampling with respect to the magnitudes of the elements so this is the element wise analogue of the row norm sampling so it's an l2 on the elements and if you do that you tend to get bounds of the following form so this is a spectral norm bound a - the sampled version and spectral norm this is a true element wise but in terms of the directions of variance that's better than something times a scale if you have this then you're going to get additive error bounds and so you you know additive our bounds yesterday you heard we're bounds of the form I'm better than best plus epsilon times some scale but that scale could be large or small and so if you're asking for something very fine and that scale is huge you might as well return nothing right I mean because you're not gonna say anything if that scale is pretty small then you might be saying something so we'll give you additive error bounds which are good but not great and the proof method uses ideas from random matrix theory if you if you're familiar with this so a different thing is the sample rows and columns and you've seen a lot of this and again uniform sampling may perform poorly non-uniform sampling does better so if you if you perform non uniform sampling according to row lengths this is what we've seen a couple times yesterday you again get you know a matrix multiplication result of this form and if you get this result this is a primitive we'll get back to but if you have that result again for a low-rank approximation and a range of other things you get additive error bounds right so you are as good as the best rank okay that SVD gives you plus epsilon times something large and that's something large is here if you sample uniformly that's something large would be the dimension or the dimension squared but that you want that something large depends in the normal so is that good or not if you're exactly ranked okay if you're well approximated by low rank that's not reflected here if the overall mass of the matrix is in too large that might be reflected so and here the basic proof method is you look at expectations and variances and more sophisticated things for spectral norms so when we first saw this this is a little bit funny because you know you're not doing anything better by looking at rows and columns and elements and you feel like you should because you're capturing subspace structure and so what can you do and so a different thing you can do then the norm squared sampling is used as leverage score sampling so this is something that you know will originally I guess Rene and I sort of reverse engineered and then I turned out have this interpretation and this is looking at you know role norms of left singular vectors which is equipment ly diagonally laments of projection matrices and I think the way to think about it is it's relatively easy to normalize rows and columns right you just normalize them it's a little bit harder to normalize away the subspace structure right so if there's an example of a of a problem you know where your look like this I asked you which data points can have the biggest fit I mean any metric that's reasonable is gonna find this one right so this is one one you know leverage is one way to capture that now do you want to find this or not I'm gonna sample this or not it depends if you're a statistician you may give a very different answer than if you're computer scientist or if you're computer science and you're making a claim about what's sitting in front of you there's no world you make an claim about what sitting in front of you someone you're an algorithm there's an input there's an output you know you better be right garbage in garbage out you better be right so you want to find that if your statistician you might say jeez you know that's an outlier that's something funny and I want to not find that empirical questions maybe data look funny and you're using least squares because there's nothing else that you can use but you know so you want to buy yourself to its funny things and a lot of the early work in theoretical computer science because it's asking algorithmic questions and computer science biases you towards that and you may want to buy us away you know away from that so they're useful in regression Diagnostics identify outliers like most things you can be naive in their computation or not to compute them exactly takes q/r time or you know something like that as you've heard you can compute them a number of different ways and you heard about one way in essentially random projection time so the time it takes implemented projection you can approximate them there's a couple different notions of approximation as a practical matter in terms of use cases you want to find a large ones and small ones the first approximation don't matter we'll get back on the second hour to a case where they do but you can usually easily take care of that and you saw an example of this and in the optimization talk this morning replace the non-uniformity which is a variant of this with half a non uniformity plus half uniform that means nothing's too small and so that's why she did that and and there was a question about business correspond to regularization and it doesn't very natural sense the other reason this was sort of of interest is that it provides a data aware subspace embedding meaning I look at the data I know something about the data but but I get an embedding which I said which I'm saying if I have a rectangular orthogonal matrix then if I'm tall then you know I know that like this that's an identity right I want to say the sampled version of that is roughly an identity and the way to quantify that is to say that this - over the spectral norm is the largest singular value they're all one if they're all between a half and three hats place that with an epsilon I want to say strictly away from zero why because if I prima tries problems in a worst-case sense I'm dead if I don't accomplish that in other use cases it might be fine to violate that and we'll see that but it gives you a data where subspace embedding so from the perspective of theoretical computer science you've heard I think was yesterday this should be thought about is a subspace analog of Johnson Lyndon Strauss right Johnson Lyndon thrust is about a discrete point set but if you put a ball unit ball on a low dimensional subspace this is a subspace analog of Johnson Lyndon Strauss numerical analysts would actually think of a very different way then say this is an acute perturbation right it's a perturbation that zeros out most of the canonical axis and most of the columns such that you don't lose rank acute means it's less than 90 degrees so you rotate a little bit you zero out most things but you don't lose rank and this is one way to think about it this is a much more geometric way to think about it and so neither is incorrect both of the case and one of the others more useful in certain use cases now wrong you know norms of columns and elements are not too hard these take a bit more work even if it's random projection time and so depending on your use case you may want a precondition so this is a good thing or not and so look at numerical analysis book there's a range of methods and it depends on the problem depends on the input it depends on the data presentation depends on a lot of things and so similarly here so the main challenge for a lot of these things is that you want to find these outline things you want to find these interesting things so you can find them or approximate them or you can just say I don't care because I want to solve this downstream thing the least squares problem or something else and so what I'm going to do is preprocessor precondition my input and so the main challenge for uniform sampling is that there might be one or a small number of coordinates that are that are particularly important you know there might be this guy there might be I think someone put up a one negative one one sort of example right you got to find that guy so think of this as a delta function right this is one coordinate out of n that's important how are you going to pre-process a delta function you hit it with a Fourier transform and it spreads it out to be a sinusoid so Hadamard transforms gaussian random projections count sketches all of these things essentially rotate you to a random basis with these things a uniform with the leverage scores a uniform all right so what why they work in a lot of ways is causing the randomly rotated basis that uniform eyes only approximately right so Ken was talking this morning the uniform eyes up to a factor of two or ten I mean in that case they could be uniform eyes only up to a factor of login and that's sufficient for what you want to do downstream so that but they uniform eyes so that should you ask for the constant factor or the log in you this is a pre-conditioner right depends on that trade-off point you're interested in so so you should think of it that way I think all right so those little bit about any pause for a second because I want to go through a little bit of detail the matrix multiplication and how this is used in the least squares case but maybe pause I forgot to bring the water so I'll be okay but there's any questions so hopefully this was a review and maybe shed a light on some of the things we've been talking about at least didn't make it unclear and hopefully clarify it a little bit it leads to something yeah you have a pretty general definition of pre-conditioner would you describe sketch and swallow that's precondition yeah so I don't know how general you want to be it on let me get back to that after the Lee's asked that question after the least-squares sketch it could be but then there's a precision issue if you want a low precision it could be because because according what I just said the random projection would would precondition in any sample uniformly so for the class of sampling matrices this is a pre-conditioner for that and and you're not going to get high precision because of the 1 over epsilon and 1 over epsilon squared but you'll get low precision so it would be yeah so if you want a high precision you got a couple this with a more traditional pre-conditioner which is the way they were you know David was talking about it yesterday we had take this low precision thing and then feed it took more traditional high precision but within the context of sampling albums it is yeah all right and so that question leads me into a couple things actually not the first thing you can see the second see it on my screen but the first thing is talking about matrix multiplication and Nunley squares so so how do you so sort of a primitive in all of these things I think and you saw one example of what the subspace embedding is matrix multiplication right so it could be simpler than this so given an M by n matrix a they're all in size n so you have an N by n matrix and n by P matrix B approximate the product and I think that when most people at least initially see matrix multiplication they think of it in the following way I have a matrix a and a matrix B and it's defined something I mean your matrix multiplication is a transformation it's a function right it's not a bunch of elements but it's defined operational in terms of elements to say that a be you know IJ is a sum over K of what of aik B KJ right meaning it's a column of a and a row of whatever a row of a and a column of B so that's not untrue think of it this way though a B is a sum of outer products all right so you the block by rows are blocked by columns if you block a off by columns block B up by rows a dot product each element of which is a rank one matrix so think of it this way and if you're dealing with rank one matrices these are objects if you're dealing with numbers these are other objects and if you're summing over you know one to a million of them you might say sampling will work so that's what's going on under the hood here and so think of it as a sum of outer products now don't think of it naively this way because you know you might not access the data this way but this is what's going on under the hood I think and so the sampling approach basically says you know fix some set of probability is whatever you want it could be what we're talking about I'll so you'll see in a second or it could be other thing and 40 goes wanted to see sometimes this is an S or an R or an M or an M sub J it's the number of samples you're drawing choose a Rank 1 component but clearly don't write it out naively choose a column of a and a row of B with those probabilities and so approximate the product a B you know this is um whatever K is equal to 1 to n if it's M by N and n by P and say that this is close to a sum from k equals 1 to C of a scaled version you got a scale by that because if I'm estimating the sum of a million numbers by sampling 10 100 of them I got a sale by was scaled by a million over 10 right so this is a non-uniform analogue of that just to get estimators to work so the same algorithm written as in matrix notation pick a bunch of columns of a and rows of B and write it as an outer product that you feed yourself through some low dimension if you have this you know you can think of this as a sampling matrix where this has one nonzero parole and once you write it this way you could say well what about other matrices you're starting to decouple the linear algebra from the algorithmic property and then you can say what if I had other so types of things but for right now think of that as a sampling matrix so if you do that so there's simple as hard things right so let's I'm trying to describe some simple things and if you do what I just said the following is simple in the sense that if you no means and variances you can prove the following so for any set of sampling probabilities see R equals a be element wise just right out of saw minutes already set of probabilities this is equal to the variance it's something - of second term which you usually drop if you're doing balance all right that's just write down means and variances nothing fancy and so from these it's easy to say that the expectation of a B minus C R for B Gnaeus squared but you can pull that off as a sum of these things right so it's it's you can get a bound on that you can remove expectation with Markov inequality or mark martingales and arrange all the methods and buts but they're sort of you heard about and are sort of more standard but a little bit more complicated you could ask what's the set of probabilities that optimizes this why this because it's a convenient thing now because it's the most important but we'll see this is a useful primitive for range of other things the probabilities that optimize that so that that's a reasonable notion not a perfect putting a reasonable notion of quality the property is optimized that of this so a norm of a and a norm of B now if B happens to be a transpose this is a norm squared right divided by a Frobenius norm so these are the optimal probabilities if you use that than a B minus C are lavinia norm of that is less than 1 over root C this is the usual 1 over root C that you get in a Monte Carlo method times the scale factor which is norm of a times norm of B and so in particular B is equal to a transpose you're saying a a transpose which of the form you've seen a bunch of - call it C C transpose is less than 1 over root number of samples times the norm of the matrix squared and this is very robust if you approximate this in any number of ways it works you got to find the large things you can miss small things if you're off by a factor of two you get over sample by a factor of two in worst case but in practice you'll do a lot better so they're very robust in in a sense what is this thing this thing is a complicated way of writing the spectral norm of that thing this is a Rank 1 component so if I'm summing numbers you know I'm going to say using any probabilities I'm going to get a good expectation but if I bias myself towards large numbers I'm gonna get a good variance it's not that this is saying so bias myself towards large things large matrices this is the matrix analog of that if you estimate the height of people it doesn't matter you choose they're all everyone's about six feet you choose a few hundred people you have a good estimate if you try to estimate wealth or something like that that has a huge range you better bias yourself in a better way so that's all we're saying here bias yourself towards larger things right that's for being its norms think of matrix multiplication is a primitive that should feed other things into and so for a lot of things you can use for beanies gnomes you might need a spectral mom but a spectral norm is less than a 4b Gnaeus mum so you could bound it that way it's gonna be suboptimal but you could you can use more direct methods so there's a range of ways to do this which here here's one result if you make the spectral norm small on the Frobenius no not too large I'm saying they're not important here but the previous result holds for anything this thing holds for a subset of matrices it holds for matrices that you're going to be interested in that have a spectral structure or something decays but it's a subset of matrices if you sample with a complexity that's slightly more complicated looking but will be better in practice then the spectral norm a a transpose minus C C transpose can be less than Epsilon so I'll say why this matters and how this is related to the previous thing but I wrote them in two different ways just to highlight something and so the way so the reason why this is a primitive that's interesting to feed into a lot of things is so matrix multiplication will be the primitive for a bunch of things and you use better and worse bounds for a sampling for projection for whatever here's the 95 it's 99 but 95 or 98 percent use case so spectral norm bound is better in the sense we'll get to here but it's it's less general in in the sense that it holds for B equals a transpose it doesn't hold for arbitrary a because those two conditions basically you need to have a low soft relatively low soft rank and so and so better results in matrix multiplication propagate to a lot of downstream things and so we're talking about here the sampling case the same is going to be true on the projection case and you heard a bunch about that so the main use case for the spectral norm bound is the following I have a matrix a which you know getting the letters right as always a mess so this is consistent the previous page but not other talks and so on so you know now I'm calling it a transpose but let this be at all orthogonal matrix namely let it be this thing and what I want to say is that u transpose U is an identity and I want the sampled version if u transpose U to be roughly an identity and a way to quantify that is to say that all the eigenvalues of u trends of the sampled version of u transpose u are close to 1 so meaning the spectral norm the largest deviation 1 - this is less than 1/2 in order to do that I can use the result but the Frobenius norm results I had two slides ago to say that the spectral norm is less than the Frobenius norm the roubini enorm is 1 over root C times the norm of a but a here is an orthogonal matrix so the Frobenius norm of u what's the Frobenius norm of an orthogonal matrix have a tall matrix it's a million by ten orthogonal columns am i standing in the way or is the answer still up I don't know if you get to see you can so if you've an orthogonal matrix the Frobenius norm the sum of the leverage scores is low dimension it's ten it's not a thousand so the reason this is a bit of interest as opposed to add of scale factors is that if you sum from 1 to n more or less anything you're going to get something like n unless you have some structure here and so the reason the leverage works is because if some is 10 more generally what if it's 11 you try and generalize this to other problems to other objectives if the sum of the leverage or influence or sensitivity this generalizations is low dimension or low dimension squared then you can typically take advantage of it if it's high dimension or root high dimension you got to eat that in general so that's why this works so you say that this is the 4b Gnaeus norm squared is d the low dimension divided by root C that's going to be a problem because that should be probably DC squared see so this should be a square root of D I guess yeah this this should be a square root of D meaning that if I want to make this less than 1/2 C needs to be d squared over 2 or something right and that d squared is gonna appear everywhere right well we're doing Q R it's high dimension low dimension squared if we need the number of samples to be d squared that's going to propagate down we can make it less than that we're gonna win and so the way you win is by looking at a spectral norm bound directly the thing I had on the previous slide if you instantiate it's gonna be log something like log C over C times the Frobenius norm times a spectral norm that should be root D not D with the spectral norm of an orthogonal matrix it's 1 so it's a root D times 1 over root C times the log factor and so I'll be able to use a matrix multiplication primitive I could use it for beanies and boils down to matrix multiplication but that's suboptimal if I use a spectrum directly then I'm gonna need C to be D over epsilon off the log factor so this is where the win is and that's true in the data aware case that oblivious case that's going to be true everywhere what if instead of having a sampling matrix are at one nonzero Perot I have two non zeros Pirolo or n non zeros Perot where the n non zeros or gaussians or Hadamard around markers or any of all the same to first order the second of order there's different log log log log differences right but the first order and so lots of lots of dense sampling matrices from a theoretical computer science perspectives and natural interpretation terms of johnson lyndon Strasse and a random projection and the subspace embeddings as I said can be viewed as as a sort of vector space generalization of that in the random eye and the linear algebra are thinking of it as a rotation roughly an orthogonal transformation the randomized Hadamard is is exactly an orthogonal transformation the other ones are approximately and natural interpretation in terms of preconditioning or pre-processing so I'll get to that in a second so this is - so hopefully this was clear by now but to relate back to what you're hearing about earlier some space embedding is that you know I preserve the geometry search so this is the expression if you remember the definition of a spectral Marmite a spectral norm is the largest direction that a unit like vector gets stretched you can write it as whatever as max you know you X over the unit length X that's what this is so with the expression we saw before is exactly this which is exactly this lots and lots of constructions some of them I've described the data aware ones bias yourself towards important things there's lots and lots of data oblivious so this was first used in it's in the data where context it actually predated the randomized the fast random I the L and chose L result then Sarlo support if you use couple this with the ion shell result and there's been a lot of follow-up in a date of livius way so you want to use a result in this area should you use the projection art should you use a sampling method was the first burger if you're gonna use a result that we're talking about here should I be using should I suggest to use a projection method or a sampling method so I don't have a suggestion it depends on you and your problem for certain problem sampling is more natural for other problems you know projections will be better but they're going to boil down to the same thing under the hood so don't think of it that way as I said this is an acute perturbation meaning it's a small rotation from a million dimensions down to ten or a zero out but not to 10 to 20 you know to 10 log 10 on her eyes zero out most of the entries but I don't lose rank in particular so one of the points is this is sort of a must-have you view things from a computer science perspective you need this in a lot of ways you know it's good to have or it's good to have something related to this but you can relax on this what if you lose one or two dimensions is at the end of the world so I'll give an example of the answer to that what if you want to you know do something statistical where there's a bias-variance tradeoff so how would this generalize in those cases so I'm going to give the least-squares example but let me pause and see if there's any questions or violent objections or violent questions or nonviolent objections we're just talk about data oblivious like the whole thing was looking at so I didn't talk about it because I was able to backward point to everyone else if if they if ESO so if you have a dense sampling mate so the reason you need this sample is if I just choose one thing I need to find this one outlying thing if my sampling matrix is dense meaning I have a plus or minus one and every entry if I have a Gaussian and every entry I have something else if I make this quote sampling matrix dense it's not a sampling matrix its sampling a linear combination of things which is a random projection with some and so the count sketch is is a very structured way to do it that allows you to get extremely sparse the the randomize Hadamard is a way to do it that as a hierarchical structure so you can implement it with fast fourier techniques I mean gaussians have other so when you make this dense but the reason it's nice to distill this out is to first order exactly the same analysis works if you sample in a smart way if you do data oblivious projections if you do any of a range of things this is a structural thing you need to satisfy and so should you do it in the data oblivious way or should you do it in the data wear way and so the answer is the same as it depends on the context alright so at least squares so I like these squares because every time we get a new result you realize you didn't quite understand something about what was going on and so again dwell on this we've been talking about this but so this has been something that's been studied forever right but I mean we're looking at in a new light and so one think of it where I'll describe some stuff you've heard the last day or two in a way that complements I mean so if some of the previous talks have emphasized numerical things I would have taken the counterpoint and described that maybe complementary to the TCS approach so think of this as in light of that but also distill out structural properties like matrix multiplication and where that might be used there so think of this as a test case for saying you know how would matrix multiplication be used here so least squares minimize ax minus B and now we're m-by-n I guess and so in a Ram model this actually is you know a far from perfect guide to practice even on a single machine but you know it's it's high dimension times low dimension squared time so far the other reason to distill out the structural properties is what if anything that we're talking about here ports to a distributed data center if you distill out the structural properties you can deal with that separately from RAM and other issues and a statistical question you can ask different questions whereas if you don't distill that is much harder to figure out what's going on so for least squares is a nice example here because it uses these techniques and and you get the best results and you get the best results for very different and rather sort of inconsistent metrics but you need to use the sort of basic method in somewhat different ways so if you want the the the algorithm that in terms of worst case Ram running time and RAM beats the algorithm Gauss had 401 an algorithm here will do that so you can use randomized Hadamard and then depending on aspect ratios you can use nnz type methods it's not obvious that if you can make a statement like that that the method is anything to do with something that would perform well numerically it turns out you can beat la pack and wall clock time and then when blend depicted that la pack respondent and those an arms race so I don't know what the state of the art is right now but you can compute things you know you can use these methods and feed them into low-rank approximation z' and get pca on 50 terabytes of data 30 terabytes of data sort of thing and the underlying embeddings and approximation techniques are the same but clearly the way it implemented is going to be very different and it's useful in a range of data applications and sometimes useful for very different reasons right it's useful not because we've squeezed the last little bit of out and objective but there might be implicit regularization right if you're using sampling the noise inside the algorithm might synergize well or poorly with noise in the generative processes of the data so least squares and so there's two there's two notions of conditioning here so this slide hopefully is a review by now one is sort of four four four low precision solutions you need to capture something about the non-uniformity on the canonical axis and for high precision solutions you need to compute the capture something about the aspect ratio of the quadratic well that sort of implicitly they're condition number is the ratio of maximum and eigen value it captures the one non-uniformity on leverage or influence the sensitivity scores either the max to the min or the max to the average and depending on the exact rejected you care about is a condition number for a class of sampling algorithms these have to do with eigen vectors these have to do with eigen values that is different and in real cases generative processes the data sometimes they're correlated especially on extreme eigen values and eigen vectors but you know in general they could be whatever these are for l1 norms as generalizations l2 norms as generalization for r1 norms you heard this morning a variant of these for an optimum interative first-order type of optimization algorithm so that's not exactly this but it's it's a first cousin of this in the context of of an iterative optimization algorithm so here's the basic least squares algorithm compute this non-uniformity structure randomly sample a small number of constraints to it and then solve the subproblem so this is sketch and solve give me a sketch solve naive version I like I'm the one of these emblems I like this result because it's a core of a lot of things but it gives you worse than the exact answer it may fail with some probability Delta and its running time is worse so that doesn't seem to suggest much for it but in fact I'm all those can be taken care of so a non naive version as I said is the best worst-case algorithms and good for all sorts of other things so let's understand what's going on here and that picture here is this this picture find an outlier this is a pip this is either the genome research of the PNAS paper so the the genome research at some stuff in genetics the PNAS we looked at something else question-how uniform or not of these is this correlate at all with something that might appear in practice and so what you have here is is snips snips single nucleotide polymorphisms a single positions in the DNA where you're going to have some difference between people and the y-axis is here is a notion of leverage this is a low-rank approximation a generalization we've been talking about and the things that stick up our flag is that's higher leverage than that now if you ask a geneticist what are the important snips here and say give me the 50 most important snips that you have certain metrics one is called informativeness it's related mutual information they'll take these the ones in red so there's a strong correlation here and this is not a one-off but it's probably a 50 off so there's lots and lots of examples where the non-uniformity structure you see in very real data look something like this so you might say why is this you know if you're using an l2 metric shouldn't you use an l3 or some other metric right and the answer is people do least squares and people do PCA and people do all sorts of things like this not because they really believe the generative process is a Gaussian but because that's something that they can compute and scale and so on right and so looking for outliers that could be a mistake you kick your machine that could also be an outlier that's the most interesting snip relative to a model that you know it's wrong but you're using for convenience this is a very common thing so these things are outlying so I mean I remember we were working some people up the hill at LBL and they applied our method on some mass spec data and it returned garbage and they went and complained and said your method didn't work I said what I mean it didn't work and he said he returned garbage I said how did you clean up the data he said we cleaned it up show me the garbage and of course it was it was all artifacts of experiment yeah the experimental setup that didn't matter for other things that were using but that that we have a method that finds those found those I said why don't you just remove all those run the algorithm again you found these things that you failed to clean up before that's fine we run the algorithm again then it zoomed in exactly on the outline things that that they were interested in so again should you by yourself towards these or use these to filter the data all right so there's almost all that maybe I'll just have one question last for the next hour right so it depends if it depends on the use case I won't promise maybe there'll be a different question coming up so but so this this is the way to think about this is a structural result if you're modeling data as a matrix in a Hilbert space or you put in space this is a very basic structural result I mean for L two objectives it is not possible for every data point to have a big influence on the least squares fit that's what it means at the sum of the leverage scores are small in theoretical computer science and I'll go into this more or less detail the next hour depending on time but you can event objectives there's lots of objectives and for some of those objectives that some of the sensitivities is small you can use sampling techniques for other of those objectives are some of the sensitivities is the high dimension you can't use sampling techniques at least not in worst case of course you can regular eyes and do lots of other rigmarole but that's the that's leading order bit so for these objectives it can't be the case that every data point is important so that's why sampling would work all right so yeah there's a sketch in solve so we can make this algorithm fast by running in RAM we can make at I precision so how do you make it fast use a Hatter martin method use account set sketch method use any of a range of methods you've been talking about or compute fast approximations to the leverage scores you know how we saw this morning you know you have your your matrix a oh oh now I won't use a QR inverse and I won't use a you are all starting to Michael Saunders before the talk he was complaining that you are I'll use a QR so a you know is equal to QR that's expensive to compute let's work on PI a what is that this is anything but in particular it's a Hadamard based projection Gallinas if you like that as a count sketch let's say PI a is equal to Q hat R hat that's exactly our thought oh that's perfect numerical issues and whatever but that's perfect for that in particular if you were to look at PI a R inverse you know you have an orthogonal matrix apply it up here a R inverse this is equal to Q hat hat that's not our thought at all there it is the right dimensions and it's close to our thuggin all what sense is a close to orthogonal in the sense that do you transpose Q minus Q hat hat transpose Q 1/2 hat spectral norm so there's matrix multiplication sense alright and there you know you need to wedge an extra projection in there to make things fast and so long that that's the structure that makes that work and you can make it high precision so you heard yesterday about one way to do this a different way is to call LS QR or you know which is which is conjugate gradient in the normal equation sort of done right so you don't mess up condition numbers and if you have any questions about that I'll pass the buck to Michael Saunders who can probably answer it in more detail so you know if you're interesting that sort of stuff call that as a black box because that's been stress tested for 30 years over now there's LS some are mrr mrr mrr LS mr so i'll give a plug to LS mr which is the new but you have call that black box and how are you going to do that so i'm saying you'd preconditioning here you could take this wherever if I'm solving ax minus B pre-multiplying off by Q transpose by Q hat hat transpose call ellis mr so what makes all this work X minus B there's a solution ax op so this is clean because there's you can write it in a nice clean form the unit of algum is a little bit more complicated because you can't write that in a nice clean form so you need to do something iterative as well as a preconditioned version so then I slipped in a new letter that's an ass or maybe a pie but this is just your sketching matrix so here's the basic structural result if Sigma min so we know that the eigenvalues of u singular values of u are one if Sigma min of the sketched version of U is greater than a half you can actually be bad on the upside but say it's less let's say it's between a half and three halves and if you a dotted into B perp what's B perp B perp is the part of be sitting outside the column space of a so you a got into B purpose zero if you a dot in the B perp on your sample it is close to zero less than epsilon that times a scale where the scale is a thing there's a number you're computing here if this is true then your relative error you plug your solution back in ax minus B u 1 plus Epsilon good there's no difference between the two norms you want plus up some good does it you know condition number factor there of course so this is a structure result there's no randomness in the statement all right and so this was the original result when we first had the relative error result and there's no randomness in the statement so you could use the left singular vectors as Omega maybe not quick to compute them but you could you could use any matrix you want you could run an algorithm and you could check these conditions after the fact so it just so happens that if you use leverage sampling or if you use random projections or if your guarantee of a subspace embedding you satisfy these in a sense that computer science like worst case a priori balance but you could run something and fail to satisfy the worst case a prior bounds but ask can I certify that these are true and so you know this is a struct result there's no randomness here and both conditions are approximate matrix multiplication right the first one eigen values are one I want the sampled versions to be close to one and the second one that's zero right so the second one is an approximate matrix multiplication result here you don't need the high probabilities and all that rigmarole this this is sufficient but you could do that if you wanted all right lots of constructions satisfy this heavy hitters I mean there's frequent items emitters of things the analysis I wasn't present in this way but you can feed it through a result like this and this is of the basic structure result to get least squares approximation if you're interested in the data in front of you date on the machine if you're sitting the world that's we'll get to that later but this is of the basic structure result and yeah you know I used to worry about this but you know I'd say be a little cavalier about the excellence theoretically you know in theoretical few times you tend not to care about it and numerical people precondition they used to early work here they used to worry about it a lot and this used to be a big sort of point of criticism but said epsilon to be a half you have a good pre-conditioner and you take two extra iterations or something right this is a trade-off point you can make it one over 100 but set it to be half and in machine learning and data analysis is just different pain points and here's the plots for blended pick that says you know you can be tell a pack something that I learned here that part should have an obvious but I didn't know part of this so a subspace embedding is is sufficient to get a good pre-conditioner but you know is it overkill all right what if you know in random matrix three people look at the spectrum random matrices and they look at the tails and you know you might ask could I get greedy you put a sample a lot less than the de over epsilon and log factors go to ten D so if you do that you'll fail in worst case so you know we know that and it may be the case that on any particular input you fail so I want to do sketch and solve that's a problem because maybe I failed to find the one thing I want if I'm gonna use that as a pre-conditioner that's not such a problem and basically the reason is that if you're going to use this and you're going to say this is a good pre-conditioner and you're going to call the black box what if I have something else that's a rank one perturbation of a good pre-conditioner they lost one dimension or rank three perturbation of a good pre-conditioner they're lost three dimensions I need to do 3 X 2 iterations or something they're just a few extra iterations so you can get much greedier on the down sampling step if you're using in the pipeline the simplest version of that is an iterative police square solver but there's many other examples of that all right that I don't know you probably hear tomorrow in this context of second-order optimization which uses these things and generalizations of these ideas but a GPU implementation so I don't know the answer we just have don't have the implementation of the straightforwardly squares on the GPU probably wouldn't be too hard but this is a few years old and the LS RN which is the extension is to use random bits with Gaussian matrices there's also a few years ago so I don't know yeah I don't know how it would perform on a GPU good question if someone's numerically inclined is it slow it's actually a very low hanging fruit all the infrastructures in place so we would know Eddie something but I don't know I don't know all right so one through matrix multiplication because I you know feel like if if you see that and how that relates to theoretical computer science objectives like Johnson Lyndon Strasser all and get every pair wise things and you know if I lose a dimension or two how its you so I wanted to go through the matrix multiplication perimeter then go through least squares primitive and then waving my hands and say you know so that all boils down to that which is a bit of an oversimplification but if you if you think about these things a lot does bow down so I want to give a little bit example just sort of how a bunch of different low rank things do boil down to this and then maybe you'll believe that a lot of other things almost do so it was so how it go had a nice so this is extending some stuff Tygart had and there was a range of scientific computing people who've got implementations and of course they don't like the log factors and and and for good reason I mean it's just not a way that they had implement things and they said how do these things perform in practice and they would describe it the typical use case they're a scientific computing you're not doing matrix multiplication but really you are on the hood you're not doing these squares but really we're out of the hood but you think this is a matrix and the matrix is a discretization of a continuous differential operator and you want typically you know medium to high precision because you're solving a PDE or you're solving a climate model you're solving some scientific so there's been a lot of attention in geophysics and Geosciences using these methods and so that's sort of one and and and are they asking a very similar question or a different question so the way they describe it is the following I want to I have a matrix a it's you know both dimensions are large it's not rectangular and I specify a target rank and then I don't say K log K over up so I specified over sampling parameter P that's five and I want to sample K plus five dimensions and so I want to get factors u Sigma B transpose that approximate a in some sense so they typically are thinking low numerical rank meaning you know fifty vectors gives the Machine precision so there's a difference there most data applications a thousand vectors gives you 50 percent the Frobenius mass which means things are very different so they would say draw and an N by K plus P Gaussian random matrix form the N by K plus P sample matrix Y which was eight times Omega which is your your PI compute northa normal basis for that form of small matrix B is equal to Q transpose a back to the small matrix and the u hat Sigma B transpose and then form U is equal to Q u hat so you know a numerical analysis you probably familiar with that otherwise it you know it's not clear what the 1 step of the other might be doing you can prove bound to the following form so a minus the projection onto the columns is this is the residual for B Gnaeus mass and it's 1 plus something of the form K over P and in spectral norm it's something having to do the best rank K spectral norm plus something else this is this is a 4b Gnaeus norm so it's best rank K times the little multiplicative factor plus something else that's large so how does this relate to we were talking about and what's the underlying structural results here P equals one looks a bit dangerous what they would suggest is five or ten or no what they'd say but the the code that happened be five or ten the probability so here we're we're doing a low-rank thing and if you keep a few extra dimensions which we didn't have in the least-squares case results get much better if you keep a few extra dimensions but not one you know that say five or ten this is what they'd say meaning you go to their paper that's what they say I'll roll I mean how do you relate it back to other things and so so so so if they change the analysis that may or may not be good for the use case so the question is what question are you asking what questions should we be asking so so in a sense I'm just describing a history and they would do this thing and make a big deal about how it sounded different they would you know we you would say oh you can change this and ask this other thing they would say yeah but this other thing you did we could change it and ask it our way I'm just trying to say yeah what sort of similar but um what's the connection between this and everything we've been talking about and to highlight that I'll say how would you prove a structure result like that a seemingly very different question is see you are you heard the other day or CSSP the column subset selection problem so I have a big matrix given me the K columns that are best that's intractable for any notion of best so give me the k plus p or k plus ten or K log K and there's a range ways to quantify that but I want the best columns so what if anything does that have to do with bounds of this form it seems like a very different problem it is so under the hood they're gonna be very similar so this is so I showed you before a structural result that underlie you know the 99% cases as long as you gotta be a little bit cavalier about log log factors this is the similar thing for low-rank approximation and this was first highlighted by blue Cetus and in in the column subset selection but it's used in as though the basis for a lot of the numerical methods so a minus the projection on to a is that rank K or not and so that's a second-order bits there could be slight differences here for what norm spectral Frobenius trace so it basically holds for any unitary variate norm and then it bifurcates depending on what you have because you don't have matrix multiplication result so although it's better than best rank k plus this so what is this form this is top k singular vectors perpendicular i mean the bottom n minus k singular vectors times best rank k bi times the complement to the best rank EK what is best rank K times the complement of the best rank K 0 I want to say that best rank K times complement of best rank K is roughly 0 so you have this result this is a deterministic statement you could ask for an a part by re bound and a posteriori bound subset selections do random rotations do Nystrom the nice term you want to preserve symmetry and positive definite there's a psych tweak on those properties but it's a very it's a first-order similar so given the structure result wide range of sort of low rank approximations will follow so this is a basic structure result that extends in a special case of this as the least squares but this is a more general thing because of the little space so this is what we'll cover in the next set of half-hour so this is some of the basics and you can use it in a lot of different ways and I'll highlight a few ways that are ways that other people this week aren't going to be talking about in the next hour so some of these things you heard from some of the other speakers and I'll hide a few that how it a few that other people aren't gonna be talking about in the next hour [Applause] since for Michael to this no I can't proximation doesn't that fall out from the matrix multiplication stuff that anybody identity yeah so I mean this will reduce to least squares which will reduce to make it multiplications what's the difference that's I mean this is a projection that's the only thing people only care about the outcome being ranked a rank P here and can I just use one of those it's this transpose is sort of cute yeah this is the question that asked they they would describe it differently and they would talk about Gaussian based projections and the structural result to justify this is the one on the next slide with that structure result you could do what you're talking about you know change change the projection as long as you satisfied those conditions you get us an analogous in a statement yeah let's thank Michael game [Applause]", "_Y5yChmonWI": "Welcome to the course on Scalable Data Science\nmy name is Anirban I am from IIT Gandhinagar. Today\u2019s lecture is going to be on a modification\nthat we look at for the QB algorithm, for the QB decomposition algorithm plus we look\nat applications of this randomized techniques, the random projection kind of techniques to\nlinear regression solve problems. So, just refresh a memory, we talked about\nhow important getting the lower rank composition of the matrixes for certain machine learning\napplications. We did not talked about the fact that while the single value decomposition\nis a pretty useful lower rank approximation of the matrix, it is very expensive. And,\ntherefore we want to get something much more cheaply right and we are for instance try\nto build something that we call QB decomposition. So, what we looking to solve a solve is the\nfollowing, that suppose we have a target rank k in mind right, that is we really want to\nget down to the we really want to get down a low ranker approximation that approximates\nthat captures the error A minus Ak either in the Frobenius norm or in the spectral norm\nright. But for that we give a little more leaving right, in approximations terms this\nis also known as by criteria approximation that is just terminology. So, we so instead\nof using exactly ranking matrix we use k plus p matrix or rank k plus p matrix right Q right.\nSo, Q is of A is of size n by d Q is of size n by k plus P and B is of size k plus B by\nd and B and Q is orthogonal matrix. And, B therefore is the projection of the matrix\nA onto the column space of Q and not we really want is that the Frobenius error A minus QB\nor spectral error or maybe both right is they are really sort of close approximations of\nthe optimal error A minus Ak Frobenius. And, or the A minus Ak 2 norm right and we further\nmore we also want k plus B to be more or less of the order k right because, if k plus p\nis very large then achieving this is easy. And we one prototype algorithm in which we\nall we did was a multiply the matrix A on the right hand side with random matrix right.\nAnd, then we obtain the orthogonal basis for the column space of this projected of this\nprojection Y right and this is my target Q and this is my t. So, we will also saw particular bound for\nthe we also saw particular bound for this for instance we saw that the A minus QB Frobenius,\nA is can be bounded by 1 plus k by P minus 1 the optimum error right which is the A minus\nAk Frobenius right ok. So, now the question I mean can we took better than this right?\nSo in reality turns out that this bound really depends on the singular values from k plus\n1 to n. So, in effect what did means is that if the singular values from 1 to k right,\nthat is if the singular values and this is really in the signal that we under capture.\nAnd this k plus 1 to n is really in terms of our model is really the noise or the perturbation\nright. So, if the signal is much bigger than noise right, then the bound actually improves\nright then the bound actually then we can then we should actually be able to improve\nthis algorithm improve the performance of this algorithm right. In fact, it is also\npretty clear that the bound improves as well as a performance empirical performance of\nthis algorithm right. But in real matrixes this does not always\nhold right, what you see is if you really plot the singular value is that the kind of\nfollow more sort of I mean slow dk rather than a very sharp of rather than the ideal\ncase which is pretty sharp right, so that does not happening reality. But can I pre\nprocess matrix, so that is something like this is becomes more true right and this is\nclever way of doing this right which is as follows. That suppose imagine that the similar\nvalue of linear dk, so now imagine the right certain squad the similar values right; then\nthe new sort of plot for this would be let me change the colour of the pen, the new plot\nfor this would be something like as follows right. If I take it to the power, if I take\nit to the power 4 then it would be even steeper right, it would be even steeper, I mean it\nwould not go could down to 0. So, but basically it would be even steeper\nok, so what it means that if I happened really power the matrix right. Then if sigma 1 is\nsigma k is bigger than sigma k plus 1 right, then sigma k to the power p is even bigger\nthan sigma k plus 1 to the power p right and so on and so forth ok. So, then what we are looking to do is that\nwe basically instead of working with original matrix we work with the power of the matrix\nright. So, if you for on the particular power you work for the matrix is this particular\nexpression A transpose to the power capital P times A. So, if you just do a matrix decomposition\nsingular value decomposition of the matrix A and just plays it here it would not be hard\nfor you to sort convince yourself that the singular value decomposition of this matrix\nis really U sigma to the power 2 P plus 1 V transpose.\nSo, essentially what is happened is that we have kept the same singular vectors both on\nthe left and right, we have just taken the singular values and taken each of them to\nthe power 2 P plus 1. And therefore right therefore, what is happened is that the top\nI mean the top k singular values now, which are i equal to 1 to I mean k 2 P plus 1 right\nhas grown much bigger with respect to the entire set of singular values this ratio all\ni right. This ratio has increases with increasing P right. So, therefore I mean in and this\nis what will do right. And this is under and this is a modification\nof that this is a corresponding modification of the algorithm right. That we what we do\nis that you given a target rank l and A P right and this capital P think of this as\n1 or 2 that sufficient and then we generate the omega as we had said right and in this\ncase we are using this is really MATLAB code. So, we are using omega to be a normal I mean\neach omega each and every omega to be n 0 1, then we multiply A by omega and take it\nis orthogonalization that is my first Q right and then I start of keep on doing this for\nP iteration right. I multiply Q by A star right A star is really\nA transpose A star is the is A transpose of A and then I orthogonalize it and then I multiply\nA by I mean Q by A again orthogonalize it right. So, if you if you forget the 2 orthogonalization,\nthen what we are doing is really doing is really A transpose to the power P right times\nA omega and Q is really orthogonal basis of this right. So, what this look does a it does\nthis repeater orthogonalization in order to maintain numerical stability right and just\nso just to prevent overflowing and so on and so forth right.\nSo, these orthogonalization and more practical reasons, but in reality this is what is happening\nthat I that really I find A transfer to the power P times A omega right and then I take\nthe orthogonal basis of this particular of this particular of this particular matrix\nright and that is my Q right and in I mean remember that in reality we have to take l\nto be slightly larger than k I mean something like a plus 5 is enough and while I will not\nshow the exact bound that we get from here because, that is a little complicated expression\nI will sort of what I will tell you is that it is sort of you something like this. That\nremembered we had A minus QB and I am just writing down the bound approximately, we had\na minus QB bounded by 1 plus k by small P minus 1. So, remember this is the difference between\nsmall P and capital P A minus Ak Frobenius right and now if I instead run this algorithm\nso this was a bound had before. So, now, if I instant run this algorithm this expression\nwill be raised to the power 1 over P something like one over P or 1 over 2 P 1 over capital\nP. So, this is capital P right and this is small p, so this small P is really the I mean\nwhat we are I mean something like 5 which is an extension of the that. I mean the number\nof which is giving me the rank the of the size of this random matrix and it is capital\nP is giving me the number of iterations here right.\nSo, I get to sort of do 1 over 1 by capital P of this of this of this particular error\nfactor right, which means that make it closer to one ok. So, this is really a practical\nalgorithm and it is been implemented in mat lab and in and in and also and several libraries\nare available, this algorithm you can find very nice expression in exposition of this\nby in Haikou Marlene syndrome ok. So, that ends our discussion our discussion of the\nQB algorithm of the QB decomposition. Now, we will move into the application of\nthe application of this randomised techniques, random projection and sampling techniques\nto another common problem in machine learning or a linear regression. So, what is linear\nregression right, so intuitively linear regression is nothing but finding the best linear expression\nthat explains a particular target right. That suppose we have a set of data points, imagine\nthe data points are given in the, imagine that the data points are given in the column\nspace of A in the in the sorry. The data point rows of A rather not column\nof space A I am sorry and now b is the target right imagine b maybe a plus minus 1 or b\nhave some set of values and then what you want to find out you want to find out the\nI mean x is a variable. So, you find out you want to find out A times you want to find\nout the best x right, such that Ax is closed as closed to B as possible right and 1 very\npopular version of this is taking the l 2 difference between Ax and b.\nSo, Ax is this is now sort of size d vector ok. So, let me give number here let us say\nA is of size n by d b is of size d by 1 and so x is of size sorry b is of size n by 1\nand x is of size d by 1 right and can be so the question is that can I find out x right\nsuch that Ax is very close to b ok. So, and this is the l 2 error version of this problem\nthe linear regression. So, if it is pretty clear that if d is much larger than n right,\nif the number of variables is much larger than n then I can make this error to be a\n0 right because, this in that case it is under constraint problem right. So, what we are interested in is it over constrain\nsetting when n is much n is larger than d, which means there is no x that satisfy Ax\nequal to b right. So, what we looking to find out is the best x such that Ax is as close\nto this possible according to the 2 norm right and geometrically this is really the picture.\nThat what we have is let us say we have the we have the space span by the by the columns\nof A and we have B right and so this particular space is a rank d space right and it lies\nin Rn. So, B also lies in Rn B is a point in Rn and what we are looking to find is what\nis the how can I best explain b right by taking linear combinations of these vectors in this\nin this rank d space ok, this is what we looking to find ok.\nBecause and we might not be able to explain b completely because b actually lies outside\nof this space that is entirely possible. So, this is geometric interpretations, so in terms\nof the statistical interpretation what we are saying is that we are find out for the\nlinear unbiased estimator of b ok. So, so how do we solve this problem? So the\nmany different ways of many established ways of solving the problem some. I mean one of\nthese settings one of these is for instance by is by looking at the most popular way by\nlooking at the normal equations right. So, the normal equation sort of says that we can\nlook at the equation A transpose Ax equal to A transpose b, that the optimal solution\nx that we find will satisfied such a equation right. Remember the A transpose A is now of\nsize d by d and A transpose b also rectifies d by d so that optimal x the A star right.\nSo just 2 sort of said the I mean we call x will call x star with optimal x throughout\nthe optimal x will satisfies this normal equation and this we get by differentiating the objective\nfunction and so on and so forth. But once we know this we just need to solve this normal\nequation and we can do it in very various different ways.\nWe can do it in Cholesky decomposition we could do it which really works when an a is\nvery well condition, we could do it using QR decomposition of A which works in a is\nnot so well condition, we can also do it using an SVD decomposition of a similar value decomposition\nof a right. We really works for all A but it is also pretty expensive right. So, the\ntime taken for each of them order nd square theoretically all other constant really differ\nin I mean depending on the composition that we are using. So, can I speed this up this is a question,\nthe here is again the prototype randomized algorithm right an then let us try to understand\nwhy this potentially works and then we will try to see this is really efficient. What\nwe want to say is that instead of I do solve this long thin Ax is equal to b Ax equal to\nb, let us try to reduce this the size of A. So, how do I reduce the size of A I multiplied\nby a random matrix omega right so and let us say that the size of omega is s pi n right.\nSo, I multiply both A and b by this by this random matrix omega, so therefore now I have\nsort of a problem of size s by d which is which is a which is omega times A right and\nthen we are stills solve A, I try to find the x and we try to make it close to a vector\nof size. Now a vector of size s right this is omega times b this is the new regression\nproblems that we are solving. This is what we are says taking that suppose if multiply\nboth A and b by the random matrix omega and then we try to solve the new regression problem.\nSo now, this is much smaller regression problem, therefore it is more coefficient right. And\ntherefore, I mean if I solve this I mean the question is that do I really get a good approximation\nof the original solution and so that the tentative claims that we want something like this right. Supposing x tilde is the is my is my is my\noptimal solution here and of the of the smaller problem right and x star is my is my solution\nof the original problem. So, that what I really want is that if I take x tilde and if I plug\nit in the original problem right and see what the error is, remember that this error is\nupper bounded by Ax star minus b right because by definition of optimality ok.\nBut what we say is that is Ax tilde minus b and all of these are 2 norm whatever I am\nnot written it is the vector l 2 norm, the tentative claim that we are going to say that\nclaim that we going to make that Ax tilde minus b is not much more than Ax star minus\nP to the rank 1 plus epsilon and this epsilon will of course of course control the size\nof s, the size of omega s the size of omega. What is the running time of this algorithm\nwell that comes a bummer right because, now if omega really is n 0 1 right. If any if\nentry of omegas comes from normal 0 1 then size then doing matrix vector multiplication\nomega times n takes time nd square and so our algorithm is not any not any faster. So,\nis it really useful right it is not faster is it useful. Let us let us understand why it is potential\nuseful right, in order to see this we need to understand the geometrical picture a little\nwell right. So, remember the pitcher that we do the that we have this span of A star\nwhich is really a rank d space in Rn. So, this is a subset of Rn but it is it is a rank\nd space and here we have b which is a point in Rn and what we looking to find is really\nthe projection of b onto the column space of A right and this error and this is and\nthis is my error or the optimal error right; which is Ax star minus b which is basically\nthe norm of Ax star minus b. So, now what we did was that we projected\neach of these A star i each of these vectors A star i into a smaller dimensional space\nomega A star i right. So, this is now a subset of Rs right depending on the SR b of and similarly\nb has been projected from Rn to Rs. So, now this picture gets little right because the\nbecause of the random position because of the random sort of projection, I mean if the\nif sort of the position of if this was if this is really the projection of b on A right\nand this is really the omega times the projection of b on A rights. The optimal might be the\noptimal projection might be somewhere a little different right than the than the previous\npoint right. But the point is that they all these plotation\nare really small right, I mean if the this is the new projection of omega b onto the\nonto the column space of omega A star i it is what we are going to claim, is that it\nis not very far of from this previous projection right. So, therefore the linear I mean the\ncomponents x that sort of created that is create this new projection is not a not very\ndifferent from the components x that that get this from the new projection ok.\nSo, mathematically here is what is going to be that if you look at the I mean look at\nthe normal equations this is this is how we get the optimal x, that than I take A transpose\nA see imagine for now that is rank d, I mean A transpose A is a full rank d matrix we can\nextend all of this to the to the rank diffusion case easily. So, then x star is really A transpose\nA inverse time A transpose b. Which is the, which also be the witness the\npseudo inverse A rank b right. The optimal solution for the for the sample problem is\nnow instead of A I replaced it by A pi omega times A right and therefore it is a it is\nthis just particular quantity right. So, this A should not be here right, so I mean it this\nparticular expression just by looking at the normal x the normal equation of the of the\nsub sample of the projected problem. Now, at it is core what we are going to mean\nwhat this proves says is that we can prove matrix concentration type inequality, to show\nthat this particular matrix has a 2 norm which is less than epsilon ok. So, remember what\nthis I mean just to un make you understand what this what this matrix is, this matrix\nhas I mean this particular matrix has a 2 norm which basically I mean not less than\nepsilon which is less than epsilon right ok. So, what it means that U omega omega transpose\nU transpose is more or less similar to UU transpose. So, remember that UU transpose\nis the projection matrix right, so therefore it has a singular set of singular values 1\nand then and then 0 right. So, it is d similar value that are 1 and everything else is 0\nright. So, it is a n by n projection matrix and this is a this is again n by n projection\nmatrix right. But it is an n by n projection matrix of much smaller rank of rank s right\nand what you gone to say is that, is that if you can look it singular values all of\nthem. In the non 0 singular values all of them are going to be about 1 plus minus epsilon\nin the range 1 minus epsilon to 1 plus epsilon this is what this particular bound says and\nwe can prove this using matrix concentration inequalities.\nI mean even using the matrix multiplication theorem that we did earlier and the randomized\nmatrix multiplication theorem that we did earlier and sort of plugging in suitable values,\nbut it is not right. So, then so then I mean it is so if I mean again then what is the\ns or other what is omega that we need to choose. We need to choose omega that essentially satisfies\njl lemma with factor with a error bound epsilon and with delta to be to be like delta is a\nis said to be like 1 over d or 1 over poly d right. Which means that if you take I mean if you\ncertainly, if you take the size of omega to be if you take size of omega to be something\nlike d log d by epsilon square it is sufficient ok. So, now comes the question that this is\ngiven with s to be something like d right. We show that we show that the random I mean\neven if s was d then multiplying omega times a takes time nd square right. So, if s is\nd log d epsilon square then it even slower right which is slower than solving the linear\nregression problem itself. So, what we are doing anything useful right. It done for the solution to the this is something\nwe have already seen right, we have seen this randomized Hadamard based random projection.\nThis randomize Hadamard transformation it is going to say right and just recall for\nthe randomize Hadamard transformation was we say that we take the Hadamard matrix, which\nis really define by this Hadamard square root n H n and H n is to find in this in this recursive\nmanner ok. And this is really that normalized the normalized\nversion then we take the diagonal matrix plus minus 1, that that randomised transformation\nright and further more instead of the I mean after that we had a sparse go symmetric, we\ndo not really need a sparse go symmetric right now what we can do is that we can just take\na sampling matrix. So, what is really a sampling matrix sampling matrix is a something very\nsimple we say that for it is a s by n matrix right and for generating every row of this\nmatrix we toss a we toss a n sided coin right and the coin says that for this matrix for\nthis a for this row I am going to put a non 0 at the ith position right and everywhere\nelse will be 0. So, the ith position you just normalize I\nmean there is a normalisation factor which is this square root n by s x and you put in\nsquare root n by s are the ith position and you put in the everywhere else 0 everywhere.\nElse say every row puts a select only one position try to put in a nonzero entry that\nis it. So, this is and this is and this happens and happens with equal probability for all\nthe rows and all the and all for all the positions and all the rows adjust IID adjust IID random\nvariable from the same distribution ok. So, this is my matrix this is my omega P H D. So, now it should feel is simple to say that\nthat calculating this P H D takes time is much faster right, that calculating this P\nH D takes time only in d log n right plus sn this is the time for protection and s and\nas I said before it is enough to take s to be at least c log d. I mean C to be d log\nd by d log d by epsilon d log d by epsilon and then we can sort of ensure that we get\nan epsilon approximation 1 plus epsilon approximation right. That the x still other I find out from\nthe smaller problem is really one plus epsilon approximation to the x star to the to the\noriginal problem right. So, the time taken to solve the final problem\nthe smaller problem right so, first so the total time taken is the taken for the projection\nfor the time taken to solve this problem right. So, time taken for projection we are already\nmentioned, the time taken to solve smaller problem is that sd square because. Now I have\na s by d s by d matrix s by d linear regression problem, so therefore the overall times. So,\npugging in the value of s the overall time something like nd log n plus d plus d cube\nlog n by epsilon plus some other terms right and this happens be faster than solving the\nlinear regression for if n is very large and these are the an these are the guarantees. So, this is the guarantee that we have seen\nbefore, further more we can also guarantee that the I mean under certain cases we can\nalso guarantee that the solution x tilde itself is closed to x star right. But, for the this\nwe need to make sum assumptions we need to because this bound is depended on the condition\nnumber of A, remember condition number of A is the ratio of the maximum singular value\nto the minimum singular value right. So, the x minus x star depends on condition number\nof A, it also depends on how much of b is present in span of A, it is dependent on this\nratio which is depend which is the data dependent data dependent bound of course. So, just to mention the extensions that I\nmean we have shown the this particular random projection we can extended it to take care\nof the Sparsity of the problem right. We can make sure that we I meant once we use the\nsparse Jonson Linerar Shros or it is variant I mean there is a more interesting variant\nalso called as optimize Jonson linear transformation subscription projection, then the theoretical\nrun time matrix depend on Sparsity of the problem.\nHowever, I mean it is not clear the that is entirely I mean a stable algorithm in practice,\nI mean the questions of numerical stability are still yield to be resolved is not been\nany good experimentation there as far as I know. We can also look at there is also be\nresults about regularized version of this problem and specifically about the of the\nl 2 regularized versions. There has been there has been some results\nthat says that if we using regularization, we should actually be able to project although\nmuch smaller on number of dimensions right. There is a very nice implementation of it\ncall Blend and pick which shows that that at least for a class of matrixes thin rectangular\nmatrixes. I mean there is implementation of this randomize regression that can beat the\ndecades old optimized LAPACK routines for a in terms of performance for this class of\nmatrices right. And the so just an most of the lecture just\nto give the reference, most of the lecture was from these lecture notes by Petros and\nMichael. And, if you are interested in looking at the papers you should look at the paper\nby blend and pick by hibernate all, as well as the other references that will be put on\nthe webpage right. Thank you.", "hn00PydWK_4": "hi welcome to harvard applied math 205 a graduate course in scientific computing and numerical methods i'm chris rycroft and in this video we're going to introduce unit 2 of the course on numerical linear algebra we'll provide a few motivating examples introduce some key linear algebra concepts and provide an overview of some of the topics that we'll cover in this unit almost everything in scientific computing involves numerical linear algebra and we can often reformulate problems in terms of solving linear systems of the form ax equal b and we've already seen a number of examples of this throughout the course in the data fitting unit we found that we could reformulate polynomial interpolation in terms of solving van der monde matrix systems and when we looked at linear least squares we found that the normal equations naturally arose that we could again solve using matrix algebra even when we look at nonlinear problems we find that numerical linear algebra is still important and when we looked at nonlinear least squares problems we found that the gauss newton and level marquette algorithms could solve these problems by approximating them in terms of a sequence of linear systems to solve and we'll see these themes arise throughout the rest of the course all of the remaining units on numerical calculus optimization and eigenvalue problems will all touch on numerical linear algebra so in this unit our goals are to first review some of the key linear algebra concepts that arise in scientific computing we'll then look at algorithms for solving matrix problems of the form x equal b in a stable and efficient manner and we'll also look at various useful factorizations of matrices such as the lu factorization and the qr factorization but to begin let's take a look at a few practical situations where matrix systems naturally arise in the modeling of physical systems so our first example is taken from electric circuits and here we can make use of two fundamental laws firstly ohm's law tells us that if we have a resistor with resistance r and current i flowing through it then the voltage drop across that resistor is given by vehicle ir kirchhoff's law tells us that the net voltage drop around any closed loop is zero and suppose we now take a look at this circuit shown here we've got two batteries v1 and v2 and six resistors and if we wanted to find the current flowing through this circuit then we could break it down into three components flowing around the three loops so we could have i1 flowing around loop one i2 floating around loop two and i3 flowing around loop three and we would then say that the current in this network is given by the combination of all three of these loop currents so now let's take a look at loop one so if we look at r1 then we'll just have i1 flowing through it if we look at r3 then we'll have both i1 and i2 flowing through it because that resistor is also in loop 2. and if we look at r4 then we'll have i1 plus i3 flowing through it and that then allows us to write down a linear equation r1 times i1 plus r3 times i1 plus i2 plus r4 times i1 plus i3 and that should be equal to v1 and if we do that for the other two loops then we end up with a matrix systems to solve for the three currents i1 i2 and i3 if we solve this linear system then we can determine the entire current flowing through the network and while this is a simple example circuit simulator software can actually allow us to scale this up to large systems of practical importance another example is in the field of structural analysis and here a fundamental tool that we can use is hooke's law and suppose we have a spring or some other structural element that has inherent stiffness k that we refer to as the spring constant i suppose now we apply a load f to this spring then the displacement x of the spring will satisfy a linear relationship f is equal to kx and this is hooke's law and this is usually valid over a certain range of loading sizes f now from the scalar equation we can look at composite structures that have many components that are all connected together and that will lead us to large matrix systems of the form capital k x is equal to f and so here the matrix k is referred to as a stiffness matrix and it encapsulates all of the connections and stiffnesses between the different components of our structure and x is a displacement vector that tells us how all parts of our structure will deform and f is a load vector that tells us about all of the loads that are applying to different parts of our structure and as an example suppose we look at trying to predict the surface of a bridge under load so this bridge could have a number of supports and we could then want to predict how the bridge surface would deform so we'd have here a stiffness matrix that would encapsulate all of the stiffnesses of the bridge surface and we could apply a load from the bridge weight and also perhaps cars and trucks that are traveling on the bridge and we could then predict how the bridge surface would deform and this type of problem would really help us optimize our bridge design for a particular purpose and there's commercial software available such as sap 2000 that can allow us to do this type of problem for very large structures such as office blocks and skyscrapers another example is from the field of economics and in 1973 lontieff was awarded the nobel prize in economics for developing a linear input output model for the production and consumption of goods and suppose we look at an economy where n different types of goods are produced and consumed then we could introduce a matrix a where an element a i j would represent the amount of good j required to produce one unit of good i we could also introduce a vector x of the amounts of each type of good that are produced and a vector d of the amounts of each type of good that are consumed and in general if we look at our matrix a then we would expect that the diagonal terms of this matrix will be zero because we would not require a certain good in order to produce a unit of that good so suppose we now look at the amount x i of good i that we need to produce then that will be equal to the inherent demand of good i plus the amount of that good that is required to produce all of the other goods that are in our economy and we can write down therefore a linear relationship where we say that x i is equal to a i 1 x 1 plus a i 2 x 2 up to a i n x n plus d i and if we look at that over all the different i it will lead us to a matrix system of the form x is equal to ax plus d and we can reformulate that into i minus a applied to x is equal to d and we could therefore solve that linear system to determine the amount of each good that we need to produce and while you could look at some simple examples of this you can imagine that for a large complex economy we would have a very large matrix system that we need to solve these three examples taken with very different fields highlight how matrix computations can emerge all over the place and in numerical linear algebra it gives us a toolbox for solving these matrix systems in a stable and efficient manner and nowadays we're very fortunate that many numerical linear algebra algorithms are available to us in libraries in languages such as python and matlab and we can often use those library functions as black boxes without knowing exactly what they're doing but it's often really useful for us to understand exactly how those library functions work that can help us choose the right algorithm for a particular purpose for example suppose that our matrix has some special structure perhaps it's symmetric or perhaps it's banded then we might be able to use an algorithm that exploits that structure for better performance and stability in addition if we understand the different algorithms of work then we can understand the different error properties of different algorithms and that can help us select an algorithm that is valid for our particular application so here we're going to look at matrix problems in the form ax equal b and we're going to focus on square matrices a of size n by n and if we look at a matrix problem ax equal b then we can think of the matrix multiplication ax as taking a linear combination of the columns of a with weights given by the components of x xj so specifically if we write that b is equal to ax then we could write that as the sum from j equal one to n of x j multiplied by the j column of a and here we make use of the notation the j column of a is written as a colon comma j where the colon indicates that the row index can run over the entire range this can be displayed schematically then as saying that the vector b is equal to the matrix a that we write in terms of its separate columns multiplied by our vector x and we can write out that multiplication saying that we would then have x1 multiplying by the first column of a plus x2 multiplied by the second column of a up to xn multiplied by the nth column of a and we can therefore interpret ax equal b in terms of saying that x is the vector of coefficients of the linear expansion of b in terms of the basis of columns of a and this is often a helpful point of view that's slightly different from the more conventional approach of thinking that every component of b is the dot product of a row of a multiplied by the vector x and from this linear combination of columns viewpoint we can immediately write down some facts firstly we can see that the matrix problem ax equal b will have a solution if b is contained in the vector space that is spanned by all of the columns of a and we can write down some useful notation where we say that image of a matrix a is equal to that vector space spanned by a's columns so now let's look a bit further at existence and uniqueness so we can say a solution x will exist if b is contained in the image of a and if a solution exists and the columns of a are linearly independent then we know that the solution is unique if a solution exists and there is a vector z that's non-zero such that a z is equal to zero then we can deduce that a applied to x plus gamma z for any real gamma will also be a solution and therefore we'll have infinitely many solutions if b is not in the image of a then a solution will not exist to x equal b we can also introduce the idea of an inverse map a inverse and that will be well defined if and only if the linear system ax equal b has a unique solution for all b and from a different viewpoint we can introduce a inverse matrix a inverse and that will satisfy that a multiplied by a inverse is equal to a inverse multiplied by a is equal to the identity and that will exist if any of the following conditions are true the determinant of a could be non-zero the rank of a is equal to n or that for any non-zero vector z a z is non-zero and that essentially says that the null space of the matrix a is just equal to the zero vector and we say that a is non-singular if a inverse exists and if that is the case then we can just solve our linear system by saying that x is equal to a inverse applied to b if a inverse does not exist then we say that a is singular", "EVHty2VwnoQ": "so a warm welcome to this part 1 of the webinar series about a sink wait today we talk about async/await best practices so I'm done in marble I'm a solution architect I call myself an enthusiastic software engineer I'm a Microsoft MVP for systems integration I live in Switzerland currently it's snowing and quite cold in Switzerland but if you want to know more about me I recommend you to listen to episode 77 of the develop rom file podcast you can reach me on twitter under at daniel marble and the block under the particular blog particular net slash blog and on planet geek dot c h which is my personal bro look i hope you guys subscribe up to this webinar so in this way a bit more we learn important terminologies around 18 Kuwait's we learn the difference between cpu-bound and io bombs we learn the difference between frets and tasks and we learn a few async best practices this webinar is divided into three parts the first part is the terminologies the second part we talk we talk a bit more about coal we dive into the codes and at the end we have a wrap-up with a Q&A session don't hesitate to put in your questions during the webinar into the question question field in the GoToWebinar settings and we will be collecting them and answering them at the end of the webinar in the Q&A wrap-up session so let's first talk about important terminologies around async/await the first one is of course async so ASIC in the real world can be compared to me doing the laundry I put my dirty clothes into the machine and select the program or timer and let the machine do its work until the laundry's done indicated by a beep of the machine I can carry on with other things like reading the newspaper playing with my son or whatever I feel like doing so I asked the worker I'm free until the external task resource like here the washing machine is done and has indicates indicated that back to me this is very similar to software a software or an asynchronous program dispatches the tasks to devices that can take care of themselves so that leaves the program free to do something else until it receives a signal that the results are finished asynchronous programming should be used for external operations which which support event-driven callbacks so when they are done usually this is the case for i/o bound work for example on Windows you have a thing called I completion ports I complete reports indicate and an i/o operation is done successfully or not successfully back to the initiate initiator of the asynchronous i/o operation async operations can be more efficient since the worker initiating the work are not blocked let's talk about tasks I'm talking out about the system threading tasks loss system threading task is an abstraction layer which represents both CPU bound and i/o bound operations as a uniformed API eyes I define a task as a it represents the states and the outcome of an asynchronous operation executed now later or never this is very similar to washing my closest if a task represents an i/o bound operation then we could compare it to the laundry machine so the state of the task or here the laundry machine is running not running or completed the outcome of the task hopefully is clean closes I can late start the machine with a timer or the machine can decide to run a health check before the process starts it's also possible that because of failures machine will never start and I will not notice it until I come back or even more likely my wife Kansas the process because yet again I have chosen the wrong temperature that has never happened to you hopefully so let's talk about CP Obama tasks see if you bomb tasks can be compared to me doing the laundry manually while I'm doing the laundry manually scrubbing my closest with some soap I'm completely blocked I cannot read the newspaper or do something else in the meantime no matter whether the task is IO bound or CPU bound frets are the workers responsible for getting tasks done that are currently scheduled on the current task scheduler but with CPU bound tasks the worker fret is completely blocked so that means a thread can only handle one CPU bound task at a time in contrast a thread can handle multiple i/o bound tasks concurrently concurrent what does concurrent mean concurrent is like me while doing the laundry walking into the kitchen I put my newspaper away fill the remaining dishes into the dishwasher and start the dishwasher and go back reading the newspaper so I'm doing all that concurrently in software this means there is a call there's a coordinator called scheduler which kills usually on a single threat with multiple concurrent work items these work items can be interleaf what they don't have to pending on the scheduler work can also be distributed over threats and processor but it doesn't have to parallel on the other hand is can be compared to the laundromat multiple people can simultaneously wash and dry their closes in the laundromat in parallel in software this means a parallel program is divided into independent sub expressions that are evaluated simultaneously on different processors of threads the goal of parallel computation is to finish the computation much faster parallel computation is ideally suited for CPU bound work such as sorting data in memory filtering data and so on to be able to benefit from parallelism the problem itself has to be able to be divided into individual sub items which can be worked on independently you can achieve parallelism by using concurrency constructs but not the other way around so let's talk about continuations when the laundry is done I can take out my closes out of the laundry machine and dry them in the dryer so the drying process is a continuation of the laundry in an asynchronous programming model a continuation is or at least to my definition a function that is scheduled for execution after its prerequisite function has completed so let's dive into the code because we already have set the baseline with all these terminologies so async await async await our keywords they are here to reduce the nesting of continuations and I call them also Christmas trees or I called nesting programming Christmas tree programming so async await ASCII words that I drastically simplified your code so in essence we can say async await is basically syntactic sugar mounted with compiler magic so let's see in our laundry example this means instead of writing clean laundry continue with dry laundry we could write just this oh wait clean laundry dry laundry that's all there is to know but let's see how this essentially reflects itself in code so we have here a test called cpu-bound and we have here everything you can use to do CPU bound operations so we have here parallel 4 which allows me to execute the CPU bound method a thousand time on multiple threads power for each is basically similar to parallel four but it operates directly on an enumerable as we can see here we have tasks to drum with a CPU bound method we should only use tasks to run with CPU bound operations not with IO operations which we will see later the same applies for tasks that factory does start new it should all only be used for CPU bound methods so so as you can see because we have a uniform API the await keyword can be used to await these CPU bound tasks as well I'm not going to execute this code because this would take too long so let's go to i/o bound work we have here an i/o bound operation which is called IO bound method so this I about methods creates a file stream creates a stream writer and write something asynchronously into that stream writer which represents the file stream what happens is the unit testing thread here entering this a weight method everything here so the using statement here the using statement here will be executed synchronously on the phret entering this method so therefore unit testing threat as soon as we come here to the first weight keyword you can see it like each weight statement in your code is an opportunity for the entering threat to go back and do something else a sink like me doing reading the newspaper during the laundry process so da that means in in theory the unit testing thread could exit here to something else and as soon as the I completion port indicates that this operation bright line async is done it comes back the writer is closed the stream is closed and the writer and the stream are disposed so let's execute this of course there's nothing surprising here so so let's see what the task based API allows you to do to do even more we create here Anna Newell of four tasks delays which will wait 1.5 seconds and then we await the outcome of these tasks sequentially so what does this mean so this means this test is going to last for approximately 6 seconds okay let's zoom in as you can see sequential turk approximately success six seconds to execute but since the tasks API is much more composable what we can also do is we can execute this task concurrently if our program program logic allows us to do this so let's see an over that range we select we do tasks delay again of thousand thousand five hundred milliseconds and then instead of doing it for each away task like above here we just do away tasks when all concurrent so this starts all these tasks concurrently and then we await the result of all these concurrent tasks with a single call and so and as we can see this this now takes only one point five seconds so allow me allow me to to go to the next example let's let's talk about async white an async void operation is the following method we have a void method which is which is marked as a sting this void method does nothing else than just console.writeline toss delay comes a right line and then it froze an invalid operation exception we have a surrounding code here try catch which catches the invalid the operation exception so let's see what happens with it with this code when we execute it we execute this test as we can see going to going inside async void going to throw but the test didn't fail and we don't see the console.writeline here hmm that's strange so let us let us let us dive into what async code actually means so async void was introduced for event handlers a merrily because these are event handlers like in Windows forms of WPF applications these are top top level events and if you want to write an a weight statement in a top-level event handler the only thing you can do is mark it as a sigmoid when in event handlers exceptions are raised these exceptions are racing the background on the current synchronization context the same behaves here when I have an async word here in my basic word method here in my test as you can also see from the from intellisense this method is marked as void so I have no indication as a color of these methods that this method I cannot do track try catch around it so the exception we just raised here it's raised behind the scenes in the background and there is no way or almost no way I can I can intercept it exception in fact there is a way I would need to write my own synchronization context but this is out of scope for this webinar so the only thing or the best thing I can do do is I switch our return a task I await the method here and then I execute this this test again and now we can see the exception was raised it was catched by the tests try-catch and we have the console.writeline which represents the invalid operation exception with the gotcha message inside so what happens underneath is that if we return a task instead of async white the compiler generated code will make sure that any exceptions raised in the asynchronous method will be captured with an exception dispatch info and will be added to the tasks internal exception dispatch info list and on each and every await statement in my call stack the compiler generated code will reflow the exception to the caller so never ever use async words only only if you have to select in event handlers but if you're writing your own code your own library you should really really avoid a async void you should always return a task which because it allows you to better to better compose those methods together and you will not be running into surprising error conditions like we've just seen here in this code so now let's start about configure weight and configuring the context I have to start here an application and windows forms application to explain it a little bit better so I'll attach the debugger to the external process and we step into this windows forms application so this windows forms application here has several await statements and what I'm doing here is I'm I'm visualizing the synchronization context so as you can see here when I enter this application the synchronization context the current is set when we step over this code so synchronization context is not now now we step into the first async methods we step into the first async method as we can see the first line is still executed on the same thread and drink that method so the synchronization context is not known now comes the first away statement when we write after a way to configure a way it false what that means is the underlying state machine will not do context capturing so it will not capture the currently executed synchronization context so this one here will not be captured and it will not be restored when the continuation which is this method here will be executed so let's see that so as we can see I added FF 10 and now I'm I'm in the continuation of this previous line and here now the current synchronization context is not the problem is of this configure wait Falls is that I have to write configure wait false for each and every await statement I want to disable context capturing by default configural way true will always be there so if I don't write configure wait false the default will be configure away true and as we can see if eyes debug step step out of this method now here because didn't have a configure wait falls the complex is restored and the current context is no longer all it's the same context again now we step into this the same method and right here on the top level configure wait false we dive into here remember this this line is still executed synchronously on the calling fret we go into the first of wait we disable context capturing we go into the next wait here either I don't have a configure wait false but here as you can see the continuation of this method is this method so here the synchronization context is is null so what does that mean for us every time we write configure away with false this means the the threat executed can essentially be scheduled on the thread pool because and the synchronization context doesn't need to be captured in a UI application this means every time I use configure await false the user in the threatens face threat will not be hammered with with tasks when the continuation comes back so essentially depending on the stuff I'm doing configure await false can optimize also my windows for most applications so but this comes it becomes even more important for if you're writing a library if you're writing a library and you don't know where your library will be used you never know it's important that each and every wave statement in your library uses configure wait false so that on the top level the one who is using your library has the ability to decide whether they want to do complex capturing or not and if you if you complex capture internally and you're doing it wrong like I'm I'm showing you in the next example your risk get in your dead look this brings us to the next basic way best practice which is don't mix blocking and basic code so I set here the synchronization context is similar simulates adult PF application I have here a synchronous method which is using a way to toss the delay milliseconds and someone said yeah you know what I want to write the synchronous wrapper which is internally using the asynchronous version because here I've already written the logic why not just three use it and do a blocking wait or a dot result call now that's the simple implementation isn't it but let's see what happens so if we execute this method here is test let's execute this test as we can see this test will be running for the reminder of the webinar and we'll never complete so what can we do to mitigate that first of all don't write synchronous wrapper over a synchronous code it's as simple as as as it sounds so what it means if you're writing a synchronous code everything in your call stack should be acing should be returning a task and depending what you have a weight statement or not should be marked as async that's it we could mitigate it here by applying the best practices I told you before by doing here configure wait false if we do that the contacts will not be captured and the test will will execute and finish in no time but still instead of risking to run into that lock code it's better that you just write async codes everywhere if you need a sink and if you need to benefit from i/o bound operations so let's go back to the slides that's all I wanted to show today when it comes to acing best practices so let's go to the wrap-up as we've just seen I was just using the task-based api's so what that means for you stop thinking in France for the most applications threats are no longer relevant thinking tasks and rest assured that the tasks parallel runtime library is heavily optimized for most productions scenarios you don't need to worry about that what does this mean for an service bus or for the next yet to be released and the response version v6 we applied the async all the way up and down best practice as well in the answer response receives code base we also apply to the other best practices like the configure await false consequently through the code base throughout the code base of engine response we six and we also have a rosin analyzer written by us which checks that everywhere we have on the wait statement we essentially write this configure await false I will be linking the rosin analyzer in the show notes so that you can have a look at this analyzer and maybe also use it in your project so let's do a brief recap you started run or tested factory to start new for CPU bound work use tasks directly for i/o bound work and use async task instead of async work void in your code libraries and frameworks should be using configure await false for each and every await statement and please try to make sure that you're all the way I think all the way and you don't mix Lockean asynchronous code because the consequences could be depending how it is used that the code is going to deadlock in the next webinar we will dive deeper and write the message pump for the service bus which combines async/await with the task parallel library we will learn the task peril library contains some unpleasant surprises and how we can work around those so please join again and they maybe invite your friends and learn to apply more than just the basics of aces await and by the way since you're here you have already signed up you don't need to sign up again for the webinar but if your friends sign up for the webinar after this webinar they also get the recording sent to them by email so they can catch up for the next webinar the next webinar will be using the things which we've just seen today as a foundation level and will be assumed in the next webinars all the slides have just shown here all the links and also the code you've just seen here is available on my github account under the four following address we will also send you this address in an email after the webinar so let's dive into some questions unmuted so Danielle hello everyone we have an interesting question from Miguel that is asking when should I use the asynch and await keywords and cotton instead of the end service bus publish and subscribe button ok I'm going to I'm going to answer this question this particular question in in the third webinar or when I show how enter response we six can be combined in a message handler with the new async API and leverage the async await keywords too for example if you're doing a give a brief explanation if you're doing multiple web service calls or multiple resource IO resource calls in the same handler you can essentially do those concurrently but I'm going to show an example in the third webinar there's another question from Alberta that is asking if the in the next webinar also are they going to speak more about execution context and synchronization contexts I haven't planned to talk about synchronization context and execution context because I think it's it's way too deep for this three-part webinar series but if people like me like to drive me into those let's say implementation details synchronization complex and execution context I can definitely do this so please provide some feedback please hammer Britt King and now we'll do a fourth or even a fifth webinar if people like it thanks there's an another question from Madhava who biomass has been incorrectly that was asking which tools are you using what the backing and testing the the demo that you show I always get this question so I was using the u.s. code debugger extension there's another question from Patrick that is asking all doesn't service post benefits from a sink away given that messages must all or queue semantics first-in first-out if a queue is configured for maximum tried how does releasing the trader to IOC be hub okay so um and the response as it is a messaging framework and ties into a transport life as a service bus RabbitMQ and other transports it's a heavily i/o bound domain so what that means is previously we we have been using multiple threats to to consume the messages now if the if we six we can essentially because everything is i/o bound and then everything is using a task-based api the thread consuming or picking up the messages from from the queue is free to do to pick up other messages of course you can control that that by setting the max concurrency so we allow you to say okay I only want to handle one message at a time or I want to handle multiple messages at the time but that means the new version can use the resources much more efficiently on on the boxes and service bus is running and this becomes even more important for things like the clouds I will be diving more into this in the third webinar and explain it in more details thanks I think this two question that can be related one to each other one is from Jack that is asking could you give a brief recap on what synchronization context actually years and the other one is from Brighton asking I'm trying to run a WPF application without the UI and I've noticed that we are seeing a with functionality is significant maybe friend would you have an explanation about that mm-hmm okay so I usually try to explain the synchronization context it's it's like it's like a message pump or or or an inbox a single threaded one so so every operation which has to be joined back to the user interface thread is essentially guarded by the synchronization context you have one synchronization context usually / / windows forms a WPF application of course if you have things like splash screens you end up having multiple of synchronization contexts but let's assume we have one so what happens is the asynchronous operation is included on the synchronization context and behind the scenes and thread is one by one getting each of these include operations and execute them and therefore the access to the user user in user interface elements is guarded is single threaded the the question but that being said I mean if people like me to dive more into synchronization context and execution context I can naturally do that in a in a fourth webinar in a deep dive Thanks is there another question from Patrick that is asking if you have a deep cold stock where you are doing a sink await all the way but none of the meters need to observe the task resulta awaiting is it better to the third task without the waiting until the end or Marco limited a sink and the way each middle coal it's it's better to return I can I can briefly show you an example and the influence of the of this so let's let's let's say we have killed like this with two methods so let's see these two methods so the one is the one is doing a weight and the other one is just returning the task so if you have a single operation a single asynchronous operations in the method in a call stack and you're not using things like try-catch you're using blocks that's important then you can expect it to just return the tasks because what happens is your sense with shortcutting I'm not marking it as a sting your short cutting the state machine generation behind the scenes so let me briefly show you what happens behind the scenes I have it here decompiled so what would happen in the first example does not shortcut this is the cold which gets generated so it's create creates internal classes which capture the state and the state is restored but if you just returned the task as you can see the generated code behind the scenes doesn't doesn't have the state machine bloat like here so it's more efficient it's awesome more memory efficient if you just return to us if you only have one one one asynchronous operation if you have multiple asynchronous operation usually for the majority of the applications it's simpler to Church market as async and do in a wait but there are other benefit is since we don't wait there is no complex category so here in this in this example the context will not be captured the context will only be captured when we have in a wait statement I hope that I am is just a question oh there's a full-up from brass that was asking about the WPF application without a UI but I think that it's much easier to handle it offline and instead of diving into that specific issue the last question that I have is well I lost it sorry if we have a v5 and lab that needs to use our sync libraries that I say like our seeing EF queries what's the best way to avoid the deadlock and will this allow and and service possibly five and the more messages concurrently and the new message while of wait for any operation to complete so the short answer is no but let me briefly show you something so for people not familiar with an service bus so when you handle a message you essentially write a class public class my handler and you implement an interface I handle messages and here is the message you want to handle that's the basic principle what that means is you have to implement a method which is called public wait handle and you get here the message instance message so but it's also I'm going to explore a little bit more but the worst thing that you can do is here async words remember I'm going to show what happens exactly if you do that in the third webinar so never ever do that so if you have asynchronous code in here what I can advise you to do is and that also prepares you for the next version is to do something like this private async task handle internal my message message I boss let's call it context you'll see why I call it context here and then you write your async code here and then what you do on the top level we call handle internal you're passing the message you're passing the boss and you do get a waiter get result so what's the difference between get away to get result and get away to dot story handle internal weight difference is weight will throw an aggregate exception essentially wrapping the original exception so if this code will will flow with some entity framework concurrency exception the concurrency exception would be wrapped in turn aggregate exception and you would see in the log statements or in the air queue on the headers of the message you would see this aggregate exception if you write this one here it will extract the inner exception and we throw the in exception the currency exception so I suggest use use this one but you won't get you won't get more performance more concurrency because this is still a blocking operation you have to wait don't will be six thank you any other question the last one is from dominant man that is asking about still about the civilization contacts and the asking for example in Web API how do I know whether you use configure await falls and what context do I use after okay so I'm not an expert in Web API so um Maru you might be able to help me here but as far as I remember we're forever API I think you shouldn't be using configure wait falls if you want to get access to the HTTP context of current but I think Mario you can help me out here yes if I record correctly that's the main reason to not use configure it force become current hu the context is captured by the dissing call Deep Web API custom synchronization context so if you need to access during the a sync pipeline the HTTP context and that's me maybe it's desirable then you don't need to call configure it false but I if I can I can dive deeper into this and write a detailed answer because I I'm not an expert in the API any more questions there's the last one that is if I'm developing a library and want to expose in a syncope I is it a good practice to provide the Nonna sink methods as well and what advice could you get how to avoid code duplication mm-hmm okay so the answer is you might want not to hear this answer but this is the truth so my personal opinion ated opinion is enough in I've been wandering around in the open source community and telling people that is if your library is in an i/o bound domain or primarily i/o bound all the i/o bound paths should be async only and should not be providing synchronous synchronous methods because I mean let's face it more and more applications and systems will be using api's which will be async enabled if there are Iowa so I think we will be will be seeing less and less usual usage or demand for the synchronous a for the synchronous API and it's pretty easy to write synchronous code into async code I can show this in the way in the next webinar so in order to avoid code duplication the answer is only provide day sync version because then on the top level users can all always synchronize or synchronously call this code like I've just shown in the previous example with the end with the answer response handler but then it's the decision of the user to decide whether they want to call this synchronously asynchronously or whatever they want to do with these api's and you don't risk getting issues filed on your issue tracker about blocking costs or bad looks and that's my recommendation okay we are approaching the end of the webinar so let's so thank you very much for listening to this webinar and if you have more questions feel free to ask them in the next webinar or you can also reach me on Twitter act on your mother walk or write me an e-mail at Daniel dot Margaux at particular net and hope to see you next time", "07nyPOHeLzY": "hello and welcome to another episode of Friday night kitchen for another episode of Friday night kitchen Friday night kitchen Friday night kitchen halloween special or friday night kitchen today we're going to make a chicken and tannin and pie quiche Lorraine in three different dishes and Tom scaring Trick or Treat muffins this week I've got a bit of a cold it will be to boil Paige Thomas according to the instrument whilst it's Oscar no see I'm gonna cut the ham I'm sorry that sure funnily enough according to 25 fun facts about bacon I can ever go to I mean it's gonna have a lotta sorry and now baked you a little bit as usual we haven't actually got any the equipment leasing makers and once it's ready to go you want to put it into a preheated oven at 200 degrees acid iggy's you should also pre-heat your oven to 220 degrees all right how long and how hot on it how are you lit we've been going to put the oven for about 10 minutes at gas mark 6 5 whatever is I'm going to put it into an oven at 200 degrees for whatever I mean but about the IDS syphilis put it into the oven for about as long as it says on the recipe that's all right overcooking eggs it's just not like the is that this is the worst thing in the world baking baking is a science I'm a business end of a really that makes lovely look there's a hair yeah this is not safe fighting with knives quickly print all of them did we work out what I see now we've added warden staring everyone i turn on doing don't overmix it ok itself at thrust mushrooms into the pasta now we need to combine the underneath the contents of all of our balls be careful not to overmix this mixture can you say mix and mixture in the same sentence I'm gonna chop the peppers for the stuffed peppers I'm gonna get papers ready to be stuffed oh good god me I'm still cutting bacon they'd all good dude laughs you helped put it back in the convent recording it didn't cry chopped garlic it's on this rental business stir in the flour to what that's stuck together really well so as a remarkable things one of the thing about mixes cheese is really versatile you viewership you to make the ham nice and warm is that this davido thanks very much for watching I hope you've been scared by my muffins I just did a free full that was in that waving right oh thanks very much for watching I hope you've enjoyed watching we make my mother I'm a sharpie but it if you've been terrified by my ego will be like a look over here shut the off my name topsham I like hugging things thank you", "_WqkEGbGVGY": "let's do this right hope you guys like Weezer", "p01q2xhjJ-Q": "Welcome to the course on Scalable Data Science,\nmy name is Anirban and I am from IIT Gandhinagar. So, today\u2019s lecture is going to be on Feacture\nHashing. So, until now you have studied applications\nof both sampling and random projections to a bunch of optimization problems relating\nto linear algebra right. We have we first saw applications of random projections to\nthis low rank approximation, here of at least two different forms, to matrix multiplication,\nto L 2 regression. And, then we and in the last lecture you also saw how we can turn\nsome of these how we can also imply sampling to solve some of these problems right using\nthe leverage score sampling techniques. So, all of a lot of these one big thing that\nis missing, while a lot of these is relevant in a unsupervised machine learning right,\nthe supervised is a slightly different ballgame ok. And they in this lecture well explore\nhow random projection and related techniques can also be useful in supervised machine learning\nright. The basic techniques and the intuition is going to be the same right, the problem\nis going to come from that of supervised machine learning. Therefore, and what we will discuss\nat the end is what a new kind of guarantees can be should we hope to have in this case. So, let me give the outline first, there are\ngoing to be two different use cases both of them are relating to text. In one use case,\nwhat well consider is that there is going to be a very large feature space right and\nwe will point out why having a large feature space is actually useful. Essentially it is\ngoing to be to capture very long range correlations in the in my document in a specific document.\nAnd, we will have to come up with some technique to control this the size of this feature space\nright and I mean the large, having a large feature space is actually useful for classification.\nHowever, I mean in a is actually useful in improving classification accuracy, however,\nit is computationally very inefficient. Therefore, what we will do is that first we\nwill pose the problem as having a large feature space, and then we will use random projection\nto cut down the feature space while preserving the benefits, some of the benefits that we\nhad brought in because we introduced the large feature space right. So, in this way we will\nsort of I mean do both right; a similar story is going to come out in mails file classification.\nHere what we are going to see is that we have to introduce a large feature space in order\nI mean as a cheap way of trying to give personalized classifiers to each individual user.\nAnd again this is going to blow up the entire dental problem size by a lot right and therefore,\nwe are going to use random projection related technique to cut it down right. And what we\nwill see in that in both these in both these settings are very specific type of random\nprojection is going to be more useful to us. So, let me outline the two use cases first.\nSo, in the text classification setting right let us think about what kind of classifiers\nwork best right; while nowadays you see a lot of this non-linear non convex classifiers\nI mean the deep neural nets that give excellent results, in speech and vision right. There\nis a typo out here and these are good because they have very high accuracy, they are also\nreasonably fast at test time. However, during training time they can be terribly slow right\nand often require a lot of thinking a lot of domain expertise in order to ensure that\nyou do not get stuck in bad local minima right there are lot of heuristics, that you need\nto know about. Furthermore it is not clear that a lot of\nthese I mean these non-convex classifiers are very useful when the data is sparse and\nhigh dimensional right i. e. when the data comes from text right. Although there are\ntechniques I like what to vet that are changing this that are creating dense representations\nof word doc of sort of very sparse documents and words and then and thereby using it. However,\nit is still not clear that they are that they can beat the I mean very practical systems\nthat are robust right, that are I mean that are formed by much simpler means ok. So, in practice what is what is still very\nuseful are what I call count based features right. And these slides are actually from\na talk by John Langford ok. So, in a sense what I mean is that I mean if you remember\nthe normalized or rather the back of words model of a document right in which we essentially\nrepresent a document as a set of its tokens along with the counts, that are associated\nwith these tokens. And, these counts could represent some normalized frequency in the\ndocument frequency things like this. We could come up with the normalization with\nthe term count in very fancy ways right, we could take this pack of words model and we\ncould augment it quite a bit right. So, the benefit of the simple bag of words model is\nthat, it is really easy to train at least on single machine ok. And paralysation is\na different ball game altogether it is also clear that its really fast at this time right\nbecause, all you need to do is essentially compute some statistics from this from the\nquery document and then use that for prediction right.\nAnd it turns out that this still works surprisingly well for text classification right, which\nis which still forms a large part of the machine learning and practical machine learning tasks\nthat we do right. And a common practice I mean if you want to augment the naive bag\nof if you think that a bag of words model itself is it is knife to start with, a common\npractice to augment it is to take various combinations of n grams and skip grams right.\nAnd the intuition is that, that if I take a I mean if I take bigrams trigrams etcetera\nand scale grams well be able to capture high level correlations. But still they are fairly\nefficient to calculate and therefore, we are capturing the high level correlations, while\nbeing while retaining the efficiency of feature construction ok. Let me give an example of bigrams and n grams.\nSo, for instance here is the sentence that we have the rain in Spain falls mainly on\nthe plain ok. So, the two grams in this and this in so, of course, the unigrams in this\nin this particular document, would be would be the individual words the rain in Spain\nfalls mainly in the plain right. The 2 grams would be the rain raining in Spain Spain falls\netcetera. And what you can easily see is that by looking at the this sort of unigrams you\nare getting a better idea of what the document is about.\nSo, that the skip grams that we that I just mentioned has various has different variants\nto it right. Here is one particular variant there is known as a 1 skip 2 gram right, that\nis you take bigrams right where between 2 bigrams that you take you just skip one of\nthe bigrams. For instance you take the and in right similarly you take rain and Spain\nright, you take in and false and so on and so forth right. And the point is that is that\nif you take a very a number of such variations of this 1 skip 2 grams to skip 3 grams and\nso on and so forth you captured you essentially capture this collection this.\nSo, first of all these set of features are trivial to automatically construct you are\nnot sitting down and thinking about special feature engineering and secondly, once you\nthrow in all these features, they actually capture a lot of semantics of the document\nright. An interesting way of looking at this is that I mean if I sort of up in take a representation\nof the document in this bloated up feature space, and then calculate the similarity of\nthis you can also think about it at some kind of kernel representation of the document right.\nEssentially instead of instead of simple features we are taking combinations two combinations,\nthree combinations, four combinations of these features. The only problem that I see right\nnow is that all these are good, but the feature space is becoming very high dimensional. For\ninstance if you take a few variants of the subsets of this unigrams trigrams and then\nand then 1 skip 2 grams and so on and so forth then the number of then the number of possible\nfeatures becomes let us say if it is a 3 gram it becomes the this the cube of the size of\nthe image dictionary which is a lot. So, how should we deal with this ok? So, for the second example let us look at\nthe problem of mail classification right and here is the problem. That is suppose that\nsuppose a piece of mail comes to us right and now we have to decide, whether to put\nit in your inner in a particular users inbox or in the spam box ok. Now, there are different\nand I mean in order to do this all the standard online email systems. So, think of our webmail\nsystem have their own classifiers that they have trained right and these classifiers they\nI mean based their decision they put it in the they decide to put it in the inbox or\nin the spam box and now the classifier also gets feedback from the users right. So, the\nuser can possibly read, reply, forward, delete, forward, and then I mean or she could delete\nor mark the user a spam right. So, it is possible that I take these as the\npositive feedback and this as a negative feedback on the email right. And based on this feedback\nI can retrain my classifier ok. So, the things to keep in mind is that each email needs to\nbe classified in the spam boxer or inbox and therefore, this has to be done in basically\nreal time. A classifier has to be updated by user feedback, which means that it has\nto be updated frequently because, the nature of spam keeps on changing and users do not\nalways agree about what is spam right. So, in machine learning terms, this is in\norder to satisfy those requirements this is what we have to do right. So, suppose we are\npresenting representing each email as a point in some high dimensional space. So, these\nare the emails, this points are the emails ok. And now and every users classified representing\nas a vector in this high dimensional plane right users 1 classifier users 2 classifier.\nSo, first I say that users have different spam preferences. So, what it means is that\nthe classifier for each user is slightly different right. So, the user 1 can potentially consider\nthe emails from Groupon and Yahoo to be spam, but maybe user 2 considers this company, this\nis a company called target and Yahoo to be spam, but not Groupon ok. So, we need to assign different classifiers\nto each user, which means that we need to store a different hyper plane for each user.\nWe also need to ensure at the same time we also need to ensure that new users get a good\nclassifier right. Because new users, for new users we do not have that many feedback from\nthe user. But we still need to guarantee that we get a reasonably good classifier for the\nuser right. Furthermore we want to update this classifier currently using new feedback\nok. So, how can we achieve all these? So, here is one particular very interesting way\nof doing all this. What we do is that we say that let us blow\nup the feature space right and how do we blow up the feature space? Let us take the email\nright and we do sort of standard bag of words or unigram or bigram decomposition and we\nget this and we get this unigrams or bigrams from the email right. Now in order to sort\nof one way now what we do is that in order to assign we assign every user their own feature\naddress space right. What it means is that every I mean the global set of features, this\nis this global set of features and then there is a set of features that belong to user 1\nthen there is a set of features that belong to user to belong 2 user 3 and so on and so\nforth. So, this is global set of features right and\nhow do we implement this? We implement this it is simply, we take all the tokens right\nand then we take all the tokens by themselves we these belong to the global feature space\nright. We just prefix the name the token by the name of the user right and these then\nbelong to the specific to the user specific feature space. So, notice that only the emails\nthat u 1 gets right I mean that belong to u 1 will have the I mean will have tokens\nthat are specific to u 1 right. The emails that the same email if u 2 gets\nit would not have it would not have this particular token right because it is prefixed by the\nuser 2\u2019s it is going to be prefix by that user 2\u2019s name not by user 1\u2019s name ok.\nAnd now what well do? What we say that that let us learn a classifier on this entire feature\nspace right. It is not very hard for you to see that what this corresponds to is that\nis that for a for every user we are learning a classifier of the following form. That a\nclassifier which has a component in the global in the I mean a set of coefficients for the\nglobal feature space and a classifier which has set of coefficients for that for the user\nfeature space right. So, therefore, for a particular email for\na test email right this I mean for the classifier does is as follows, that it takes it I mean\nlet me call this w global this is w global and the set of coefficients here to be w user\nright. So, therefore, for a test email, we again do sort of representation of the email\nin the global space and in the user space and then we take the corresponding dot products.\nAnd therefore, what it turns down to is that we sort of and we could apply our threshold\non this and therefore, what it says is that we are combining some a global classifier,\nwith a user specific classifier and we are giving use and we are giving a particular\nuser the combination of these two classifiers right I mean it is entirely possible that.\nSo, why is this good? This is good because maybe it is a considered particularly user\nwho has just come into the system. So, for that particular user the I mean the w transpose\nuser is 0 right because I have not yet trained user specific classifier, because the user\nhas not given any feedback right therefore, there would not be any token on the on the\non the on the user specific part ok. So, but then, but for this user we still have the\nglobal component right and we are still and because that is trained by everybody\u2019s feedback\nright. That is a reasonably good classifier it is\nnot clear it is not very tailored to that users to that users needs, but it is fairly\nits fairly good ok. So, therefore, if I start of this new user with his global classifier,\nshe has a reasonably good experience there are and they are all on onwards this particular\nuser can start providing feedback if she wants to and therefore, the value of this the importance\nof this part of the of the classification technique increases. And, so there might come\na time if the user provides enough feedback that this part of this part of the of the\nof the this part of the classification, really has more importance than this part which is\nthe global part. And then the user will start getting them will start sort of acquiring\nthe benefits of personalized classifier ok. So, this is a very natural way of going from\na from a global initialization to a personalized classifier for more involved universe ok. What is the bad thing about it right? And\nobvious bad thing about it is that the size of the dimension the resulting dimension space\nhas blown up a lot, because if you if imagine that if I have n users right if D is the if\nD was the size of the of my initial dictionary my current set of dimensions is N times D\nright. Because I am multiplying the I am multiplying the user by the I am multiplying the number\nof possible size of the possible dictionary by the number of users and this is potentially\na huge right. Because while the while the emails are sparse\nthe vectors w and sparse. So, therefore, keeping all these vectors the w in this and I mean\nmaintaining this vector w in this particular space is fairly expensive right. So, maintaining\neven a single w which is what you have to maintain in this in this space of size ND\nis very very expensive it is impossible. So, what can we do? So, if we want to compress\nthis features into a into memory right and the way to compress. And what we want is that\nthis compression should be linear why is this linear? It makes updating easy right because\nif you remember that we want to update the classifier constantly right and one of the\nways of doing the update is by doing some kind of a gradient descent.\nFor instance a sarcastic gradient descent that that when you get a new email or a batch\nof emails with the user feedbacks on them you just do you just do a linear update on\nthe I mean you just do gradient update on the classifier ok. Ideally it should have\nguarantees right and should maintain sparsity as much as possible right that I do not want\nI mean the emails themselves are; obviously, sparse data I and I do not want to densify\nit by too much ok. So, so how can we do this right? So, here\nis an obvious way that suppose we create a hash table that hashes features into buckets\nright. So, what I mean is as follows; that suppose I have and this is my original set\nof features right, which is which essentially very large right, the dimension is very large,\nbut it is sparse. So, maybe I have some values out here C plus 1 plus 2 and then plus 3 and\nso on and so forth. So, this dimension is really large let me\ncall it capital D. So, what I could easily do is that is that hash it into a small number\nof dimensions small d right. So, so all that I need is that I need a hash function that\ngoes from capital D to small d right and I can put it in here I can put it in here right.\nSo, basically then a bunch of features can collide. So, a bunch of features might go\nto the same position in the smaller in this in this in the smaller representation, and\nwhat I will just do is basically add up put in half a counter out here that is the addition\nof these counts. So, suppose these three these three features\ngo into the same bucket in this using this hash function, then the value out here will\nbe the let me call this a 1 let me call this a k and let me call this a i. So, the value\nout here is a 1 plus a k plus a i right that is the value that you put in here this could\nbe one way of doing it right. The question is that is there a principle better simple\nway of thinking about this right sure is this enough, should I be doing something else right\nand the what about the guarantees you because we were promising some bounds with the I mean\nwe were promising some algorithms that comes with some guarantees right. Now if you have paid attention in this course\nyeah you should it should not be very hard for you to realize that this is essentially\nSparse Johnson linear transformation right. We were transforming from one particular vector\nto a smaller set to a smaller set right by using a hash function right, except that here\nwe are using only a single hash function right instead of using multiple hash functions.\nSo, I could think of it as a hash function, as a hash transformation or I could think\nof it as a random projection that preserves sparsity ok. So, then in that case we should\nreally take guidance from the principles of designing a random projection.\nAs well as and if you do this well be able to draw upon the theoretical guarantees random\nprojection comes with right which we have seen is really strong. And in particular one\nof the lessons that you should draw is that it is not enough to say supposing this is\nagain a 1 a k a i and let us say all of these have gone to the have gone to the same bucket.\nIn particular it is not enough to say to put a 1 plus a k plus a i in the bucket right\nbecause remember what SparseJL was doing SparseJL was adding a random sign to each of them was\nmultiplying each of them by a random sign. So, I should do s 1 times a 1 you pick keep\nthis as it is. So, instead I should do s 1 times a 1 plus s k times a k plus s i times\na i, where each of the sis is a binomial is a Bernoulli plus minus one with probability\nhalf. So, why is this important intuitively? Intuitively\nthis is important because it helps the counts out here be unbiased estimators of the counts\nof the original set of counts ok. So, so this is one of the and once we do this, we get\nexactly this Sparse Johnson linear transformation with the I mean with the sparsity 1. And in\nfact, we have seen this variant this is what we called as a subspace projection right and\nwe saw that, it does preserve some very nice properties for the subspace reservation ok. So, how do we use it practice ok? So, the\nalgorithm seems fairly clear right in terms of the theory. So, what do we do in practice?\nSee the theoretical guarantees, we assumed all independent hash functions or at least\nsome key wise independent hash functions for some value of k; typically k is like log one\nover delta right by delta right. So, in practice we do not really create k\nwise independent hash functions by ourselves. What we do is that we typically use standard\nfast hash functions. There are deeper reasons for doing this as to as to why this should\nwork. So, people have thought about it and there are some theoretical guarantees, but\nin, but basically the idea is that there I mean given the given that the practical data\nis not entirely adversarial chosen, you do not really have to guarantee complete randomness\nperfect randomness in your in your hash function. It is enough to it is enough to choose a practical\nfast hash function for instance there is a particular software called Vowpal Wabbit right\nthat actually implements this I mean gives you a very fast classifier using this feature\nhashing, and it uses and it uses something known as a murmur hash 3. And in fact, you\ncan you can a lot of the a lot of APIs standard API I mean machine learning, packages official\nhave feature hashing have a as a standard APO including ones that including packages\nthat run on scalable systems. Like Hadoop and spark ok. So, then given I\nmean that ideally if you I mean if you are using one of these machine learning packages\nthe suggestion is that you should use definitely use that feature hashing API that they have\nprovided. If you are using implementing your own classifier, you should use one of the\nstandard fast hash functions like murmur hash right and this is what you typically do right.\nThat you create your let us say you are doing text classification, you create your I mean\nyou find out a representation of your document in terms of the engrams bigrams etcetera,\nthen you hash your input data and then you learn your classifier over this hashed space\nok. So and when testing we first hashed the test point using hash functions that we have\nused. So, it is very important that you that you use the same hash functions that you use\nit, do not change the random seed and then you apply the classifier collision will happen\nof course, collision will happen, but that is exactly the point.\nThat we are using these collisions we; I mean using these collisions to compress the set\nof features into a much smaller much smaller dimension, while preserving a lot of the geometry\nof the original set of data points right. And the kind of geometry that they preserve\nis what we have seen for Johnson linens transformation right. That it preserves moral is approximately\nthe pair wise distance of the data points and therefore, if two classes are separable\nit reserves the susceptibility right. So, this is the standard way of implementing;\nit beyond this you should use a domain knowledge for instance if you know that there are super\nimportant features that you do not want to collide with anybody or there are groups of\nfeatures, such that you do not want this set of features to collide with this set of features,\nbut maybe collisions among this set of features is fine right then you should design your\nhash function in an you should basically partition your hash the range of your hash functions\nin an appropriate way right. And all and for all that you need to think carefully about\nthe problem setting ok. So, before we end I will show you a bunch\nof experimental results on the for spam classification from a particularly from a paper that we had.\nThis is this is a result that took 3.2 million year emails from yahoos data and about 400\nkey users right. So, if we used the user personalization technique that I had mentioned as well as\nthe stand a standard hash map it would have taken us 70 terabytes.\nJust to store this right because there happens to be around 40 times 10 to the 6 unique tokens\nin this in this data right what we did was that, we sort of used these hash functions\nin which in which we controlled in which we control that the size of the size of the size\nof the of the hash space right. And then we say that the hash space is only going to contain\n2 to the 26 floats and therefore, the maximum size is 256 MB right and on the x axis, you\nsee the number of bits in the hash table and therefore, the size of the hash table is 2\nto the power of this right. So, this is 256 MB instead of the 70 terabytes\nright and on the y axis you we have plotted the relative classification error; that means\nthat the black one is the baseline classifier; where there is a single classifier that is\nassigned to everybody right. If we assign I mean if we hash happened to hash the classifier\nthat performance is; obviously, a little worst right, but it soon boils down to the performance\nof the standard baseline classifier. If you assign everybody their own persons classifier\nwe get a huge boost more or less like 30 percent I mean 30 percent decrease in the error rate\nok. So, this is actually something that is very practical. So, just to summarize hashing has proven to\nbe an important practical component in order to scale supervised machine learning the large\nfeature spaces some amount of background theory for this is provided by the random projection\nliterature right. That this gives us the other random projection literature gives us intuition,\nthat what were really doing is preserving the structure the pair wise geometrical structure\nof the original space. And therefore, the and therefore, it is useful in class in the\nsupervised setting because the classes remain separable if they were originally separable.\nHowever, it is actually a lot of open questions for instance, if the if the margin is large\nhow does that play a role in deciding the size of the target hash function right what\nabout generalizability what kind of generalizability bounds can we can we actually prove right.\nCan I get do I really need the entire the entire strength of the random projection or\ncan we sort of do with much smaller number of dimensions. So, these are all questions\ndoes this generalize to non-linear in the non-linear setting for instance. And these\nare all questions that people are pursuing for research ok. So, the primary reference for this are is\na paper of us that is called Feature Hashing for Large Scale Multitask Learning as well\nas the original feature paper that introduced Hash Kernels by Shi Petterson, Dror and Smola\nand Strehl and Vishwanathan that came out AISTATS and then finally.\nThank you.", "sHI6r78PjAA": "hi have you been searching for this product or service we've got solutions to suit your needs at this moment you should purchase this products in the web shop along with a take a look at rate we provide you with the best discounted price tag for this item get it now at an incredible selling price together with supporting help save the instant today [Music] get a reduction now much more info click into the description to find out extra satisfied purchasing and hopefully you will always be inexpensive [Music] hi are you presently searching for this products we have got responses for you at this time you should buy this product or service in the online store in addition to it take a look at price we give you the most effective discounted price tag for this product get it you", "Pi094R57YKc": "hi do you think you're in search of this products we have responses for yourself today you can purchase this item and the we give you the most beneficial discounted price for this product or service get it now at an unbelievable value together with aiding save the instant right now [Music] get a reduction now far more details simply click into the outline to view far more content buying and ideally you will always be affordable hello are you presently in search of this item we have responses for you personally at this time you can purchase this products in the online shop along with a checkout cost [Music] thanks for watching if you're ready to take action click the link in the description below this video now", "ibJcsb4GCLM": "hi everyone how is it going welcome back to our channel where we talk about various hot products in the market based on our own research in this video we are going to show you another interesting product in the market you may be interested in here are some previews of the product if you want to know more about the products price specifications or may discount please click link in the description below the store usually gives special offers for a limited time please read carefully the product details and specifications in the link if you want to buy the product please click link in the description below don't forget to like our video and subscribe to our channel because we regularly update our channel with another hot products in the market [Music] [Music] [Music] do [Music] [Music] you", "W6H-jMf6828": "hi everyone how's it going welcome back to our channel where we talk about various hot products in the market based on our own research in this video we are going to show you another interesting product in the market you may be interested in here are some previews of the product if you want to know more about the product's price specifications or maybe discount please click link in the description below the store usually gives special offers for limited time please read carefully the product details and specifications in the link if you want to buy the product please click link in the description below don't forget to like our video and subscribe to our channel because we regularly update our channel with another hot products in the market [Music] so [Music] [Music] [Music] so [Music] [Music] so", "emmD6z1aCS4": "[Music] [Laughter] [Applause] [Music] i'm afraid i wake up when i die and it is too late to climb any mountain [Music] time is up the answers the blood's running it's best to not feel sorry when it's over i will be the greatest it's her nose", "q5FyT1cmXQQ": "hi everyone how is it going welcome back to our channel where we talk about various hot products in the market based on our own research in this video we are going to show you another interesting product in the market you may be interested in here are some previews of the product if you want to know more about the products price specifications or may be discounted please click link in the description below the store usually gives special offers for limited time please read carefully the product details and specifications in the link if you want to buy the product please click link in the description below don't forget to like our video and subscribe to our channel because we regularly update our channel with another hot products in the market [Music] so [Music] so [Music] so [Music] you", "Hlh8QidVEsw": "hi everyone how is it going welcome back to our channel where we talk about various hot products in the market based on our own research in this video we are going to show you another interesting product in the market you may be interested in here are some previews of the product if you want to know more about the products price specifications or maybe discount please click link in the description below the store usually gives special offers for limited time please read carefully the product details and specifications in the link if you want to buy the product please click link in the description below don't forget to like our video and subscribe to our channel because we regularly update our cho with another hot products in the market [Music] [Music] [Applause] [Music] [Music] [Music] [Music] foreign", "uM9EkjvSzP4": "[Music] hi everyone how's it going welcome back to our channel where we talk about various hot products in the market based on our own research in this video we are going to show you another interesting product in the market you may be interested in here are some previews of the product if you want to know more about the product's price specifications or maybe discount please click link in the description below the store usually gives special offers for limited time please read carefully the product details and specifications in the link if you want to buy the products please click link in the description below don't forget to like our video and subscribe to our channel because we regularly update our channel with another hot products in the market [Music] do [Music] so", "XvEmvtu5T3c": "[Music] [Laughter] [Applause] so [Music] i'm afraid i wake up when i die and it is too late to climb any mountain [Music] time is up the answers the blood running it's best to not feel sorry when it's over i will be the greatest [Music] it's", "bELLbX5D1OY": "hi everyone how is it going welcome back to our channel where we talk about various hot products in the market based on our own research in this video we are going to show you another interesting product in the market you may be interested in here are some previews of the product if you want to know more about the products price specifications or maybe discount please click link in the description below the store usually gives special offers for limited time please read carefully the product details and specifications in the link if you want to buy the product please click link in the description below don't forget to like our video and subscribe to our channel because we regularly update our channel with another hot products in the market [Music] you", "x36ERieAPug": "[Music] [Music] [Applause] [Music] [Music] you", "Uh692NIBy-w": "[Music] [Music] hi everyone how's it going welcome back to a channel where we talked about various hot products in the market based on our own research and this video we are going to show you another interesting product in the market you may be interested in here are some previews of the product if you want to know more about the product price specifications for maybe discounts with link in the description below the store usually gives special offers for a limited time please read carefully the product details and specifications link if you want to buy the product please click link in the description below don't forget to like her video and subscribe to our channel because you regularly update our channel with another hot products in the market"}